<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[5π - fish's blog]]></title><description><![CDATA[Outside the box, no one can hear you scream.]]></description><link>https://5pi.de/</link><generator>RSS for Node</generator><lastBuildDate>Thu, 24 May 2018 13:35:13 GMT</lastBuildDate><item><title><![CDATA[Found severe Azure Kubernetes Bug but ain't even got a lousy T-Shirt]]></title><description><![CDATA[About half a year ago at one of my last gigs, I built a Kubernetes Cluster on
Azure. On Azure, the Kubernetes Volume Claims are commonly…]]></description><link>https://5pi.de//2018/02/17/found-severe-azure-kubernetes-bug-and-aint-even-got-a-lousy-t-shirt/</link><guid isPermaLink="false">https://5pi.de//2018/02/17/found-severe-azure-kubernetes-bug-and-aint-even-got-a-lousy-t-shirt/</guid><pubDate>Sat, 17 Feb 2018 12:19:27 GMT</pubDate><content:encoded>&lt;p&gt;About half a year ago at one of my last gigs, I built a Kubernetes Cluster on
Azure.&lt;/p&gt;
&lt;p&gt;On Azure, the Kubernetes Volume Claims are commonly implemented by a “Virtual
Hard Disk” (VHD) file stored on the &lt;a href=&quot;https://azure.microsoft.com/en-us/services/storage/blobs/&quot;&gt;Azure
Blob Storage
Service&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Kubernetes “cloud provider intergration” takes care of allocating and
managing these disks.&lt;/p&gt;
&lt;p&gt;While I was working on some issues we had with the cloud provider intergration,
I realized that the VHDs related error messages include a full URL like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;https://abc1234.blob.core.windows.net/5678901234/example-dynamic-pvc-834987df-ffe3-12d3-9dec-feaab9e30f09.vhd&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Curious about this API semantics, I tried it download it with curl. And it
worked.&lt;/p&gt;
&lt;p&gt;First I though I made some mistake setting up the storage account or Kubernetes
itself, but I quickly realized that all of the dozens of VHD urls I found in
various github issues could be downloaded without any authentication!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Turns out, all Volume Claims that were created on Azure between 1.6.0 and 1.6.5
were world readable.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I reported the issue by following the &lt;a href=&quot;https://kubernetes.io/security/&quot;&gt;Kubernetes Security and
Disclosure&lt;/a&gt; guidelines. While the team was
responsive in general and the problem fixed quickly, I wasn’t very happy with
how the announcement was handled. It felt like it was kept under the radar and I
never got credited, let alone qualified for a bug bounty.&lt;/p&gt;
&lt;p&gt;Of course I’d love to get credited and get a bug bounty but I’m more
worried about what this handling means for the next person considering to
responsibly disclose similar issue.&lt;/p&gt;
&lt;p&gt;Kubernetes is my bread and butter right now and I have high expectations towards
it. I’ve contributed to several components and I have high regards for the
maintainers of the project and the companies involved. I too had the impression
that Microsoft changed and takes these things serious these days, so I was
surprised how this was handled. I also was quite disappointed by not getting any
response anymore around why this didn’t qualify for a bug bounty.&lt;/p&gt;
&lt;p&gt;I’ll continue to disclose issues responsibly but I fully understand people who
don’t want to jump through these hoops if they don’t even get credited for it.&lt;/p&gt;
&lt;h2&gt;Timeline&lt;/h2&gt;
&lt;h4&gt;2017-06-15&lt;/h4&gt;
&lt;p&gt;Initial Report and first response with promising to send a
follow up later that day.&lt;/p&gt;
&lt;h4&gt;2017-06-17&lt;/h4&gt;
&lt;p&gt;The issue was fixed in Kubernetes:
&lt;a href=&quot;https://github.com/kubernetes/kubernetes/pull/47605&quot;&gt;https://github.com/kubernetes/kubernetes/pull/47605&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;2017-06-19&lt;/h4&gt;
&lt;p&gt;After not hearing back, I asked again about a CVE and the plans
to announce the issue. I’ve got a response the same day, confirming that a CVE
will be issues and that they’re still working on identifying the affected
customers.&lt;/p&gt;
&lt;h4&gt;2017-07-07&lt;/h4&gt;
&lt;p&gt;So far, no CVE was issues so I asked again. I also asked if they
are okay with me blogging about it.&lt;/p&gt;
&lt;p&gt;In the response I was told that CVE was requested but not yet issued. I was also
asked to please not talk about this issue publically until they finished letting
affected customers know.&lt;/p&gt;
&lt;h4&gt;2017-07-13&lt;/h4&gt;
&lt;p&gt;I’ve got an update, letting me know that the security team is in their final
stages. They apologize for the “clearly no acceptable” time it took to process
the issue.&lt;/p&gt;
&lt;h4&gt;2017-07-20&lt;/h4&gt;
&lt;p&gt;CVE-2017-1002100 was assigned to the issue.&lt;/p&gt;
&lt;h4&gt;2017-08-02&lt;/h4&gt;
&lt;p&gt;I’ve asked again if there will be some sort of announcement which would also
credit me and encourage people to upgrade. I was pointed to &lt;a href=&quot;https://groups.google.com/forum/#!msg/kubernetes-security-announce/n3VBg_WJZic/-ddIqKXqAAAJ&quot;&gt;a post in
kubernetes-security-announce&lt;/a&gt;
which I never heard about before (it only includes three posts). After
searching, I also found it was noted in the &lt;a href=&quot;https://groups.google.com/forum/#!topic/kubernetes-announce/WodXsASu6y0&quot;&gt;1.6.6 notes on
kubernetes-announce&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;2017-08-03&lt;/h4&gt;
&lt;p&gt;I’ve explained that I expected an announcement with discussion of risks and need
for upgrade, as well as credit me for my finding. Apparently though this is all
the announcemnts that will be done in such situation.&lt;/p&gt;
&lt;p&gt;I’ve also ask specifically if this qualifies for some of Microsoft’s bug
bounties.&lt;/p&gt;
&lt;h4&gt;2017-09-19&lt;/h4&gt;
&lt;p&gt;I’ve asked again about a bounty. Since I just launched &lt;a href=&quot;https://latency.at&quot;&gt;https://latency.at&lt;/a&gt;, some
Azure credits would be super useful to me.&lt;/p&gt;
&lt;p&gt;I’ve got the answer that “the MSRC folks didn’t think it matched”, but they’ll
look into some startup credits.&lt;/p&gt;
&lt;h4&gt;2017-09-25&lt;/h4&gt;
&lt;p&gt;I’ve asked why it didn’t qualified for a MSRC bounty, given that there is a
&lt;a href=&quot;https://technet.microsoft.com/en-us/dn800983&quot;&gt;Microsoft Cloud Bounty&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;2017-10-05&lt;/h4&gt;
&lt;p&gt;Asked again.&lt;/p&gt;
&lt;h4&gt;2018-01-19&lt;/h4&gt;
&lt;p&gt;Asked “one last time”.&lt;/p&gt;
&lt;h4&gt;2018-02-17&lt;/h4&gt;
&lt;p&gt;Released this blog article&lt;/p&gt;</content:encoded></item><item><title><![CDATA[kubecfn: Cloudformation installer for reasonably secure multi-master Kubernetes Cluster]]></title><description><![CDATA[Introduction These days I’m helping  koko  to build their Kubernetes
infrastructure on AWS. Initially, we used kops to deploy the cluster…]]></description><link>https://5pi.de//2018/02/01/kubecfn-cloudformation-installer-for-reasonably-secure-multi-master-kubernetes-cluster/</link><guid isPermaLink="false">https://5pi.de//2018/02/01/kubecfn-cloudformation-installer-for-reasonably-secure-multi-master-kubernetes-cluster/</guid><pubDate>Thu, 01 Feb 2018 13:07:16 GMT</pubDate><content:encoded>&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;These days I’m helping &lt;a href=&quot;https://itskoko.com/&quot;&gt;koko&lt;/a&gt; to build their Kubernetes
infrastructure on AWS. Initially, we used kops to deploy the cluster but weren’t
very happy. I’ve researched various option and &lt;a href=&quot;/2017/12/15/production-grade-kubernetes/&quot;&gt;blogged about
them&lt;/a&gt; earlier.&lt;/p&gt;
&lt;p&gt;tl;dr: There aren’t that many options that are secure, highly available and
don’t require buying into a vendor ecosystem.&lt;/p&gt;
&lt;p&gt;kubeadm is the official vendor independent solution to installer Kubernetes.
While it lacks automated setup of multi master clusters, &lt;a href=&quot;https://kubernetes.io/docs/setup/independent/high-availability/&quot;&gt;it’s
possible&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To have a reliable and reproducable way to build clusters, we decided to create
&lt;a href=&quot;https://aws.amazon.com/cloudformation/&quot;&gt;Cloudformation&lt;/a&gt; templates to fully
automate this setup.&lt;/p&gt;
&lt;p&gt;All this is opend sourced and available on &lt;a href=&quot;https://github.com/itskoko/kubecfn&quot;&gt;https://github.com/itskoko/kubecfn&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Architecture&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/kubecfn-2f7d38406a33c80962b9131bc20ee2bf.svg&quot; alt=&quot;kubecfn architecture diagram&quot;&gt;
The cluster consists of multiple components required for the cluster.&lt;/p&gt;
&lt;h3&gt;Offline key generation&lt;/h3&gt;
&lt;p&gt;The first step to create a new cluster is &lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/Makefile#L84&quot;&gt;generating the TLS keys for etcd and
kubeadm&lt;/a&gt;.
These credentials get uploaded to S3 and the instances get a IAM instance
profile with a &lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/kubernetes.yaml#L759&quot;&gt;policy allowing it to download
them&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Controller Instances&lt;/h3&gt;
&lt;p&gt;The controller instances are managed by an autoscaling group.&lt;/p&gt;
&lt;p&gt;They are running etcd and the controller components like the apiserver,
scheduler and controller manager.
&lt;a href=&quot;https://coreos.com/ignition/docs/latest/&quot;&gt;ignition&lt;/a&gt; downloads the &lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/kubernetes.yaml#L859&quot;&gt;TLS
keys&lt;/a&gt;
and &lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/kubernetes.yaml#L978&quot;&gt;sets up systemd units for kubelet and
etcd&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On initial cluster creation (&lt;code&gt;make create-cluster&lt;/code&gt;), the ClusterState flag is
set to &lt;code&gt;new&lt;/code&gt;, which enables etcd bootstrapping. To find the other etcd peers,
we’re using SRV record based &lt;a href=&quot;https://coreos.com/etcd/docs/latest/v2/clustering.html#dns-discovery&quot;&gt;DNS
Discovery&lt;/a&gt;.
This option give us one way to bootstap, as well as dynamically add and remove
members. On the other hand, the ‘static’ and ‘etcd’ discovery can be only used
for initial cluster configuration.&lt;/p&gt;
&lt;p&gt;Unfortunately AWS can’t automatically add DNS records for instances in an
autoscaling group. To solve this, we’re using a lambda to update a route53 zone
(see next section).&lt;/p&gt;
&lt;p&gt;Once etcd is up, &lt;code&gt;kubeadm init&lt;/code&gt; creates the manifests and keys for the
controller components but use the pre-generated keys for the CA generated in the
first step. This makes sure all controller keys are signed with this trusted CA.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/kubernetes.yaml#L1158&quot;&gt;An ELB&lt;/a&gt;
in front of the apiservers provide a stable endpoint for both the workers as
well as users of the cluster.&lt;/p&gt;
&lt;h3&gt;Route53 Lambda&lt;/h3&gt;
&lt;p&gt;The cluster creates it’s own Route53 zone, which needs to be a subdomain of the
given &lt;code&gt;ParentZoneID&lt;/code&gt; parameter. Beside a record pointing to the apiserver ELB,
this zone is updated &lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/kubernetes.yaml#L643&quot;&gt;by a
Lambda&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Each time a controller instances get added or removed by the autoscaling group,
the lambda gets triggered and adds/removes DNS records for etcd discovery.&lt;/p&gt;
&lt;h3&gt;Workers&lt;/h3&gt;
&lt;p&gt;The worker setup is straight forward. It’s also managed by an autoscaling group.
The ignition config downloads credentials for the kubelet from S3&lt;/p&gt;
&lt;h3&gt;Conclusion and Caveats&lt;/h3&gt;
&lt;p&gt;kubecfn wasn’t build by a big team nor as an end in itself. We focused on making
it easy to maintain, reliable and fail gracefully. That means there are some
&lt;a href=&quot;https://github.com/itskoko/kubecfn/labels/automation&quot;&gt;rough edges around
automation&lt;/a&gt;. The security
&lt;a href=&quot;https://github.com/itskoko/kubecfn/labels/security&quot;&gt;could be improved further&lt;/a&gt;
and I’m not very happy with the whole concept of IAM instance profiles which
effectively grants these privileges to every process running on a host. We
mitigate this by installing
&lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/manifests/kube2iam.yaml&quot;&gt;kube2iam&lt;/a&gt;
but this isn’t a robust as something like this should be. Unfortunately it’s
current best practice and there isn’t an easy way to improve. Let me know if you
know a better way.&lt;/p&gt;
&lt;p&gt;There is also &lt;a href=&quot;https://github.com/itskoko/kubecfn/blob/e3cf1d21db72ca5467ac6f43abc02c5871fd444a/kubernetes.yaml#L247&quot;&gt;the nasy
workaround&lt;/a&gt;
of patching the kube-proxy configmap after &lt;code&gt;kubeadm init&lt;/code&gt; which requires
upstream fixes.&lt;/p&gt;
&lt;p&gt;That being said, thanks to CloudFormation with primitives like
&lt;a href=&quot;/2015/04/27/cloudformation-driven-consul-in-autoscalinggroup/&quot;&gt;WaitOnResourceSignals&lt;/a&gt;
and features like &lt;a href=&quot;https://github.com/itskoko/kubecfn#dry-run&quot;&gt;Change Sets&lt;/a&gt;, I
feel more comfortable operating this cluster than I was with any other installer
I used in the past.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Production Grade Kubernetes]]></title><description><![CDATA[About a year ago I blogged about how to build a  $15 Production Kubernetes
Cluster on DigitalOcean 
and submitted it to  Hacker News . HN…]]></description><link>https://5pi.de//2017/12/15/production-grade-kubernetes/</link><guid isPermaLink="false">https://5pi.de//2017/12/15/production-grade-kubernetes/</guid><pubDate>Fri, 15 Dec 2017 14:08:29 GMT</pubDate><content:encoded>&lt;p&gt;About a year ago I blogged about how to build a &lt;a href=&quot;/2016/11/20/15-producation-grade-kubernetes-cluster/&quot;&gt;$15 Production Kubernetes
Cluster on DigitalOcean&lt;/a&gt;
and submitted it to &lt;a href=&quot;https://news.ycombinator.com/&quot;&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;HN being HN, soon after these comments trickled in:&lt;/p&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-26951.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 560px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 20.357142857142858%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAAsTAAALEwEAmpwYAAAArElEQVQI101P2QqDQBDz/7+ql9e6QqXFB31o672HuuvtWyMLpRAyYYbMZKxpaue505p3XdP3jVJca6EUG0e5LD1GAMRP/3esqnpzXtT1ByxE2TQZUJYv+LdNG6yrMmLfB8PoHGYUHHTdM6VuHN/D0PX9a5o+Pe8SRQGlNqUOIbckeUQRIcR2nFMQ2IxlMFomQNtWUpZSVrjPeY510AYYAQgCRjQhCsbyYTie+gKvHdkxzMfpiwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;What people call production nowadays... - pst&quot;
        title=&quot;&quot;
        src=&quot;/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-26951.png&quot;
        srcset=&quot;/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-b2ad5.png 148w,
/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-2283b.png 295w,
/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-26951.png 560w&quot;
        sizes=&quot;(max-width: 560px) 100vw, 560px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    .&lt;/p&gt;
&lt;p&gt;Fair enough. If you only need $15 worth of resources, running a three node
Kubernetes cluster might not be the best idea. As explained in that article, I
was more referring to the way it’s deployed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Highly available: Clustered etcd, multiple master/controller instances&lt;/li&gt;
&lt;li&gt;Secure: TLS for etcd clients and peers and apiserver+kubelet&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Personally I built it mainly to have a Kubernetes playground. But in the
meanwhile I founded &lt;a href=&quot;https://latency.at&quot;&gt;Latency.at&lt;/a&gt;. It’s a service to measures
performance and availability of sites and services from multiple global
locations and provides the results as Prometheus metrics. And of course it’s
running on Kubernetes, so I didn’t have the need for a playground and migrated
this blog to &lt;a href=&quot;https://github.com/gatsbyjs/gatsby&quot;&gt;Gatsby.js&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I’m also &lt;a href=&quot;/hire-me/&quot;&gt;consulting&lt;/a&gt; people on, among other things, how to build
production grade Kubernetes infrastructure. For this I looked into various way
to deploy Kubernetes today. There are &lt;em&gt;many&lt;/em&gt; and most claimed to be “production
grade”:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/kops&quot;&gt;https://github.com/kubernetes/kops&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/coreos/tectonic-installer&quot;&gt;https://github.com/coreos/tectonic-installer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/aws-quickstart/quickstart-heptio&quot;&gt;https://github.com/aws-quickstart/quickstart-heptio&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes-incubator/kube-aws&quot;&gt;https://github.com/kubernetes-incubator/kube-aws&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes-incubator/kubespray&quot;&gt;https://github.com/kubernetes-incubator/kubespray&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&quot;&gt;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Highly available?&lt;/h3&gt;
&lt;p&gt;In my book, having a highly available cluster is a strict requirement for
production deployments. You might have specialized use case where this isn’t
necessary but if you run realtime, business critical applications this is a
requirement.&lt;/p&gt;
&lt;p&gt;While an outage of the controller components won’t affect running applications,
you can’t operate the cluster anymore: If your ingress controller gets restarted
during that time, it won’t know about your backends. If an important pod dies,
nobody will restart it.&lt;/p&gt;
&lt;p&gt;This requirement already rules out one of the most popular options:
&lt;a href=&quot;https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/&quot;&gt;kubeadm&lt;/a&gt;.
At least it doesn’t claim to be stable yet and everything except this looks very
promising.&lt;/p&gt;
&lt;p&gt;The same limitation applies to projects built upon kubeadm, like
&lt;a href=&quot;https://github.com/kris-nova/kubicorn&quot;&gt;kubicorn&lt;/a&gt; and the &lt;a href=&quot;https://github.com/aws-quickstart/quickstart-heptio&quot;&gt;hepio cloudformation
templates&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Secure?&lt;/h3&gt;
&lt;p&gt;A Kubernetes cluster can be “(in)secure” on multiple layers. First etcd should
require peers and client certificates to be signed by a trusted CA. Next the
apiserver needs to verify that the certificate of the etcd endpoint it connects
to is signed by a trusted CA, as well as certificates of clients connecting to
the apiserver. Now the kubelet connecting to the apiserver needs to validate
that certificate too.&lt;/p&gt;
&lt;p&gt;Beside the transport level security provided by TLS, Kubernetes supports RBAC to
limit the access pods in the cluster have to the Kubernetes API. This needs to
be enabled too.&lt;/p&gt;
&lt;p&gt;Again, in my option all these are production requirements. Keep in mind that
just the ability to run a privileged container or mount a volume is enough to
compromise your infrastructure. This can be done on each of these layers with
different effort. Not require TLS on any connection should be consider missing
no authentication at all.&lt;/p&gt;
&lt;p&gt;There are setups which don’t require etcd TLS but limit it’s reachability to
controller nodes, then using tains to prevent “untrusted” containers to get
scheduled on the masters. While this is better than nothing, it’s still a risk
not worth taking.&lt;/p&gt;
&lt;p&gt;Unfortunately none of the more light-weight options fulfill these requirements.
Frankly, there are so many ‘installers’ out there, I can’t say this for sure but
the most popular options like kops are lacking. When running with calico
networking, it even requires etcd to be writable from all nodes without
authentication. Other options like kubespray don’t support full TLS either. Not
even the tectonic-installer, which in general looks very promising, supports TLS
out the box by default.&lt;/p&gt;
&lt;h3&gt;(Self-)Sufficiency?&lt;/h3&gt;
&lt;p&gt;The most common Kubernetes setups are not self-sufficient but depend often on
multiple external Docker registries and components during runtime. Since usually
these are referred to by mutable tags, there is also no way to guarantee that
nobody changes a dependency without you noticing.
Unfortunately this is something rarely discussed and there isn’t any
light-weight tooling I came across helping with this.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;I can’t claim I looked into every Kubernetes installer project and ignored
complex “enterprise” stacks like &lt;a href=&quot;http://www.projectatomic.io&quot;&gt;Red Hat’s Atomic&lt;/a&gt;
or &lt;a href=&quot;https://jujucharms.com/canonical-kubernetes/&quot;&gt;Canonical Kubernetes Juju&lt;/a&gt; but
I’m surprised how many different options there are, yet how few of them provide
what I’m looking for: A simple, immutable, secure and available cluster.&lt;/p&gt;
&lt;p&gt;As of right now, the
&lt;a href=&quot;https://github.com/coreos/tectonic-installer&quot;&gt;tectonic-installer&lt;/a&gt; looks like
the best option, especially if you need to deploy on bare metal or openstack.
But it’s a fast moving project and it could become a drag to keep up with
upstream changes. Another thing leaving a bad taste is that the component for
automated updates is closed source and not available for free.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes/kubeadm/issues/261&quot;&gt;kubeadm works on HA&lt;/a&gt; but I
wouldn’t be surprised if it takes another year until this works reliably.
Fortunately you can build upon kubeadm to create secure and HA clusters. Since
this, to me, appears to be the best option, I implemented this based on
cloudformation for AWS for one of my clients. Hopefully I can share the results
as open source soon. For now you can find valuable hints on &lt;a href=&quot;https://github.com/kubernetes/kubeadm/issues/546&quot;&gt;this GitHub
issue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But should you even run your own Kubernetes cluster? Probably not! Kubernetes is
a incredibly fast moving project. As a rule of thumb, I’d say operating a
Kubernetes cluster is a full time job. Don’t assume any of the installers will
free you from developing a deep understanding of Kubernetes’ internals. If you
operate your own cluster, be prepared to read source code and fix bugs yourself.
This is especially true with all the managed solutions and &lt;a href=&quot;https://cloud.google.com/kubernetes-engine/&quot;&gt;Google Kubernetes
Engine&lt;/a&gt; now being available for
free.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Use Prometheus Vector Matching to get Kubernetes Utilization across any Pod Label]]></title><description><![CDATA[Prometheus was designed for dynamic environments like Kubernetes. Its powerful
service discovery and query language allows you to answer all…]]></description><link>https://5pi.de//2017/11/09/use-prometheus-vector-matching-to-get-kubernetes-utilization-across-any-pod-label/</link><guid isPermaLink="false">https://5pi.de//2017/11/09/use-prometheus-vector-matching-to-get-kubernetes-utilization-across-any-pod-label/</guid><pubDate>Wed, 08 Nov 2017 23:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Prometheus was designed for dynamic environments like Kubernetes. Its powerful
service discovery and query language allows you to answer all kind of questions
that come up while operating a Kubernetes cluster.&lt;/p&gt;
&lt;p&gt;This flexibility comes with a somewhat steeper learning curve than some
alternatives and hosted services, with &lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/querying/operators/#vector-matching&quot;&gt;Vector
Matching&lt;/a&gt;
being one of the more advanced topics.&lt;/p&gt;
&lt;p&gt;I’m not going into the details which the excellent &lt;a href=&quot;https://prometheus.io/docs/prometheus/latest/querying/operators/#vector-matching&quot;&gt;Prometheus
Documentation&lt;/a&gt;
has already covered but walk you through some useful queries to get resource
utilization in a Kubernetes cluster.&lt;/p&gt;
&lt;h2&gt;Aggregated memory usage per label&lt;/h2&gt;
&lt;p&gt;Kubernetes provides a &lt;code&gt;container_memory_usage_bytes&lt;/code&gt; metric reflecting each pods
memory usage, e.g:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;...
container_memory_usage_bytes{beta_kubernetes_io_arch=&quot;amd64&quot;,beta_kubernetes_io_fluentd_ds_ready=&quot;true&quot;,beta_kubernetes_io_instance_type=&quot;g1-small&quot;,beta_kubernetes_io_os=&quot;linux&quot;,cloud_google_com_gke_nodepool=&quot;small-preemptible&quot;,cloud_google_com_gke_preemptible=&quot;true&quot;,container_name=&quot;POD&quot;,failure_domain_beta_kubernetes_io_region=&quot;us-east1&quot;,failure_domain_beta_kubernetes_io_zone=&quot;us-east1-c&quot;,id=&quot;/kubepods/burstable/pod13d4221c-c484-11e7-bff5-42010af0018b/67e5bb069ab9881ff8a55b8628ef4935b0d1ace09c18df20db059522bdfd5b7d&quot;,image=&quot;gcr.io/google_containers/pause-amd64:3.0&quot;,instance=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,job=&quot;kubernetes-cadvisor&quot;,kubernetes_io_hostname=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,name=&quot;k8s_POD_latency-api-971504058-jzs5h_default_13d4221c-c484-11e7-bff5-42010af0018b_0&quot;,namespace=&quot;default&quot;,pod_name=&quot;latency-api-971504058-jzs5h&quot;}	389120
container_memory_usage_bytes{beta_kubernetes_io_arch=&quot;amd64&quot;,beta_kubernetes_io_fluentd_ds_ready=&quot;true&quot;,beta_kubernetes_io_instance_type=&quot;g1-small&quot;,beta_kubernetes_io_os=&quot;linux&quot;,cloud_google_com_gke_nodepool=&quot;small-preemptible&quot;,cloud_google_com_gke_preemptible=&quot;true&quot;,container_name=&quot;POD&quot;,failure_domain_beta_kubernetes_io_region=&quot;us-east1&quot;,failure_domain_beta_kubernetes_io_zone=&quot;us-east1-c&quot;,id=&quot;/kubepods/burstable/pod81d0f651-c500-11e7-bff5-42010af0018b/309e05b118e618122c70ccf88538d13ca41c3b5a770d5d67882426854391c23c&quot;,image=&quot;gcr.io/google_containers/pause-amd64:3.0&quot;,instance=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,job=&quot;kubernetes-cadvisor&quot;,kubernetes_io_hostname=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,name=&quot;k8s_POD_latency-api-971504058-gszpw_default_81d0f651-c500-11e7-bff5-42010af0018b_0&quot;,namespace=&quot;default&quot;,pod_name=&quot;latency-api-971504058-gszpw&quot;}	372736
container_memory_usage_bytes{beta_kubernetes_io_arch=&quot;amd64&quot;,beta_kubernetes_io_fluentd_ds_ready=&quot;true&quot;,beta_kubernetes_io_instance_type=&quot;g1-small&quot;,beta_kubernetes_io_os=&quot;linux&quot;,cloud_google_com_gke_nodepool=&quot;small-preemptible&quot;,cloud_google_com_gke_preemptible=&quot;true&quot;,container_name=&quot;latency-api&quot;,failure_domain_beta_kubernetes_io_region=&quot;us-east1&quot;,failure_domain_beta_kubernetes_io_zone=&quot;us-east1-c&quot;,id=&quot;/kubepods/burstable/pod13d4221c-c484-11e7-bff5-42010af0018b/497e6fdf2217771cb3f52e6fef93734d023f0e7f23f92c58d22139fc18dc5f13&quot;,image=&quot;registry.gitlab.com/latency.at/latencyat@sha256:8ea057e064b64cc9c8459a68ef3f6d0fc26169b4f57aef193831779e1fe713d4&quot;,instance=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,job=&quot;kubernetes-cadvisor&quot;,kubernetes_io_hostname=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,name=&quot;k8s_latency-api_latency-api-971504058-jzs5h_default_13d4221c-c484-11e7-bff5-42010af0018b_1&quot;,namespace=&quot;default&quot;,pod_name=&quot;latency-api-971504058-jzs5h&quot;}	11014144
container_memory_usage_bytes{beta_kubernetes_io_arch=&quot;amd64&quot;,beta_kubernetes_io_fluentd_ds_ready=&quot;true&quot;,beta_kubernetes_io_instance_type=&quot;g1-small&quot;,beta_kubernetes_io_os=&quot;linux&quot;,cloud_google_com_gke_nodepool=&quot;small-preemptible&quot;,cloud_google_com_gke_preemptible=&quot;true&quot;,container_name=&quot;latency-api&quot;,failure_domain_beta_kubernetes_io_region=&quot;us-east1&quot;,failure_domain_beta_kubernetes_io_zone=&quot;us-east1-c&quot;,id=&quot;/kubepods/burstable/pod81d0f651-c500-11e7-bff5-42010af0018b/7b438a8e9df0cf1ab29d067fd36c97099f9f5e7e9257f6187c5be6bff846a62c&quot;,image=&quot;registry.gitlab.com/latency.at/latencyat@sha256:8ea057e064b64cc9c8459a68ef3f6d0fc26169b4f57aef193831779e1fe713d4&quot;,instance=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,job=&quot;kubernetes-cadvisor&quot;,kubernetes_io_hostname=&quot;gke-latency-at-small-preemptible-0c981b61-9489&quot;,name=&quot;k8s_latency-api_latency-api-971504058-gszpw_default_81d0f651-c500-11e7-bff5-42010af0018b_0&quot;,namespace=&quot;default&quot;,pod_name=&quot;latency-api-971504058-gszpw&quot;}	11448320
...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Unfortunately it doesn’t contain the pod labels. For that there is the
&lt;code&gt;kube_pod_labels&lt;/code&gt; though which includes a static timeseries with all labels and
the pod name. This metric is provided by
&lt;a href=&quot;https://github.com/kubernetes/kube-state-metrics&quot;&gt;kube-state-metrics&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can use it to look up the labels for the pod from the
timeseries (&lt;code&gt;pod_name=&quot;latency-api-971504058-jzs5h&quot;&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;kube_pod_labels{instance=&quot;10.116.0.12:8080&quot;,job=&quot;kubernetes-service-endpoints&quot;,k8s_app=&quot;kube-state-metrics&quot;,kubernetes_name=&quot;kube-state-metrics&quot;,kubernetes_namespace=&quot;kube-system&quot;,label_app=&quot;latency-api&quot;,label_pod_template_hash=&quot;971504058&quot;,namespace=&quot;default&quot;,pod=&quot;latency-api-971504058-jzs5h&quot;} 1
kube_pod_labels{instance=&quot;10.116.1.26:8080&quot;,job=&quot;kubernetes-service-endpoints&quot;,k8s_app=&quot;kube-state-metrics&quot;,kubernetes_name=&quot;kube-state-metrics&quot;,kubernetes_namespace=&quot;kube-system&quot;,label_app=&quot;latency-api&quot;,label_pod_template_hash=&quot;971504058&quot;,namespace=&quot;default&quot;,pod=&quot;latency-api-971504058-jzs5h&quot;} 1&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Both metrics can be merged by using vector matching. We get this metric twice
because there are two instances of kube-state-metrics running. So before we
continue further, we aggregate both. Since they should be identical, we can use
min/max to aggregate. Since we later want to aggregate on &lt;code&gt;label_app&lt;/code&gt;, we need
to keep that label. We also need to keep the &lt;code&gt;pod&lt;/code&gt; label to join both metrics
on. We can preserve them by specifying them in the BY clause. Since the pod
label is called &lt;code&gt;pod&lt;/code&gt; in &lt;code&gt;kube_pod_labels&lt;/code&gt; but &lt;code&gt;pod_name&lt;/code&gt; in
&lt;code&gt;container_memory_usage_bytes&lt;/code&gt;, we need to also use label_replace to rename the
label. This gives us:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;max by (pod_name,label_app) (
  label_replace(kube_pod_labels{label_app!=&quot;&quot;},&quot;pod_name&quot;,&quot;$1&quot;,&quot;pod&quot;,&quot;(.*)&quot;)
)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Which returns something like:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;...
{label_app=&quot;latency-api&quot;,pod_name=&quot;latency-api-971504058-n8k6d&quot;}  1
{label_app=&quot;latency-api&quot;,pod_name=&quot;latency-api-971504058-jzs5h&quot;}  1
...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Now we can use vector matching to join &lt;code&gt;container_memory_usage_bytes&lt;/code&gt; with the
result of our expression. We use the &lt;code&gt;*&lt;/code&gt; operator which effectivly is a no-op
since it will multiply the memory usage by the matched timeseries value in
&lt;code&gt;kube_pod_labels&lt;/code&gt; which is always 1.&lt;/p&gt;
&lt;p&gt;We need to specify &lt;code&gt;group_left&lt;/code&gt; because there are multiple
&lt;code&gt;container_memory_usage_bytes&lt;/code&gt; timeseries for each pod, one for each container.
Since we want to preserve the &lt;code&gt;label_app&lt;/code&gt; label, we specify it as argument to &lt;code&gt;group_left&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This gives us:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;  container_memory_usage_bytes * on (pod_name) group_left(label_app)
  max by (pod_name,label_app) (
    label_replace(kube_pod_labels{label_app!=&quot;&quot;},&quot;pod_name&quot;,&quot;$1&quot;,&quot;pod&quot;,&quot;(.*)&quot;)
  )&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;To aggregated the memory usage of all your pods, summed by a &lt;code&gt;k8s-app&lt;/code&gt; label you
can use the following expression:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;sum by (label_app,namespace) (
  container_memory_usage_bytes * on (pod_name) group_left(label_app)
  max by (pod_name,label_app) (
    label_replace(kube_pod_labels{label_app!=&quot;&quot;},&quot;pod_name&quot;,&quot;$1&quot;,&quot;pod&quot;,&quot;(.*)&quot;)
  )
)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-0a8a9.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 79.38144329896907%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAACRElEQVQ4y5VTW24TQRDcg/jLf/6whOADJ1Yi28hPya/4Br4HCgQQQoJTBCkovgC5gi9BHNsB4pAEvLszs48punv9iKMEhZFqe2Z7pra6ptex1iIMQyilJGqt4fs+XNfFfD6HMWaVZ/Ca857ngc/ehcOPwASCOI4RRZGASYIgkDm/5zVjmQuDZH9sYyGSOcHhF9pX0J7CclhKwGI97O3FeghJRB/TAYzSiMIIzkqNNtCuokhJT8P4Wg7YW8x8OKZDgTIbiLmKaKlwIZUPhmZZYgR/7opqJg/IN+35Ugkr4XLDhR08oihcYKFQa9rIMKTMKIFSHnzPxfXPGZTvCZI8fSAg0jCQPXxOa1/AlTpsPN+cfcAji/v9W9trN/Ynt8ztQGoMtUISk3nA0EbyEZtO8wTkL4M7Q0DvaA976djY4ub3DS5+zTC7usSlYIaLK17PqN/m+ONeY/JjjPH5GJPvU4oTjM5H+DY5xel0hLPpGUYUPeXDYanD4RCHnw8xGAwEx6t4jKMvRzg5+brwlbxij8lDbbTMZW2SdUSXKYSv9l/i2dMnKBZL2N3dFRQKBeRyOaTTadRqNWmLxwwhfP3mHfL5HbRaTdTrdTQaDUGxWEQmk0Gn01m1yH2/28avx5vevj9AfmdLSCqVCqrVqqhihalUCu12+z8JD4hwe1sImWxJuiy52WxKS9xtkwdL3ifC51ukkMplMlZXKpWEMJvNotvtStM+mvDTxw94USpgb68nfjFarRZ6vR7K5TL6/f5Gyf8afwEmkHH4kHQEfAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;mem_per_app&quot;
        title=&quot;&quot;
        src=&quot;/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-fb8a0.png&quot;
        srcset=&quot;/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-1a291.png 148w,
/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-2bc4a.png 295w,
/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-fb8a0.png 590w,
/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-526de.png 885w,
/static/mem_per_app-0f0a37acb794c49740014ce150e3ae1f-0a8a9.png 970w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;h2&gt;CPU and IO usage per label&lt;/h2&gt;
&lt;p&gt;Now that we know how to join the &lt;code&gt;kube_pod_labels&lt;/code&gt; metric with cadvisor metrics,
we can figure out usage of other resources too.&lt;/p&gt;
&lt;h3&gt;CPU&lt;/h3&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;sum by (label_app,namespace) (
  rate(container_cpu_usage_seconds_total[2m]) * on (pod_name) group_left(label_app)
  max by (pod_name,label_app) (
    label_replace(kube_pod_labels{label_app!=&quot;&quot;},&quot;pod_name&quot;,&quot;$1&quot;,&quot;pod&quot;,&quot;(.*)&quot;)
  )
)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-0a8a9.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 79.38144329896907%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAACR0lEQVQ4y5WTzW4SURTHeZCu2LEgUTfSEkAwUCDhq7wB72Eaq8aY6FvYpCZ04cpAxYWbmhJakKqgViENlE7LlzPADF9/7zkwLSWEtDf555577p3fPeeeM4bxeIzBYIBut8uzqqpsK4oCWZahaRr7e70ei9a03+l0QN/OyzAcDq8+IptmUr/fZ5Fv1q/btDcajXg9Oxv0AzTohmVjfl+PamITdHANXBT+vCiCRX4a/b7KYiCFfxugHuAi4L/2mXjXxvIIaWhaR6Si8VpTFVGQhjivTf19UUSZdVbNQ5Evbr6hqnY5Lf1NJkBF3FwXoBYkqYha7VhUXxIVbwm10WyUcXHxE+XSvjjXnACVtgylJUOqVtC6rPNaqlTQlOoon2RQ+JbCaekQvwop/C5+Qulkf6ovPBe/7+HH8Qd0CUipyO0GGlIVdamCy/Myzmt/2a5Lpyj9SaNQ+Iz81xQymQSyR0nksgmk0++RO0ogn0sie0j+jyJlAaS0DtIH2N5+i93dOCsej1/ZOzvvkEjuceFUlXqT+lE803Ak5tHEnor7kICbm0/w4P49OJ1O2O122Gx2OBwOWCwWGI1GeL1ebtybvTjbk9c2A58+ewGr1YZgMACfzwe/38+iC0wmE8Lh8A3gMjHw+cstWG2rDPF4PFhfX+eoKMKVlRWEQqE7ArcEcG2NgQTToXrKgUBgpp2W/56TNxTAh6siQpEuwSg6l8vFQLPZjEgkcuv/nYFvXr/CY9cjbGxE+b1IwWAQ0WgUbrcbsVhsQVEWj//2cGesm4V6ogAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;cpu_per_app&quot;
        title=&quot;&quot;
        src=&quot;/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-fb8a0.png&quot;
        srcset=&quot;/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-1a291.png 148w,
/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-2bc4a.png 295w,
/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-fb8a0.png 590w,
/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-526de.png 885w,
/static/cpu_per_app-ee8bde4b930678ada8505a9cc8e749ba-0a8a9.png 970w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;h3&gt;Disk IO&lt;/h3&gt;
&lt;p&gt;I wanted to show you some disk IO stats too. Unfortunately these are
&lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/55397&quot;&gt;broken&lt;/a&gt; once
&lt;a href=&quot;https://github.com/kubernetes/kubernetes/issues/55398&quot;&gt;again&lt;/a&gt; in Kubernetes.&lt;/p&gt;
&lt;h3&gt;Network&lt;/h3&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;sum by (label_app,namespace) (
  rate(container_network_transmit_bytes_total[2m]) * on (pod_name) group_left(label_app)
  max by (pod_name,label_app) (
    label_replace(kube_pod_labels{label_app!=&quot;&quot;},&quot;pod_name&quot;,&quot;$1&quot;,&quot;pod&quot;,&quot;(.*)&quot;)
  )
)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-0a8a9.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 79.38144329896907%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAAAsSAAALEgHS3X78AAACEElEQVQ4y5VTS24aQRCdg7BixwIp8ibYyOCZiG/EAOYG3CNCdhJFkZJbIDbmAJyBcySb2HyGAYb59XNVDT0GTCKnpZqu7q5+9epVjxHHMYIgwHa7RRiG2O124q/Xa7iuC9/3Zd/zPDFe8/lms4FS6pUZHKwvRVEkMyfQdrh/GsNkeH04G/xh00PFdEBM/ncwOwFkZM4mlOkgZmauI76iAJ4jb4uY4s6VqE0TMzRd3oQGXC33WRPm4cpBTGVKkj0bnUzfO2LImpwF3EsROEsCDKD25bEfbtwU6AhQN0WXHAW+AOCg5MBZSKI0xk9iUlkOS+aF7lDMPgX7SwKg6CiM5JK/mENF4UsHuOMkw+lINQw5O11QrCX5EZXDmjErVo0TLJ8esZg9wSHwxeMfzH//Et+Zz2ieYUUx3FyDkafTKYbDIcYPDxiPtY1lHo1GmEwmIotH3eaHz37yTv3EuCqymJ6cAA4Gn3Bx8Q6maaFUKomVy2UUCgVks1nU63WSJX7TexTAu89fUSxew7ZbaDQa+NhsoklmmiZyuRw6nY7o/NJVxc7p005+PXa/fLtH8fpSQKrVKmq1mrBihplMBu12+wjwX5YA3hPg1ZUAMpgG1SW3Wq3091SvmJ0peUCA7y+JIZXLYMzOsiwBzOfz6Ha78vjfDPjzx3d8sG5we9sTvdhs20av10OlUkG/3z/R8O/jGTNndueoXiN5AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;net_per_app&quot;
        title=&quot;&quot;
        src=&quot;/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-fb8a0.png&quot;
        srcset=&quot;/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-1a291.png 148w,
/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-2bc4a.png 295w,
/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-fb8a0.png 590w,
/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-526de.png 885w,
/static/net_per_app-9cdc5336080b286047c358a645a9a8ed-0a8a9.png 970w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;</content:encoded></item><item><title><![CDATA[Frontend Testing Zoo - Or, Nightwatch without Selenium]]></title><description><![CDATA[What I always find hardest to figure it when it comes to unknown systems, it’s ‘how things fit together’. These project might have great…]]></description><link>https://5pi.de//2017/04/17/selenium-webdriver-chromedriver-nightwatch-chrome-headless/</link><guid isPermaLink="false">https://5pi.de//2017/04/17/selenium-webdriver-chromedriver-nightwatch-chrome-headless/</guid><pubDate>Mon, 17 Apr 2017 12:50:12 GMT</pubDate><content:encoded>&lt;p&gt;What I always find hardest to figure it when it comes to unknown systems, it’s ‘how things fit together’. These project might have great documentation, but how the ecosystem plays together is usually left as an exercise for the reader.
This happened to me again when diving into frontend testing. I figured out how to get nightwatch+selenium-standalone+chromedriver+chrome working somehow, without fully understanding each part in the puzzle. I went with Nightwatch because I like the syntax and it seems one of the obvious options nowadays. That works until I updated some dependencies and broke everything again. So I took some time to figure out how these things fit together and what is really needed for me tests.
I’ve learned that Selenium is the godmother of modern frontend testing. It’s a framework with bindings for multiple languages. To decouple the server from the actual browsers, it invented “Selenium RC” as a protocol. That lead to the development of the WebDriver W3C standard which replaces Selenium RC is newer versions and is implemented for the most common browsers (Chrome -&gt; Chromedriver, Firefox -&gt; FirefoxDriver).&lt;/p&gt;
&lt;h2&gt;Nightwatch without Selenium&lt;/h2&gt;
&lt;p&gt;Apparently WebDriver is also used for talking &lt;em&gt;to&lt;/em&gt; Selenium. I think this is called ‘hub’ or something, that’s about as deep as my ‘deep dive’ gets for now. Either way, this makes the big picture look like:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;Nightwatch -[webdriver]-&gt; Selenium -[webdriver]-&gt; Chromedriver -&gt; Chrome&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Selenium comes in a server and standalone flavor, where the server apparently is only used for grid based testing where you would have a bunch of Selenium nodes running on different systems. Turns out, if you don’t need grid based testing or support for legacy browsers which don’t provide WebDriver adapter, there is little use for Selenium after all.&lt;/p&gt;
&lt;p&gt;Fortunately you can make Nightwatch talk directly to Chromedriver. This requires start&lt;em&gt;processto be set to false and selenium&lt;/em&gt;host/_port to point the address of Chromedriver:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;{
  &quot;output_folder&quot;: false,
  &quot;src_folders&quot;: [&quot;tests&quot;],
  &quot;selenium&quot;: {
    &quot;start_process&quot;: false
  },
  &quot;test_settings&quot;: {
    &quot;default&quot;: {
      &quot;selenium_host&quot;: &quot;127.0.0.1&quot;,
      &quot;selenium_port&quot;: &quot;9515&quot;,
      &quot;screenshots&quot;: {
        &quot;enabled&quot;: true,
        &quot;on_failure&quot;: true,
        &quot;on_error&quot; : true,
        &quot;path&quot;: &quot;results/screenshots&quot;
      },
      &quot;desiredCapabilities&quot;: {
        &quot;browserName&quot;: &quot;chrome&quot;,
        &quot;javascriptEnabled&quot; : true,
        &quot;acceptSslCerts&quot; : true,
        &quot;chromeOptions&quot; : {
          &quot;args&quot;: [
            &quot;--no-sandbox&quot;,
            &quot;start-fullscreen&quot;,
            &quot;window-size=1280,800&quot;
          ]
        }
      },
      &quot;launch_url&quot;: &quot;http://app&quot;
    }
  }
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;The only caveat being, you need to run Chromedriver manually. I run all this in a Docker container with a entrypoint like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;Xvfb :23 &amp;
chromedriver --port=9515 --url-base=/wd/hub &amp;
while ! curl localhost:9515; do echo -n .; sleep 1; done
nightwatch&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;h2&gt;Chrome Headless&lt;/h2&gt;
&lt;p&gt;Recently Chrome got native support for running headless, controlled via a remote debugging API. Apparently this could further simplify things, but I couldn’t figure out in reasonable time what exactly it would replace. I suspected it would mean that chrome natively talks webdriver, but I can’t find anything about that in the sparse documentation and this issues sound like.
Which brings me to my initial observation: Figuring out how a thing works is hard, figuring out how things work &lt;em&gt;together&lt;/em&gt; is so much harder. Let’s all keep that in mind.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[$15 Production Kubernetes Cluster on DigitalOcean]]></title><description><![CDATA[
 Four sweets or production Kubernetes for a month Introduction As you might already know, I’m into  containers ,  static configuration  and…]]></description><link>https://5pi.de//2016/11/20/15-producation-grade-kubernetes-cluster/</link><guid isPermaLink="false">https://5pi.de//2016/11/20/15-producation-grade-kubernetes-cluster/</guid><pubDate>Sun, 20 Nov 2016 10:36:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a title=&quot;By Charlotte Marillet (originally posted to Flickr as Quatre macarons) [CC BY-SA 2.0 (http://creativecommons.org/licenses/by-sa/2.0)], via Wikimedia Commons&quot; href=&quot;https://commons.wikimedia.org/wiki/File%3AQuatre_macarons%2C_October_2009.jpg&quot;&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-518ec.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 85.71428571428571%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAARABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAQCAwX/xAAXAQADAQAAAAAAAAAAAAAAAAAAAgMB/9oADAMBAAIQAxAAAAHPKuGVblqi6hBPVGn/xAAcEAEAAgEFAAAAAAAAAAAAAAABAgMTABEiMjT/2gAIAQEAAQUCQySYysjDcQy2TLLazg96PS6//8QAGREAAQUAAAAAAAAAAAAAAAAAAQAQERIh/9oACAEDAQE/ARA0qz//xAAWEQADAAAAAAAAAAAAAAAAAAABIDH/2gAIAQIBAT8BFT//xAAaEAEBAAIDAAAAAAAAAAAAAAABABAhEWGx/9oACAEBAAY/AnrK2jXsckzj/8QAHhAAAgEDBQAAAAAAAAAAAAAAAAEhETFBEFFhgaH/2gAIAQEAAT8hmGREk0l1UnVYRfVnkq1mHIaoh7Hi0Nx//9oADAMBAAIAAwAAABC4IED/xAAYEQADAQEAAAAAAAAAAAAAAAAAAREQIf/aAAgBAwEBPxB49B26PP/EABcRAAMBAAAAAAAAAAAAAAAAAAABERD/2gAIAQIBAT8QpuQWf//EAB4QAQABBAIDAAAAAAAAAAAAAAERABAhUTGBQaHR/9oACAEBAAE/ENExM8FS0uhFy7dUM2ycUhUciJCPMUA4hIO9+VmmyXTXcWD3Wx//2Q==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Four tiny Macarones - or one month Kubernetes cluster&quot;
        title=&quot;&quot;
        src=&quot;/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-f8fb9.jpg&quot;
        srcset=&quot;/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-e8976.jpg 148w,
/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-63df2.jpg 295w,
/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-f8fb9.jpg 590w,
/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-85e3d.jpg 885w,
/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-d1924.jpg 1180w,
/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-9452e.jpg 1770w,
/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-518ec.jpg 2268w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/a&gt;
&lt;small&gt;Four sweets or production Kubernetes for a month&lt;/small&gt;&lt;/p&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As you might already know, I’m into &lt;a href=&quot;https://5pi.de/2015/01/08/containerized-infrastructure/&quot;&gt;containers&lt;/a&gt;, &lt;a href=&quot;https://5pi.de/2015/08/31/dont-manage-config-unless-you-have-to/&quot;&gt;static configuration&lt;/a&gt; and &lt;a href=&quot;https://5pi.de/2015/04/22/scope-and-ownership-in-tech-companies/&quot;&gt;self-service infrastructures&lt;/a&gt;. Naturally, I love &lt;a href=&quot;http://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;, which I consider the most promising cluster scheduler around.&lt;/p&gt;
&lt;p&gt;In fact, the biggest reason to use containers is that they make it possible for something like Kubernetes to &lt;em&gt;operate your cluster&lt;/em&gt;. Cluster scheduler like Kubernetes, Mesos or Swarm take care of deploying and moving your applications around without requiring an Operator to allocate resources and redeploy services manually.&lt;/p&gt;
&lt;p&gt;Cluster schedulers are here to stay. They will become as ubiquitous as version control and getting experience with it is something I can encourage everyone in the DevOps world to do. Especially if your job is mainly &lt;em&gt;operating&lt;/em&gt;. Chances are, your job gets automated.&lt;/p&gt;
&lt;p&gt;Getting Kubernetes up &lt;em&gt;somehow&lt;/em&gt; is easy. There are tons of scripts for doing that. But those setups are intended as temporary test environments. Setting up a production environment is much harder and unfortunately not very well documented.&lt;/p&gt;
&lt;p&gt;Reading this you will realize that there are things you might not want to do this way in production and I agree. So I’m sorry if the title is a bit click-baity. The reason I’m still calling this ‘production’ is because this setup is highly available, TLS authenticated and has a way going forward which doesn’t require you to start from scratch like most Containers/Kubernetes getting started guides.&lt;/p&gt;
&lt;p&gt;You can find all code mentioned here: &lt;a href=&quot;https://github.com/5pi&quot;&gt;https://github.com/5pi&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/5pi-Infra-020fc8ece35a34a40c4d951f78df9481.svg&quot; alt=&quot;Infrastructure Diagram&quot;&gt;
Mainly to keep things cheap, I choose &lt;a href=&quot;https://digitalocean.com&quot;&gt;DigitalOcean&lt;/a&gt; and use the smallest $5/month instances. For a highly available cluster, we need at least three hosts.&lt;/p&gt;
&lt;p&gt;Kubernetes doesn’t schedule containers directly, it schedules &lt;a href=&quot;http://kubernetes.io/docs/user-guide/pods/&quot;&gt;Pods&lt;/a&gt; which again can consist of multiple containers sharing the same storage volumes and IPs. The pods running on different hosts need to communicate with each other. There are several options to do this, like using overlay networks, routing IP ranges to each server on you routers or co-locating the servers on the same ethernet segment.&lt;/p&gt;
&lt;p&gt;Since static routes are least complex to setup and maintain, while allowing to grow easier than with one ethernet segment for all pod IPs, it’s the option I choose here.&lt;/p&gt;
&lt;p&gt;Because on DigitalOcean there is no way to have custom routes, I’m using &lt;a href=&quot;https://www.tinc-vpn.org/&quot;&gt;tinc&lt;/a&gt; to form a private, flat ethernet segment and route a /24 to each host.&lt;/p&gt;
&lt;p&gt;To consider the infrastructure immutable, we need to store state externally. Fortunately DigitalOcean just released their &lt;a href=&quot;https://www.digitalocean.com/products/storage/&quot;&gt;block storage&lt;/a&gt; product which we use to store pod volumes on.&lt;/p&gt;
&lt;p&gt;We will also create DNS names to make accessing the cluster easier:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;master0X.[domain] points to a hosts internal IP; Used for tinc to connect to peers&lt;/li&gt;
&lt;li&gt;edge0X.[domain] points to a hosts public IP; Used for remote access etc&lt;/li&gt;
&lt;li&gt;edge.[domain] points to edge float IP; Used to reach LB directly&lt;/li&gt;
&lt;li&gt;*.edge.[domain] also points to edge float IP; Used for virtual hosts on LB&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Kubernetes Deployment&lt;/h2&gt;
&lt;h3&gt;Configuration&lt;/h3&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/dgfb2mik71by4tkt5urd9lasj.js&quot; id=&quot;asciicast-dgfb2mik71by4tkt5urd9lasj&quot; async&gt;&lt;/script&gt;
&lt;p&gt;The &lt;a href=&quot;https://github.com/5pi/infra&quot;&gt;infra repository&lt;/a&gt; contains all sources needed to build the images and deploy the stack. Beside the committed configuration in the repository, there is also some cluster specific configuration required. While some of it only affects the cluster deployment, other is included in the images. This means changing this always requires rebuilding the images.
The configuration is kept in &lt;a href=&quot;https://github.com/5pi/infra/tree/master/config&quot;&gt;config/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To create a new cluster, first checkout the repo, edit &lt;code&gt;config/env&lt;/code&gt; and run &lt;code&gt;./mk_credentials&lt;/code&gt; to create credentials:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;$ git clone git@github.com:5pi/infra.git
$ cd infra
$ vi config/env
$ ./mk_credentials&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;h3&gt;Building Images&lt;/h3&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/786ffzwvjdupff56ka5258ckk.js&quot; id=&quot;asciicast-786ffzwvjdupff56ka5258ckk&quot; async&gt;&lt;/script&gt;
&lt;p&gt;&lt;tty-player autoplay controls loop src=packer-k8s-build.rec&gt;&lt;/tty-player&gt;
I’m using &lt;a href=&quot;https://www.packer.io&quot;&gt;packer&lt;/a&gt; to build machine images. Since all our hosts are controller+worker nodes, we only have one image. This image includes tinc, etcd, kubernetes and the &lt;a href=&quot;https://prometheus.io&quot;&gt;Prometheus&lt;/a&gt; node-exporter for monitoring. Beside installing general configuration files and services, the &lt;a href=&quot;https://github.com/5pi/infra/blob/e8ccca5de9a5d7759c40355c860d3ed13e349fc7/packer/base.json#L22&quot;&gt;packer config&lt;/a&gt; also refers the cluster specific configuration. This means for each cluster you need to build custom images. It does &lt;strong&gt;not&lt;/strong&gt; include host specific configuration like a host’s TLS keys, or configuration that simply isn’t available before deployment, like a host’s IP address. This need to be configured at deployment.&lt;/p&gt;
&lt;p&gt;As described in the &lt;a href=&quot;https://github.com/5pi/infra#deploying-a-new-stack&quot;&gt;README&lt;/a&gt;, to build the images run &lt;code&gt;make -C packer&lt;/code&gt; after you created the configuration and credentials.&lt;/p&gt;
&lt;h3&gt;Deploying Stack&lt;/h3&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://asciinema.org/a/1hpbxy5pcvmidxxuu1u6w39vr.js&quot; id=&quot;asciicast-1hpbxy5pcvmidxxuu1u6w39vr&quot; async&gt;&lt;/script&gt;
&lt;p&gt;Deployment is configured in &lt;a href=&quot;https://github.com/5pi/infra/tf&quot;&gt;tf/&lt;/a&gt; and uses &lt;a href=&quot;https://www.terraform.io&quot;&gt;terraform&lt;/a&gt;. It spins up hosts with the specified image and allows the provided ssh key to connect. It also creates the DNS records required for tinc to connect to peers and external users to access services on the cluster.&lt;/p&gt;
&lt;p&gt;Since we didn’t want to include all necessary configuration and credentials in the image for flexibility and security, we need to upload the remaining configuration, like TLS keys, after spinning up the host. To set general configuration, &lt;code&gt;/etc/environment.tf&lt;/code&gt; is created and can be sourced by scripts in the image to get deploy-time configuration.&lt;/p&gt;
&lt;p&gt;To deploy a new cluster, you first probably want to change &lt;code&gt;tf/id_rsa.pub&lt;/code&gt; to include your SSH public key. You need to run ssh-agent and the key needs to be added to it’s keyring with &lt;code&gt;ssh-add&lt;/code&gt;. The key may not exist on DigitalOcean already. You can run &lt;code&gt;ssh-keygen -f id_rsa &amp;#x26;&amp;#x26; ssh-add id_rsa&lt;/code&gt; in &lt;code&gt;tf/&lt;/code&gt; to create a new keypair.&lt;/p&gt;
&lt;p&gt;You also need to create the domain you specified in &lt;code&gt;config/env&lt;/code&gt; in DigitalOcean before proceeding. For whatever reason DO requires you to attach the records to some droplet. It doesn’t really matter which one.&lt;/p&gt;
&lt;p&gt;To spin up the configured cluster with the built image, run:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;./terraform apply -var cluster_state=new -var &apos;image=&quot;image-id-from-last-step&quot;&apos;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;Be careful to quote the image parameter properly like described here). &lt;code&gt;cluster_state&lt;/code&gt; is required to make etcd not wait for consensus before spinning up the next instance.&lt;/small&gt;&lt;/p&gt;
&lt;p&gt;That’s it! After a few minutes, the cluster should be up and running.&lt;/p&gt;
&lt;p&gt;If you created a new domain for your cluster, it may take some time until the cluster is formed. If you run &lt;code&gt;journalctl -fu tinc@default&lt;/code&gt; to watch the tinc log you should see that the DNS records are not resolvable yet. This should fix itself after a few minutes.
Once tinc is running, etcd should reach it peers. Run &lt;code&gt;journalctl -fu etcd&lt;/code&gt; to see the etcd log.&lt;/p&gt;
&lt;h3&gt;Rolling Upgrades&lt;/h3&gt;
&lt;p&gt;Another thing most getting started guides are missing is upgrades. Most likely because they are often very environment specific and, well, hard. Which is also why while upgrades &lt;em&gt;should&lt;/em&gt; work, this is the most brittle part of it and one of the reason I hesitated to call it “Production Grade”. But heck, if Docker 1.0 was &lt;em&gt;production ready&lt;/em&gt; this here is as well.&lt;/p&gt;
&lt;p&gt;Since all pod volumes get stored on DO block storage, we consider the systems immutable. To upgrade the cluster, all instances get replaced while Kubernetes makes sure to reschedule services and maintain their availability. The tricky part is to orchestrate this with terraform which does &lt;a href=&quot;https://github.com/hashicorp/terraform/issues/2896&quot;&gt;not really support this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Because of that we need a &lt;a href=&quot;https://github.com/5pi/infra/blob/master/tf/upgrade&quot;&gt;wrapper script&lt;/a&gt;. This script stops etcd on the first old server, which will cause it to remove itself from the cluster. Then it executes terraform and sets &lt;code&gt;-target&lt;/code&gt; for each server individually. Unless &lt;code&gt;cluster_state=new&lt;/code&gt; is given, the new instance’s provisioning script will block until etcd joined the existing cluster and the cluster is healthy. Only after that, the script continues with the next instance.&lt;/p&gt;
&lt;p&gt;Removing an instance from the cluster before replacing it is required. Otherwise the replacement instance can’t join the cluster. This means, if an instance ever dies, you need to manually remove it from the cluster with &lt;code&gt;etcd member remove&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Deployment on Kubernetes&lt;/h2&gt;
&lt;p&gt;A cluster without services doesn’t make much sense, so I’ll also show quickly how to deploy services to the cluster and make them accessible. I’m using just a bunch of yaml files I can apply with &lt;code&gt;kubectl apply -f&lt;/code&gt;: &lt;a href=&quot;https://github.com/5pi/services&quot;&gt;https://github.com/5pi/services&lt;/a&gt;
They are pretty specific to my setup, so probably only useful as example. For real reusable components on top of Kubernetes have a look at &lt;a href=&quot;https://github.com/5pi/services&quot;&gt;helm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Beside the public configuration, you need to require to setup some secrets:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;apiVersion: v1
kind: Secret
metadata:
  name: default
type: Opaque
data:
  pg-password:...
  pg-ghost-fish-password: ...
  pg-grafana-password: ...
  do-token: ...
  grafana-gauth-client-secret: ...
  smtp-infra-password: ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;For the passwords, just generate something, base64 encode and put them into a file and apply it&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;do-token&lt;/code&gt; is a DigitalOcean API token required for floating IP and volume configuration&lt;/li&gt;
&lt;li&gt;&lt;code&gt;grafana-gauth-client-secret&lt;/code&gt; is a Google OAuth client secret for Google Auth based Grafana authentication&lt;/li&gt;
&lt;li&gt;&lt;code&gt;smtp-infra-password&lt;/code&gt; is the password for a Google Mail I use to send Prometheus alerts&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;SkyDNS&lt;/h3&gt;
&lt;p&gt;This is a core service providing DNS resolution for pods. I’m using an &lt;a href=&quot;https://github.com/5pi/services/blob/master/01_skydns-rc.yml&quot;&gt;adapted copy&lt;/a&gt; of the &lt;a href=&quot;https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/skydns-rc.yaml.in&quot;&gt;upstream config&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Traffic Tier&lt;/h3&gt;
&lt;p&gt;I’m using &lt;a href=&quot;https://github.com/containous/traefik&quot;&gt;traefik&lt;/a&gt; as reverse proxy / load balancer. It depends on a &lt;code&gt;traefik-config&lt;/code&gt; ConfigMap where I specific TLS keys etc. It runs three replica and assumes a three node cluster, so we run one instance per host. The vhost configuration is part of the application yaml files.&lt;/p&gt;
&lt;p&gt;To have a stable entrypoint into our infrastructure while the host’s IPs change on every rolling upgrade, terraform deployed a floating IP. To assign this IP to any available traefik instance, I’ve created a simple &lt;a href=&quot;https://github.com/5pi/img-do-float-ip&quot;&gt;container image&lt;/a&gt; and this &lt;a href=&quot;https://github.com/5pi/services/blob/master/01_float_ip.yml&quot;&gt;yaml spec&lt;/a&gt; (The IP needs to get changed when using this spec).
This will make sure the float IP is always assigned to one of the running hosts.&lt;/p&gt;
&lt;p&gt;This requires the &lt;code&gt;do-token&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;PostgreSQL&lt;/h3&gt;
&lt;p&gt;PostgreSQL is used as main database powering this blog for instance. The &lt;a href=&quot;https://github.com/5pi/services/blob/master/20_postgres.yml&quot;&gt;configuration&lt;/a&gt; is straight forward but uses my custom DigitalOcean Storage flexvolume plugin. Before using it the first time, the volume needs to get created, attached, formatted and detached again. You can run the &lt;a href=&quot;https://github.com/5pi/infra/blob/master/packer/files/usr/libexec/kubernetes/kubelet-plugins/volume/exec/5pi.de~do-volume/do-volume&quot;&gt;flexvol plugin&lt;/a&gt; to do this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;do-volume create pgdata 10G my postgres volume
do-volume attach &apos;{ &quot;volume&quot;: &quot;pgdata&quot; }&apos;
mkfs.ext3 /dev/disk/by-id/scsi-0DO_Volume_pgdata&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;After that, the yaml file can get applied and postgres should spin up.&lt;/p&gt;
&lt;h3&gt;Ghost&lt;/h3&gt;
&lt;p&gt;Next is Ghost, the blogging platform powering this and &lt;a href=&quot;https://textkrieg.de&quot;&gt;textkrieg.de&lt;/a&gt;. It uses Postgres as database and another DO volume for assets.
Since the official Ghost image can’t be easily configured automatically and is huge, I’m using my own (&lt;a href=&quot;https://alpinelinux.org/&quot;&gt;alpine&lt;/a&gt; based) &lt;a href=&quot;https://hub.docker.com/r/fish/ghost/&quot;&gt;Docker image&lt;/a&gt;.
Although designed as a &lt;em&gt;modern&lt;/em&gt; blogging platform, it doesn’t fit particular well into the new container/12factor world. For example, it assumes that the current working directory is writable. Since we want to run ghost as unprivileged user, we need to make the volume writable by that user. Unfortunately there is no good way to do that. People often create images that run chown as root when starting, then dropping privileges. But this can become time consuming and has possible security implications. The upcoming &lt;a href=&quot;https://github.com/kubernetes/kubernetes/pull/26926&quot;&gt;flexvolume redesign&lt;/a&gt; will fix this issue. For now we need to make the volume world-writable initially:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;do-volume create ghost-fish 1
do-volume attach &apos;{ &quot;volume&quot;: &quot;ghost-fish&quot; }&apos;
mkfs.ext4 /dev/disk/by-id/scsi-0DO_Volume_ghost-fish

mount /dev/disk/by-id/scsi-0DO_Volume_ghost-fish /mnt
mkdir -m 777 /mnt/{apps,data,images} # See 5pi/infra#11
umount /mnt

do-volume detach /dev/disk/by-id/scsi-0DO_Volume_ghost-fish&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Here I choose simplicity over security for the time being since I’m the only operator of this cluster anyway. You might want to make a different trade off.&lt;/p&gt;
&lt;p&gt;Now we still need to create databases and users, another thing that hasn’t been addressed by Kubernetes yet. For that we need to find the Postgres pod, then &lt;code&gt;exec&lt;/code&gt; into it to create database and users:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;kubectl exec -ti postgres-2416409090-jf8uh -- psql -U postgres
create user ghost_fish with password &apos;pwd-from-secrets.yml&apos;;
create database ghost_fish;
grant all privileges on database ghost_fish to ghost_fish;&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Now we can use the yaml file to deploy Ghost. Beside deploying the pods, we also create a &lt;a href=&quot;https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/50_ghost_fish.yml#L13&quot;&gt;Ingress to configure Traefik&lt;/a&gt;. This routes requests for &lt;code&gt;5pi.de&lt;/code&gt; to service &lt;code&gt;ghost-fish&lt;/code&gt; on port 80 which is defined above and maps to the Ghost deployment&lt;/p&gt;
&lt;p&gt;To access the blog, you still need to create an CNAME DNS record matching the Ingress route and pointing to the managed &lt;code&gt;edge.[DOMAIN]&lt;/code&gt; DNS name.&lt;/p&gt;
&lt;h2&gt;Monitoring&lt;/h2&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-7f419.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 61.0479797979798%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsSAAALEgHS3X78AAACAklEQVQozzVR2Y4UMQzsz9jp3HGSdtJJ3+fM7swuAgQC3nlD4v8/ArMIycpRjl2pcrUiP7bl/nhdt/N6vS7L0ufQlTwMQ0oJEUMIdMg5l1KmeR77vA6xy9i2bWWg5kzkFl+PmDBIzjljFErIVGJb0Cj1HxEIIXWYUtRKs7quBONWi9+fza83NydNNYJzamGccggpAlr1D1FMhC/gB5+DBcIYq56eWPbiy2aeZIPU8J1Z1lxHialB5wP141y8k8MnU3JOzoEgoK7AUY7acC1YcI4k0lNil7Sx2mqjpeY1J/o2YgoNr6mOPkLFrHJgiQ+UjKCCVcmZPRollRSK1yw2OHem4OW2XN6Oeu35Utja1cdYN+5SeXCth6+7/n7qnw/74zTfrvrs5ZJ0MMy7sI3N82SPzj2Pbu+bvXdDtHPxoEUlhVhTvPd47/DR4+sQz4y3El+6GLSiL2zTcIzdVtJayj4PS1eOodyWEbSqXJBUX9cXciGmGCN6D6SWFNNKZkmSyC4EtzFQYOMIlySpvlRkUjDOCnLfgfL2PbRqjABJs/mbBSsMaGc12UNvnFLBSKu4qLxzvcctpCXEDZsz4tykCeMeMSgFAAVS59o+5M5j52N2uSvDXHKjdGW1Xkt6mciV8JibjyveJ6Trhy02lkYmxgz7AOcE1xFuk9sHey7hZQGv+R9YFW0nnzb+igAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Grafana&quot;
        title=&quot;&quot;
        src=&quot;/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fb8a0.png&quot;
        srcset=&quot;/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-1a291.png 148w,
/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-2bc4a.png 295w,
/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fb8a0.png 590w,
/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-526de.png 885w,
/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fa2eb.png 1180w,
/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-7f419.png 1584w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;p&gt;Now that all this is running, it also needs to be monitored properly. Of course I’m using &lt;a href=&quot;https://prometheus.io&quot;&gt;Prometheus&lt;/a&gt; for this. The setup is straight forward now that Ingress and Volume configuration is nothing new. It tunes Prometheus to run better on small instances and create a DO Volume to store metrics on. To gather metrics about the services running on Kubernetes, the &lt;a href=&quot;https://github.com/kubernetes/kube-state-metrics&quot;&gt;kube-state-metrics&lt;/a&gt; exporter is deployed. To monitor website response times, I also deploy the &lt;a href=&quot;https://github.com/prometheus/blackbox_exporter&quot;&gt;blackbox-exporter&lt;/a&gt;.
To send out alerts, two instances of the HA Alertmanager are deployed.
&lt;a href=&quot;https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/prometheus/prometheus-config.yml&quot;&gt;This ConfigMap&lt;/a&gt; configures Prometheus, sets up alerts and configures the Alertmanager. The SMTP credentials get passed in at container startup.
The alerts are quite trigger happy, which is fine in such small cluster. In a bigger cluster you need to adjust those to limit noise. In the end you’re not interested even if whole nodes crash as long as Kubernetes can reschedule the pods. In my tiny cluster I would be surprised if a node crashes, so I alert on this for now.&lt;/p&gt;
&lt;p&gt;I’m also using Grafana to show metrics on dashboards. Grafana is configured to use Google OAuth authentication against my Google Apps account. This allows Grafana to be available on the public internet and allow everyone in my Google Apps Org to access it.&lt;/p&gt;
&lt;p&gt;Here is where the wildcard DNS domain comes in handy. Instead of having to add new DNS record for each new service like Grafana, we just access it via the wildcard domain: A request to &lt;code&gt;anything.edge.[DOMAIN]&lt;/code&gt; ends up at the floating IP and is accepted by a traefik instance. So the only thing we need to configure is the host in the &lt;a href=&quot;https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/grafana.yml#L19&quot;&gt;Ingress definition&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Retrospective and Future&lt;/h1&gt;
&lt;p&gt;I’ve started all this a few month ago already with the goal of building a small cluster from scratch. I didn’t want to use tons of bash scripts not running on a specific cloud. Back then there was &lt;a href=&quot;https://github.com/kubernetes/kops&quot;&gt;kops&lt;/a&gt;, which I would recommend to look at first if you’re deploying to AWS.&lt;/p&gt;
&lt;p&gt;Now there is also kubeadm which looks like a promising way to setup a cluster and I might change my deployment to use it instead. I’ll also consider using CoreOS and cloud-config to configure the Kubernetes components, as well as looking deeper into systemd units to coordinate stopping of services and draining of hosts.&lt;/p&gt;
&lt;p&gt;If you’re looking for reusable components on top of Kubernetes, there is &lt;a href=&quot;https://github.com/kubernetes/helm&quot;&gt;Helm&lt;/a&gt; which might be a better option than just keeping the services yaml files around. You might be also interested in CoreOS’s &lt;a href=&quot;https://coreos.com/blog/introducing-operators.html&quot;&gt;Operator&lt;/a&gt; to fully automate Prometheus operations.&lt;/p&gt;
&lt;p&gt;The most important next thing on my TODO list is proper tests though. I want a full integration test which spins up a new cluster, deploys services to it, runs blackbox tests, runs an rolling upgrade and tests that during and after that the cluster services are available.&lt;/p&gt;
&lt;p&gt;Pull requests to make this more generic and fix issues are welcome, but I won’t accept larger changes until I got the tests going.&lt;/p&gt;
&lt;hr&gt;
&lt;h4&gt;Update&lt;/h4&gt;
&lt;p&gt;Feel free to discuss and comment on &lt;a href=&quot;https://news.ycombinator.com/item?id=13006296&quot;&gt;Hacker News&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[The Future: Fabrication Cloud]]></title><description><![CDATA[The Cloud 
Given the ubiquity of “The Cloud” even in mainstream media, you might expect the readers of this blog know what it means. From my…]]></description><link>https://5pi.de//2015/10/05/the-future-fabrication-cloud/</link><guid isPermaLink="false">https://5pi.de//2015/10/05/the-future-fabrication-cloud/</guid><pubDate>Mon, 05 Oct 2015 11:50:49 GMT</pubDate><content:encoded>&lt;h1&gt;The Cloud&lt;/h1&gt;
&lt;p&gt;&lt;a data-flickr-embed=&quot;true&quot;  href=&quot;https://www.flickr.com/photos/horiavarlan/4777129318&quot; title=&quot;Single white cloud on a clear blue sky&quot;&gt;&lt;img src=&quot;https://farm5.staticflickr.com/4079/4777129318_934309e7af_b.jpg&quot; width=&quot;1024&quot; height=&quot;616&quot; alt=&quot;Single white cloud on a clear blue sky&quot;&gt;&lt;/a&gt;&lt;script async src=&quot;//embedr.flickr.com/assets/client-code.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
Given the ubiquity of “The Cloud” even in mainstream media, you might expect the readers of this blog know what it means. From my experience, that isn’t true. Too many consider it a meaningless buzzword, synonym for the Internet or the act of outsourcing handling of precious personal data. It has to do with outsourcing, the Internet and definitely is used as a buzzword but yet, the Cloud is a incredible concept. Indeed, it’s more a concept than anything technical. It’s the idea of abstracting all the nasty details and turning computing resources into a utility without any ramp-up costs.&lt;/p&gt;
&lt;p&gt;Before the Cloud, if you needed compute resources, you needed to build your own datacenter, figure out cooling, order racks and servers, build them together, figure out power supply, buy network gears, contract with a carrier. To make sure you can still operate, do that twice so if one datacenter explodes, you still enough resources available. To make sure you can serve customers world-wide with low latency, do this all over the world.
Cost for all that can runs in the millions and requires large teams to operate it.&lt;/p&gt;
&lt;p&gt;Granted, even before the Cloud, there were alternatives. Colocation centers, where you get all the basic DC setup and just need to take care of servers. Or even just rent out servers. Still, you take care of ordering them, setting them up, configuring network equipment (or have the provider do that) and provisioning of those resources usually takes days. With the Cloud, if you need to run software somewhere, store data somewhere, you can do simply that in the cloud. Depending on the level of abstraction you need, you can run a container in the Cloud, use virtual machines you can customize as you want or have a provider like Heroku run your code directly. Since you share resources with other customers in a Cloud, there is no need for dedicated provisioning which leads to a very scalable cost model: If you don’t require much resources, you almost pay nothing. You can get virtual machines for $5/month. A highly available stack in various regions to reach your customers with low latency is possible for ~$150/month.&lt;/p&gt;
&lt;p&gt;The Cloud is an abstraction, hiding away the physical world so you can focus on your virtual product. It makes writing a web service (almost) as easy as writing an app.&lt;/p&gt;
&lt;h1&gt;Fabrication Cloud&lt;/h1&gt;
&lt;p&gt;&lt;a data-flickr-embed=&quot;true&quot;  href=&quot;https://www.flickr.com/photos/kakissel/6165114664/&quot; title=&quot;3D Printer at the Fab Lab&quot;&gt;&lt;img src=&quot;https://farm7.staticflickr.com/6165/6165114664_5fab6e38ff_b.jpg&quot; width=&quot;1024&quot; height=&quot;678&quot; alt=&quot;3D Printer at the Fab Lab&quot;&gt;&lt;/a&gt;&lt;script async src=&quot;//embedr.flickr.com/assets/client-code.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
I believe that in the future we’ll see the same abstraction for other, physical resources and it will be huge.&lt;/p&gt;
&lt;p&gt;One thing that comes to mind are factories. Imaging we would share factories and everyone can build whatever they have in mind via well defined interfaces without any ramp-up costs. It will not only kill some established companies, it will end “industry” as we know it. There won’t be warehouses of goods and mass products. Not because all consumers start building their own products but because above the abstraction everything is just software, every company will be a software company and software can adapt to customer needs.&lt;/p&gt;
&lt;p&gt;There are already a few companies providing some form of API around production of physical goods. Custom printed merch like T-Shirts and ball pens, cereal mix and custom chocolate bars. But you can’t fabricate your own inventions beyond a custom design.&lt;/p&gt;
&lt;p&gt;We need to come up with something more generic to build physical things from digital data. The first thing which comes to mind is 3D printing. I would argue it’s doing for Fabrication Clouds what virtualization was doing for the compute Cloud: It’s not necessary the only way to do it, but it makes things easier.&lt;/p&gt;
&lt;h1&gt;How to get there&lt;/h1&gt;
&lt;p&gt;And there are already a few 3D printing services like &lt;a href=&quot;http://www.shapeways.com&quot;&gt;http://www.shapeways.com&lt;/a&gt; and &lt;a href=&quot;https://i.materialise.com&quot;&gt;https://i.materialise.com&lt;/a&gt; and what they are doing is amazing. Yet, 3D printers are far from being universal machines. They have some tight restrictions. You can’t (yet) 3D print your cereal mix with them.&lt;/p&gt;
&lt;p&gt;So what will happen next? There won’t be a universal factory able to build everything anytime soon, but if you don’t need to dedicate a factory to a specific product, you can share those resources.
Most electronic companies already build products based on the specification of their customers. The problem are still huge ramp-up costs, lack of continuous automation and well defined APIs. 3D printing can reduce those costs for small product batch series where advancements in robotics will make assembly flexible enough so mass production isn’t necessarily more efficient.&lt;/p&gt;
&lt;p&gt;Yet, given the complexity of the physical world, compared to the simplicity of computers, I don’t think somebody will flip a switch and we’ll suddenly be there. It will grow over time. On the small scale end, 3D printing services will get more and more sophisticated and eventually integrate with flexible manufacturers for parts where 3D printing doesn’t make sense. On the large scale end, factories will get more flexible to react to changing demands, more automation will further increase the flexibility and a growing number of startups will increase the demand in small batch series.&lt;/p&gt;
&lt;p&gt;I’m really not an expert on factories and industrial fabrication, so there are probably tons of problems I missed, but I don’t think the general idea violates any laws of physics and the impact and, it’s capitalism after all, business opportunity for the pioneers in this field is huge.&lt;/p&gt;
&lt;p&gt;Eventually everything will grow together and building a Smartphone, even a Car, will be (almost) as easy as writing an app.&lt;/p&gt;
&lt;p&gt;PS: Oh, and we’ll not only lose tons of jobs, we’ll lose tons of professions and if we don’t fix the system, &lt;a href=&quot;http://www.politico.com/magazine/story/2014/06/the-pitchforks-are-coming-for-us-plutocrats-108014&quot;&gt;the pitchforks will come&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[The Future]]></title><description><![CDATA[Since I was a kid, I was fascinated with sci-fi and today I’m thrilled to live
in the unique age of unprecedenced velocity of invention. A…]]></description><link>https://5pi.de//2015/10/02/the-future/</link><guid isPermaLink="false">https://5pi.de//2015/10/02/the-future/</guid><pubDate>Fri, 02 Oct 2015 10:57:53 GMT</pubDate><content:encoded>&lt;p&gt;Since I was a kid, I was fascinated with sci-fi and today I’m thrilled to live
in the unique age of unprecedenced velocity of invention.&lt;/p&gt;
&lt;p&gt;A few years ago, Elon Musk’s idea of building an electric car that can compete
with regular cars was a moonshot. It was close to impossible, yet he succeeded.&lt;/p&gt;
&lt;p&gt;From todays perspective, this seems like a nobrainer. I found myself recently
thinking: “Boohh, cars for individual transportation is to 1900”. Soon, you
won’t buy cars anymore. (Sure, that’s something Tesla knows as well and they are
still in a good position).&lt;/p&gt;
&lt;p&gt;Beside being incrediably fascinating, this velocity has another advantage: You
can think crazy sci-fi and see just a few years later how close you got. So I
thought I start blogging about a few crazy ideas. Maybe someone even finds
themself inspired by those.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Uploaded thousands of Photos to Google Photos, here are the best classification mistakes]]></title><description><![CDATA[I’m a lazy person who never took time to organize their photos so I got really excited about Google Photos and just finished uploading all…]]></description><link>https://5pi.de//2015/09/02/uploaded-thousands-of-photos-to-google-photos-here-are-the-best-classification-mistakes/</link><guid isPermaLink="false">https://5pi.de//2015/09/02/uploaded-thousands-of-photos-to-google-photos-here-are-the-best-classification-mistakes/</guid><pubDate>Wed, 02 Sep 2015 13:38:13 GMT</pubDate><content:encoded>&lt;p&gt;I’m a lazy person who never took time to organize their photos so I got really excited about Google Photos and just finished uploading all my Photos to it. I used flickr in the past but just dumping photos there, even if I can search for EXIF tag data doesn’t make much difference to storing them on a hard disk in a drawer.&lt;/p&gt;
&lt;p&gt;Google Photo on the other hand &lt;em&gt;understands&lt;/em&gt; what’s going on in a picture.
Google uses a &lt;a href=&quot;https://medium.com/backchannel/how-google-s-new-photos-app-can-tell-cats-from-dogs-ffd651dfcd80&quot;&gt;neural network&lt;/a&gt; which is trained by millions of photos on the web to identify objects in pictures.&lt;/p&gt;
&lt;p&gt;This works very well and when thinking about it’s mistakes, the uncanny feeling just amplifies: They are mistakes, they aren’t bugs. It’s not faulty, it’s just a little unexperienced - almost like a child.
In fact, I would be very interested in a catogorization standoff between a child and Google.&lt;/p&gt;
&lt;h1&gt;Concerts&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-f69b5.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 92.17506631299734%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAASCAYAAABb0P4QAAAACXBIWXMAABYlAAAWJQFJUiTwAAAE/klEQVQ4yx2UeUyUVxTFPzcsKiDDsA0OznzMBsOMMIwOOIMIiDKibBVcA8riXgSCCmqxFRQFlVVRAQFF44IWtVKr1arRRGNs/7Bp2tgabWLamlqVEbBN469f/OPlvPfy8u6599xzhXv37tPU1MSNq/1cPtHNuQMN9DXVc/7gYb6ub6Wr8wpfvYPu3rO0NDbQe/oEPa0t9DQ30F1fT1djC8d27aX1wn3u/AdC/e5qRJWSn3/6kZ6SZVxeNI2+BdGs9PPjoslOSdhyQvfcwVM0kJGawq2rl8jR+lAf6U+RhInj3WkO0qBRpxFbfw3hyIEWYuwzef7HC/bkFNK0YAnNqbOojLDQY02nVOnAqXOiC1KxfmMFd2/eJCM0ktIYG4VGkaxgkUqdjfARPqTEJCM0SJQnyJRYrDYiAnVM8dESNdGXj0UNKV4aRMGbqOAQVEo1Hj6B+Pn5oxgnx9vNC+VH7pgnylGN8GCkIJDgsCO0HTqMQi7HqNUTFqIjMkRDuBiCebKITQwlXG0kQmvAHhFFRIjIFI0Gs1aHVSuhWiRCJRKtM6ILCCZzTjpC4759uEm/izJvVF6e+EhRfd3ckI91Z+K48Xi4yVDO3oItNhlrgJxkvY4YRSBhnp5EjJ9AmLcvRom5f+AMUqquINTursFP5sOmshKSw0Wcwd4kBckwSqxnuctQ6xcRfQ/MiSksSU2juWYHJXEmKqZryDWoyJykYZWPFfWaLyh8Kam8d2cVzrmpvAfyE+LZnmCj3BFOQpCCGkMM9kA7CmsBFm04Z89f5MWzpzQuzuZ2RT61TjubNRb64/Ix6deSvLoNoaZ6F4rgUNauK0YvFzHIQ6R0PIj0VaAfo8BTGIVa5otWoSGvsJSSgiKSxBnM0VjJDFGzWB/D7AkmJgkeLEuYh9B+pI0AeSCiUkWYWo9Rpf0gSpQkxBSp90SlBovJzOzYRCyGCKL0JuJCp+HQmUk0mnCarMwwRKNXGlixIBthYOANz549w+VyMTw8zIBrUFou/n71isGhYVxvB3njesvQ0BBvpfPg8D+4ht5J+6EP968HBqV373gj4es3AwhnTp9h/uxEivJz6W2v48n1Q9w+18ojqYGLl2bRJIlw6+xRDhTl0pQ3l7Z1aXQVzeP45hwutVRy4fMcdq3Oou/ANv787hJC/8l2cqOCcGplLHOE0pAdRmv5Yq61dTBzkpzVSSa6yvMlu8nZYBAoMo1mjVagwu5PxydO2tP9mR88gprcOH7tb0K4c7WPqrwEusqS6Vo/h5Mrp9O9NZPvT1Xx8n4Hh4tSKEmMZGN8GMuneBFnnEyBJYDqWSF0labTmWNmfWww25fG89uNDoRf7vbRvi2az7bG0tlSyDc9tRyrzOPBqS3w6grHKxZRHKvj01QzOq8RLEyy0bHWSWW8hv3ZVvqKojm8Kp5dS6fzqG8/wv0zLSydM4pFmTIsDoEjNblcbSljZ5aVTqlWmxxqajKiSJkWhiA5anvBPL49uIEtGQ4a00NpW6iThomO2vggzlatRnjQf4y6Qgcn68rYV13Gkx8u0btvE+X2QCoT9azQ+1OVFMpUvRa3kaNJmaonzWaQ6m3+kHbdTDk1sf5UR7rTuiweYfCvx7ieXufd7w/59/lD3r9+zJdHmymPnUyZxG6xVpp7FjUWtVry9ji8xoxlukFJYeoMim2T2RHjzU6HH3uiJ9CxYhb/A7RN/oThssunAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Screen Shot 2015 09 02 at 14 42 18&quot;
        title=&quot;&quot;
        src=&quot;/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-fb8a0.png&quot;
        srcset=&quot;/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-1a291.png 148w,
/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-2bc4a.png 295w,
/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-fb8a0.png 590w,
/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-526de.png 885w,
/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-fa2eb.png 1180w,
/static/Screen-Shot-2015-09-02-at-14-42-18-36bddfe74c91b79319b26a9fd726c4a5-f69b5.png 1508w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
I’m really not sure about this one.&lt;/p&gt;
&lt;h1&gt;Dessert&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-474b7.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 81.25819134993446%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAAD/klEQVQ4y32Ta1CUVRjH3+lDH+xrFzOcXDWHQsyyMbVBCpCbMtEk44SMjsHiCFQS6SAXRTIRWCcc1FYBd1WUbcEQFQUcubhkEMRNkKFFgoFAoMXdZZd3L8Cvd8GpmRx7Zv7zvOec5/ze85z5H8HhcOB0OrFMTmI0GhkcGprLdrsdm01EFCVNTf0rcYopKVutrrGI1WLBZDIyMz2NKwQXzBVOacL17XTY5yCTFiuPzZNYbTZE6aeifV4Wac0u5Wmp3uGqdThxzMwyIzEc0n6hr6+PG+XlDPXp6enVo2tuxWw2MTk+wkBrA4Z+PbaxYcS/RhEN42CzMjE+Rk+PnpFHY1geDTHY0cTwgzZEqUvhXEEBgiAQuyeKLxPi8VjlSXtHO47+Trryj9Ce/y39xScZvqZisOwiOnUeOyOiif0imeL0JBShG4hZ/wbRa2U81N1ByJeAa9+SkR0Zwu3sOHZ4raS+vp4/9d1kJ8ZRdFhOXV4iAzdPcVWRwibfABYucmejz1YOBa8nZvUi4r09CPNczK3zeQh5KjVBHq+j3SJjLGMzZ2ICqf25geHmOrIi/CnN+ZpqZRJdN09TV5TJ9m2f4OsbhJdPKCvdl+P7phthXp74bVxOetp+BNWFiwS+vYIMLxn3vvJDER1Czb1GRn69Tb48kKKDOyn+PonfSk/SfOs00ZE7eO99b9as28RLbm5z1/Xa0le4cDaMn7SFCEqlkuefE9i8zpMt765g5eKXqarVMd5azQ+f+3FO/iEFydup1ijIORiDf1AILy6U4fVRKBmHDrN4yRLCwz+jXKNBd71MOqFazQsLFpC7O4RI3zV4e8poa29jSFeGRu6Nan8EJ6KDUUQFcP7oPuKi5Ph/HMEu+V4KM1LJTt5HWnwMmQmxdNdUIRgMBvT63yUTWeY0YzXCrOQx0cq02cD0lEUyvRHRapbWbZgeT2C2TEnmN2E2jGEYHZE0jFOqk8wpGVu0Yx0zYRqVCkZd2YxxeALTiJGW+w9obmvjj74RWjr0VNzR0T8wyLNi1vVSzBMmeu/30tPZR2NzFw31jdTdbSRVeZylgcvIyj1OQ9VZ9gSv5tWl7pzRXvnnZc3Ozj4lofJuFaHyMIqv/kh3XTmtxUpqy9T4Ra3B/dNlFOSlUHQkANXxFHw+eIcsxbE54MwT4H9D6O/qoCQ3m8YrlyhKiyc1PICy7ANcV+VQqTlJhVpJVUmR1EEnu5K3cUqdMw+ceRawvYVrmWmUHksmLdyfSJ9VpGzdQM7uLWjS91JZUkh1uZamCjXq/AQqa67P39eTFp8CPmxt4lJ6IpqjSWi/+4bLh2IoTIulOOsAWumklSXnKS3IpeREKjcun6Gx4Zf/Bf4N09vWbE/XctoAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Screen Shot 2015 09 02 at 14 44 38&quot;
        title=&quot;&quot;
        src=&quot;/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-fb8a0.png&quot;
        srcset=&quot;/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-1a291.png 148w,
/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-2bc4a.png 295w,
/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-fb8a0.png 590w,
/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-526de.png 885w,
/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-fa2eb.png 1180w,
/static/Screen-Shot-2015-09-02-at-14-44-38-305de6716396787bfbf295278dda2def-474b7.png 1526w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
Let’s finish this course with a nice bowl of raw chicken.&lt;/p&gt;
&lt;h1&gt;Dogs&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-5daa8.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 83.35500650195058%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAABYlAAAWJQFJUiTwAAAEWUlEQVQ4y4WUe0xUVxDGr9WokTRafGFSTUuKoqCIRGpaLFKNFrMECz4QFFaQUlF5LeKjBRelUYuCimCxgSrERFONSGusVrAqrqAg0FVAQEBpKqjAsiv7BH49u2iTxj86ydwzd+7cb/Kd+c6R+vv76evrQ6PR0NHRYYsNBsO/rtfr//P+JvfGe3t7sVgsWHGsJg0M9GM0Gm0fdTqdLTYaTTY3m82i2IzJZHqdN77OWWxuja0NrKsVcHBwEIn/MZMofisnGuj1BgYG366XLl/6lYryCspVKlS3blJ28wZld+5Rcuue6GrhaUsjdTV3+P3KRW7fKBEstLS3P6H4ws/UVqpoaarjl+KL1N6/y4DZhOQn88d3mYykOAXf7lKi3L2XrxWpbNl+wAZYkJVNzMZQ4uMTCFkbxOOWFpqrStmrkHPkQApnC7I4nR5DUWYCfVoN0iyXecx0diPQfyVXSypQph0iWpHGwSP5NgqJm6P4fLEMv8BwnJw9qG9qpfWPs5xIjuJkTib52Zk0nE/jonIdPRod0vjxE7F/zx47u3eJjNxKyp4M/ANCSFRsF3tlRJm8i2B5DKvlcSzyDaahuY2O2odkbEug+FwhquvXKN0vJ08RhKb3FdL8Oa6MGC4xTJKYPM6OaQ6T+GiKPdGREbbp+S5fQVBUKknfn0EWHE9TSzvVJaUc3bsLteomlWU3+O1wHMmhMnp1eqQdsVtYtfwzZAvd+NLHnYBFHqzw9mSTPEQAmti8NYGwTbuJ/eYoq8Liedz6FzUC5FTWPs4dzyM38xBn8rJE/Qb0BiOSpqcHBvsZsJjoNwut6ft4JcRtNFtse/jy5Usx1Xa0Wp0QsVZoDcxCNl1dXXR1i38Z0o7ZaBjS4bKlywgMWEnwmrWEhYRyIDGeorTtHIwN5897FXR2PuM7ZRJ5Bcepqi6nqvKOTfS15VdQJm4gOvEgO9KLyDldQp84HJIw64OpkxxYutCLhXM9SA0ORO7tQWlxEQ31dbg7TeGr9QFEha1h0zp/nj1pQ323mLS4xcyZMY3Ro8bhOd9LsNAOAVonnLpzJzkZ6SzwWED+D3nEyCMp/PEnNM/bifB3Z2u0DB/PWThPnUhjYzPnC7L51GkMbk6TcJwwGj+vuei0vUOAY4aPwtdnCSnbkvD52ItkRRIrFnlzMjcXbXcn/kvmsGC2IxGr/fB0mY7q7n1O5x7GYYTEBxPsmDVxJKu83XhlBfR0dSVi7Rr27NwmppVD4YksTmTsJ2dfCrUVt2nveE7w+iASVi9my8Zw5rnMpKnhIfnHMnAYJuE62Y7Z44ex0stlCDA2PJRT2Ue5fP4sZVcvUX79Cqprl7lQmEfzQzX1jx7xocNYAj9xwmP6+zhNGEnP360cS9+HvWDnPPYdHEdJfOHuOES5Tq3maVsb3UIemu5utJoem3e9eI5BSEgvJFRdVUmdupamerVYa8SdqedFZwc1lRU8qLnPg+pKGkVz6xX2D1Dj2XTR4/Z5AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Screen Shot 2015 09 02 at 14 40 14&quot;
        title=&quot;&quot;
        src=&quot;/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-fb8a0.png&quot;
        srcset=&quot;/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-1a291.png 148w,
/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-2bc4a.png 295w,
/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-fb8a0.png 590w,
/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-526de.png 885w,
/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-fa2eb.png 1180w,
/static/Screen-Shot-2015-09-02-at-14-40-14-b6300e467befbb18c0a227ecbbd91c04-5daa8.png 1538w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
To be fair, you could consider &lt;a href=&quot;https://en.wikipedia.org/wiki/British_Shorthair&quot;&gt;british shorthair&lt;/a&gt; the dogs of cats.&lt;/p&gt;
&lt;h1&gt;Birds&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-585a2.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 81.14209827357239%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAQCAYAAAAWGF8bAAAACXBIWXMAABYlAAAWJQFJUiTwAAAEEUlEQVQ4y62SW0zTZxjG/9fe7GpxLDFbjG7J4tyiWZbMhS1kmXiYYnSG6jyCQxoF3DhtiCALMs7VSiuncdCCVLFAqW1HW2illKMBZASkCOWgmUCkTKAtld/+crOL3exib/Ikb77ne57v+b73E3w+H36/n8XFRebn53n27BlLS0ssLy//Z7jdblZWVtZ8hNXVVRDh8XjWjBYWFtZ6n8+Lz+v5l9jn9a5xXs8/3OswK2Kw1dVXCB6vj8VlL/9HvRIhrHoWMNaUkZsvJ192FYWigK52O965UWaH2qko/Y0CRRHXCxTU1FQzM/kY/8wQnSYtCmUxSmXhmsZus+B3TyMs/zmCLCKYze+8zZaNAWwOWIfs11ie9jXSU5nCV1s38v7GDbz31hvsDtxCf1s9Ey2lKM8f4N2AN9m0YT2b1q8jLSEc91gHwlhfG8bceJTRR0g/s5fk/R9Rej6Y9IvR6IqyuJscSa70IGlHg7gS8gFlv5zjyqVYLFeTUF44RkZECMkh25GHfYG6vADBer+O25nxXJOGELXzQxIObKMq/TjpCRKuisK6rB+J/2Y7P+zaSp40mOrsCKIiJNzNuYQ88iBng0TNvm2U/vQtankcwq0SBdLQYNKiJYQf+ISIw58jv/gd+Rd2cXrvZ1yJO0NUaCCn9n3K5XMhlKYeJ+bQdmLDJaSLGumhHUSGBiFLOoEqSype2TlCnaqcbksjoz0Wem16hh1GnANdtJoMtDfpcXZbGBTX+lrqef1Eg70dNOsaGOi08aTPRm/r7wyJ/NTIAMLS8hLzL/9ibuEFk8/HmJpxMeuex+PzM+eeZWx6GOeEk6fPZ1gUv5h78SUT0xPMvRA51ySzLxZ4PjfH0MgI4xMuhMmpCbp6HHQ9tCIvTSA7J4qqqkI6Ojpo0FWRkHKMSxfDKS66htVqobvbjkz5M7KCRPJyxKGp1Wjra4mJCyUzOx5h3DWGo83KtJiivKqEsPCTqG5W4LBbGR0eIEOWzBGJBFVlOc0mI8N/9KLR3iIjK5W8zCzUKhX2ZgvZuWmkXk5CcDof02IxMDk+yB11GR8H7iEjOx+b2cCToUcUl+Sy7cs95ORdw2xooLvdytGT59jx9X6SEhO5XVHGfU0tO3cfYf/hE2LC8VHMTTr6ex0MPuomJvksKZmJmPX36XJYcdjMnIqScDkjCZO+EUericIbNwiLOk5qSiKVRUVoa2tISU7h++jTCC7XE3S6e3R1tvKo/yGNJg3XS7LRqGtoNhtFgweU3FSgEJPqtRpsLUbx8A50hjpksiyKC+Ti3mr0jfXcvFWGMDI2jqbhLs02A/Z2C63tzahqb4tkOfXaOxibtOJw7mFotdOo19JkasDSoqflgRm91UFhkZLqqjKqqyuob9DwNzw9z6G8lEJwAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Screen Shot 2015 09 02 at 14 39 47&quot;
        title=&quot;&quot;
        src=&quot;/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-fb8a0.png&quot;
        srcset=&quot;/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-1a291.png 148w,
/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-2bc4a.png 295w,
/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-fb8a0.png 590w,
/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-526de.png 885w,
/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-fa2eb.png 1180w,
/static/Screen-Shot-2015-09-02-at-14-39-47-76ffd3846a9708abec735e4b77acd861-585a2.png 1506w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
Birds. That’s definitely correct.&lt;/p&gt;
&lt;h1&gt;Skylines&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-5daa8.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 92.71781534460338%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAABYlAAAWJQFJUiTwAAAEeklEQVQ4y32UC1CUVRTHv5oma2omWEihDHmoiBj4ABTzBatSiCWZozM+xkfmKzHdJkQJAlQUn4Eyo+WIqPmYMRRFyFhgQR4qa0ktCpUNjjx2Z3F5uQ922V+XXW10xvHO/L5zvvvd79xz7znzlxCjr68Pa28v7Xo9JpORp4fd8bA7PWHtgv71TzAan10vlZSVoyqvoLRMRbFSiUpVTlVVlYOamhquC27cVFNzo5YbtWpq1be4XfcHf2rqqa+vR6PR0NjYSFNTE2azGWnx8tUoNicSvzWZxOQ0UlK3kZq2jfT0dHbt2kVWViZ7vz9E8vbdbM/YT8b+LA5mH+HwkaMcO5bDyZMnOX78OPn5+c6AUTGxzF+0lGWfr2H12jjWrd+AQvE18fGbSUhIIC01jS1Jqaxct5E4xWY2fbOVhMRkUsSmO8Sme/bsJSNjNzk5Oc6AubknOH36DHl5eWKXixRczqfwymVKS4pRlSqprqygWhy/oKCAoqJCrv5SRInyV1RlSgeVFWVcKy/lN/VNrFYrUqvBRIfJTncvGMT9tvf0oeu00qI38kDXw31tD60dVvSPQNct6AExRVuXk9bOxxh6sdn6kFQPLFzRdHCm8j45yn848HMd6afUpOaqUWSWsDzpLFtyNWQUdbHjUjspF3R8l6cj6byOree0fHuujaSzLRwsbMVoFhk+FKW+KzKq01lRt1govtvJiZIGVifsY86S9cjnr2DPhTv8WK7lkLKNw9UWsitNZFWYyFQZyVI94oCym6PXOjFabEgdBgOGrk5+v9NA0s59bM/8geWiADKZO55evrw/VU7ajnhWKDax89RlKvVQ+C8U3IMLDTYu/WXnfANcvGvBYrMjRUydxIJ5cxkdHISbTIariysyFxcCh7yHt7cfPmPGsWpZJJ8uXcq27KNomh9ySVnK1epbNBqsVDc8IL+iFvW9ZmeVPd3fIsDPh1HDfAn2H0qArzeBPl5EBPgQPNyPIcEhzIn9mNDpMYTJZxMVNYvPYj9hXkw0uxM3kPbVQrJTVlJ65Se6uwxIbm4yfIcMxsfDHf/Bngx9ZyDDhB/k6YqPpwfuviNwGeTNm4MDedtvNAPecMVFNgivd71wc5UxOnA4M8Z4ELfkI7rE1UmBowKZPSeWD2fKiZZPISpyKtERk5k7fRIxMyORR89iQvgEwj6YQvi0mYwPn0LYxAhCw/uZRuSs+URETEMR94UI2IXU0qKlp8eEaCEhEEIkrH309vZjF77d8d5vzWbRZ6Jx7Y8F4mmeEYeAEXbCwiAkBEJDnlj7M3bsWJg4ESECzp+EyDx39MeWJEmU+n8sAh5jf8raHL5a7RQ0m83+3Ez7kTxehZdfcQZZMLIZb9dmBrym5aWXbaJQ18V8n+PbgNfN1N22OTJ5YcCFHjoWjdITP7mBglXdrJlxhxS5Ev+BzawLr2JjqJHF44rZIq/h7/oXH9lxh35SDcuC2/hynJaRQb1EBj9iRVA70/21RIc2sXaMnrXj65gRWcH1W0IdsAiVNjua+Hn8BzhyWFNGQuF4AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Screen Shot 2015 09 02 at 14 49 11&quot;
        title=&quot;&quot;
        src=&quot;/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-fb8a0.png&quot;
        srcset=&quot;/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-1a291.png 148w,
/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-2bc4a.png 295w,
/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-fb8a0.png 590w,
/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-526de.png 885w,
/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-fa2eb.png 1180w,
/static/Screen-Shot-2015-09-02-at-14-49-11-f9f2873c3d2aea66273e2893b6cf2f89-5daa8.png 1538w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
Bar graph or 8-bit skylines?&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Don't manage configuration unless you have to.]]></title><description><![CDATA[At my first job, there was no code driven infrastructure at all. A nightmare from today’s perspective. The next job, there were some perl…]]></description><link>https://5pi.de//2015/08/31/dont-manage-config-unless-you-have-to/</link><guid isPermaLink="false">https://5pi.de//2015/08/31/dont-manage-config-unless-you-have-to/</guid><pubDate>Mon, 31 Aug 2015 13:45:56 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a data-flickr-embed=&quot;true&quot; data-header=&quot;false&quot; data-footer=&quot;false&quot; data-context=&quot;false&quot;  href=&quot;https://www.flickr.com/photos/tuinkabouter/1884416825/in/photolist-3Sw8tg-4pYNoQ-5kuQnX-5gVeH1-sVYLE3-a3DqaD-9gzdMP-9Uspfa-2Xkkg8-8oZ8r-5aUCED-e4npy8-bn7jhK-ipQLm-4pZj1h-4xV1jk-5kuMV6-9sUBkF-b8nM3X-7tsJDh-9hY5PC-dUvDCG-4QZswm-4io4Y8-812A6f-qprAf-5ojVcV-5wUanj-9kNyLj-5GXFGx-5YFqsj-6VJnS6-dUmEaq-99qjSa-4QZsvG-4QZsuW-4QZsu7-4QZsuo-eL5jpe-4EWyKW-CXs86-etDrFm-4M5joL-jpb3gs-5knnT4-GAgoa-6d2TiA-63t6NF-7TRs3F-51XCQG&quot; title=&quot;Jungle&quot;&gt;&lt;img src=&quot;https://farm3.staticflickr.com/2037/1884416825_521e525758_o.jpg&quot; width=&quot;1024&quot; height=&quot;576&quot; alt=&quot;Jungle&quot;&gt;&lt;/a&gt;&lt;script async src=&quot;//embedr.flickr.com/assets/client-code.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;At my first job, there was no code driven infrastructure at all. A nightmare from today’s perspective.&lt;/p&gt;
&lt;p&gt;The next job, there were some perl scripts to massage systems in a imperative way.&lt;/p&gt;
&lt;p&gt;Then, at SoundCloud, I used modern configuration management for the first time. SoundCloud heavily invested into Chef which was, at some point, used to drive almost every aspect of the infrastructure. A new service? Add a cookbook.&lt;/p&gt;
&lt;p&gt;Compared to a world without any proper infrastructure automation this obviously was a great step in the right direction but it was a growing pile of technical debt and constant source of bikeshedding around how to actually use it.&lt;/p&gt;
&lt;p&gt;Maybe you don’t have those problems. Maybe you have strict guidelines on how to use your configuration management or some chief architect to tame the chaos but I argue that lot of larger companies with agile and independent teams run into similar problems. If you disagree, please leave a comment.&lt;/p&gt;
&lt;h3&gt;Issues with Chef&lt;/h3&gt;
&lt;p&gt;There are several categories of problems with this setup. Some are organizational nature, like the way it was ab(used) to drive the complete infrastructure. There are also very specific issues with chef, like its internal complexity both from a operational perspective as well as from its complex interface. Things like 15 unintuitive precedence levels for node attributes to its multistep execution flow and leaky cookbook abstractions it’s often frustrating to use and accumulates a lot of technical debt due to it being hard to test and refactor without potentially breaking your infrastructure.&lt;/p&gt;
&lt;h3&gt;Alternatives - or the lack thereof&lt;/h3&gt;
&lt;p&gt;That being said, there isn’t an obvious better alternative. In the meanwhile I used ansible and salt which have their very own problems. Even though they try to be less complex than chef, they heavily depend on template driven metaprogramming and struggle with proper code reuse and testability similar to chef.&lt;/p&gt;
&lt;p&gt;Over the years I came to the conclusion that configuration management in it’s current form as used in reality has some fundamental design issues.&lt;/p&gt;
&lt;h3&gt;Design challenges&lt;/h3&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-733ee.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 81.31720430107528%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAME/8QAFgEBAQEAAAAAAAAAAAAAAAAAAAID/9oADAMBAAIQAxAAAAGks2udCg//xAAZEAADAQEBAAAAAAAAAAAAAAABAgMAERT/2gAIAQEAAQUCazY0PfRiDpCZRIDn/8QAGREAAwADAAAAAAAAAAAAAAAAAAIDAREh/9oACAEDAQE/AaUmvENywf/EABgRAAIDAAAAAAAAAAAAAAAAAAACESJB/9oACAECAQE/AVnSx//EABoQAAIDAQEAAAAAAAAAAAAAAAARAQIhEEH/2gAIAQEABj8CyTZMGxWrD95//8QAGxAAAgIDAQAAAAAAAAAAAAAAAAEhQRExYdH/2gAIAQEAAT8hhPA3RqPqOUFZU6OuAfo1tdn/2gAMAwEAAgADAAAAEGvf/8QAGREBAAIDAAAAAAAAAAAAAAAAAQARIXGx/9oACAEDAQE/EGAN0RwF7P/EABcRAQEBAQAAAAAAAAAAAAAAAAEAIRH/2gAIAQIBAT8QD1XVt//EABwQAQACAwADAAAAAAAAAAAAAAEAESExQVGR0f/aAAgBAQABPxBaBsV1C5l4OZgoacC3mOoqaCd/JcnSqix8npqK6qCBracZ/9k=&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;vortex&quot;
        title=&quot;&quot;
        src=&quot;/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-f8fb9.jpg&quot;
        srcset=&quot;/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-e8976.jpg 148w,
/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-63df2.jpg 295w,
/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-f8fb9.jpg 590w,
/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-85e3d.jpg 885w,
/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-d1924.jpg 1180w,
/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-9452e.jpg 1770w,
/static/Airplane_vortex_edit-29dd07b878f06e9e5cece16d0d7a2b39-733ee.jpg 2976w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
The idea of defining a (distributed) systems state and mutating it to eventually converge to the desired state is sound but the interface the distributed systems provide is simply too complex.&lt;/p&gt;
&lt;p&gt;All the mentioned configuration management systems use some agent or ssh access to execute commands on the systems, similar to the imperative design of user interfaces: You run commands in a specific order to modify the state of the systems.
But since there is no unified interface to configure applications, how to achieve a specific state is highly dependent on the application.
Configuration management systems try to solve this by abstracting a “thing” in a system and give it a clear interface with some idempotent functions to move it into a given state like &lt;code&gt;installed&lt;/code&gt;. Whether it’s called chef cookbook, salt state or (the most misleading name) ansible role.
If the “thing” is your web application, this might work very well but in reality you often have to configure third party applications that have subtle dependencies on specifics of other “things”. Or you have low level system configuration which affects and depends on other things installed. At this point, the abstractions usually break and you often end up introducing site specific changes to what is suppose to be reusable, generic components.&lt;/p&gt;
&lt;p&gt;The lack of a generic system configuration interface that can be used to configure every aspect of a system, imposes a lot of complexity on the configuration management.&lt;/p&gt;
&lt;h3&gt;Split configuration based on life cycle&lt;/h3&gt;
&lt;p&gt;&lt;a data-flickr-embed=&quot;true&quot; data-header=&quot;false&quot; data-footer=&quot;false&quot; data-context=&quot;false&quot;  href=&quot;https://www.flickr.com/photos/chloeophelia/6633874815/in/photolist-b7dkRZ-j6g2XM-9hrBqG-aTDRwD-99nLPX-tm1pU-bn3u8p-qHs14o-qCjhgq-4HCs5T-r9mjBJ-9MyabW-q4629m-oCKk2V-7zyviy-7HivZw-8Wzh3Y-k2UGmz-iuChmC-dKHA-qe133d-itoizS-83TM-oHsMP1-bstTCp-iY9u4G-jdRX2T-5SY4oB-7wmEBq-Cv7Z2-t2Hab-7ofMcb-99Eqbu-r5sCGR-93jB3F-qyomNx-riyS2i-dNMQsp-91DhgW-4ahpVw-qHSWG3-keBc61-5SJkro-qixdjy-dSyxEq-8nmzni-4aKkPV-f2gfQ-q96Qs1-iyGqwU&quot; title=&quot;frozen in time&quot;&gt;&lt;img src=&quot;https://farm8.staticflickr.com/7142/6633874815_5d4ecf8b71_b.jpg&quot; width=&quot;1024&quot; height=&quot;684&quot; alt=&quot;frozen in time&quot;&gt;&lt;/a&gt;&lt;script async src=&quot;//embedr.flickr.com/assets/client-code.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;As long as there is configuration, there is configuration management. You will always have some form of desired state you want your infrastructure to be in. The question is not if but how to manage configuration. Since with configuration that changes rarely less things can go wrong, I believe the best way is so identify different configuration life cycles, find the right solution to manage this kind of configuration while compromising dynamically for correctness.&lt;/p&gt;
&lt;h4&gt;Build/install time configuration&lt;/h4&gt;
&lt;p&gt;If the lifetime of a single host or container image is lower than the lifetime of a configuration option, it often makes sense to move this configuration to install/build time. Since no change can happen during the life cycle of the host or container it’s easier to reason about the infrastructure since change to this set of configuration can be ruled out as reason for a given observation.&lt;/p&gt;
&lt;p&gt;Moving configuration to install time might mean making your bare metal installer preseed some configuration or building a static OS image for your cloud provider. The point is to bake in this configuration and just rebuild when changes are necessary.&lt;/p&gt;
&lt;h5&gt;Examples&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;Operating system release&lt;/li&gt;
&lt;li&gt;Partition schema&lt;/li&gt;
&lt;li&gt;Hostname&lt;/li&gt;
&lt;li&gt;Installed packages / core services&lt;/li&gt;
&lt;li&gt;OS level configuration (sysctl, ssl keys)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All configuration that is same across all environments (dev/test/prod) should be considered to be hardcoded where environment specific configuration (credentials, URLs / service identifiers) should be passed in on runtime so you can build, test and deploy the same static artifact.&lt;/p&gt;
&lt;p&gt;What in reality gets hardcoded is a case by case decision and depends on a lot of factors. On a bare metal infrastructure where all services are deployed straight to the host, there will be a lot of configuration that is site-wide and environment agnostic but simply is changed that often that reinstallation of the whole host isn’t feasible.&lt;/p&gt;
&lt;p&gt;But in a containerized infrastructure you have host image and container image life cycles. There is little host configuration, so usually it all can be hardcoded. Even if reinstalling the host takes 20 minutes, if it only happens every few weeks and is fully automated, it’s probably fine.
Building the container images in a continuous deployment pipeline might just take a minute from a change until the changes are deployed, so here again it’s feasible to bake in all suitable configuration.&lt;/p&gt;
&lt;h4&gt;Start time configuration&lt;/h4&gt;
&lt;p&gt;&lt;a data-flickr-embed=&quot;true&quot; data-header=&quot;false&quot; data-footer=&quot;false&quot; data-context=&quot;false&quot;  href=&quot;https://www.flickr.com/photos/storm-crypt/326228715/in/photolist-uQ1r2-t62c-jjzxu-pzhfMZ-7pBiCr-gKqR8Z-cqHKJS-hZsSz-522oaS-b3uw2-bXcpyw-5zdLNo-6wvjpw-nv1KL3-3aNjwL-cpBpTC-dbcLKt-4o3Qkk-9pyoHg-6dvJoB-hzvCsP-4o7SHY-aCL42j-4tGWBK-4wxATP-bX13ov-8MgTTH-cZKwvf-iKDCAd-jRyyK1-nb69S1-kvg2qy-kvdS8F-kveev8-kvdGta-kvdFzg-kvdxKi-kve2eV-bpcAmi-rbVVC-4gi4N-4rA5gu-aazkxv-o2XySp-dxnLQi-2oa9re-eC2cX3-8JA67v-8KYUaA-8JA6aB&quot; title=&quot;Sorting Facility&quot;&gt;&lt;img src=&quot;https://farm1.staticflickr.com/135/326228715_dea4917fda_b.jpg&quot; width=&quot;1024&quot; height=&quot;768&quot; alt=&quot;Sorting Facility&quot;&gt;&lt;/a&gt;&lt;script async src=&quot;//embedr.flickr.com/assets/client-code.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Especially environment specific configuration like credentials should be passed to the services by whatever deploys your application.
Even though a lot people still deploy their applications with configuration management instead of cluster schedulers, I’m convinced that will change in the next few years. Whether you’re using &lt;a href=&quot;https://mesosphere.github.io/marathon/&quot;&gt;Mesos/Marathon&lt;/a&gt;, &lt;a href=&quot;http://kubernetes.io/&quot;&gt;kubernetes&lt;/a&gt; or &lt;a href=&quot;http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Schwarzkopf.pdf&quot;&gt;Omega&lt;/a&gt; the high level concepts are similar: You define your application and the scheduler decides based on the available resources where to run it.
Whether services are deployed by config management systems or the cluster scheduler, since it’s starting services, it’s the right place to pass configuration to your service. Instead of writing configuration files, &lt;a href=&quot;http://12factor.net/config&quot;&gt;12factor style configuration&lt;/a&gt; is usually better suited.&lt;/p&gt;
&lt;h4&gt;Runtime configuration&lt;/h4&gt;
&lt;p&gt;Instead of configuring your systems on a regular interval with some configuration management daemon, it’s often a better pattern to have the application or a wrapper around it determine the configuration on runtime.
Instead of making configuration management orchestrate various services or instances, it’s more robust and arguably less cognitively challenging to consider the service as it’s own independent entity.
This only works well if site-wide configuration is built in. Determining all this on runtime leads to similar complexity as we have with full blown configuration management today.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;&lt;a data-flickr-embed=&quot;true&quot; data-header=&quot;false&quot; data-footer=&quot;false&quot; data-context=&quot;false&quot;  href=&quot;https://www.flickr.com/photos/ph0t0s/96911576/in/photolist-9yGrf-rf1xpg-iV3Hti-9yGuq-9mJACS-dF5fsX-4CYijn-96MGD7-4CYinT-rcHmFs-e3mfmk-7paoHV-e3rYs3-ebixTV-e3miwv-e3mehH-ebz9br-ebz6nv-9yGn2-maubuc-4eMsoq-gascSh-j448Si-bUvoKX-7tWwN9-8oTPht-fw2NWm-34Ydj3-zMTMt-9NFVNX-4q1pdk-9eX7xW-e72dfP-eb7MBt-4xMiAY-bznrxn-ze6qt-5GtxU5-bhCNRp-fw2NjN-fvMuU4-ebEQHU-fvMxdc-e3rZzd-ebiz6V-fw2Qn7-9NEtYQ-9NGKrb-u79vMN-9NGokq&quot; title=&quot;partly random&quot;&gt;&lt;img src=&quot;https://farm1.staticflickr.com/36/96911576_1a57864a0b_b.jpg&quot; width=&quot;1024&quot; height=&quot;768&quot; alt=&quot;partly random&quot;&gt;&lt;/a&gt;&lt;script async src=&quot;//embedr.flickr.com/assets/client-code.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
Isolating change to specific points in the life cycle of systems and services reduces the complexity of runtime configuration and simplifies the mental model when reasoning about the infrastructure.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[CloudFormation driven Consul in AutoScalingGroup]]></title><description><![CDATA[CloudFormation templates are a great way to manage AWS resources. All resources for a stack are configured in a json and CloudFormation…]]></description><link>https://5pi.de//2015/04/27/cloudformation-driven-consul-in-autoscalinggroup/</link><guid isPermaLink="false">https://5pi.de//2015/04/27/cloudformation-driven-consul-in-autoscalinggroup/</guid><pubDate>Mon, 27 Apr 2015 13:22:30 GMT</pubDate><content:encoded>&lt;p style=&quot;font-size:10px&quot;&gt;

  &lt;a class=&quot;gatsby-resp-image-link&quot; href=&quot;/static/dreamy_consul-2-09a85130490779b28e2855643e68eb19-9b0e7.jpg&quot; style=&quot;display: block&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;
  
  &lt;span class=&quot;gatsby-resp-image-wrapper&quot; style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;&gt;
    &lt;span class=&quot;gatsby-resp-image-background-image&quot; style=&quot;padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url(&amp;apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQFAgP/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAABm5qJWpnUP//EABkQAAIDAQAAAAAAAAAAAAAAAAABAgMSBP/aAAgBAQABBQLDTcWZH2USUrqmbgf/xAAVEQEBAAAAAAAAAAAAAAAAAAAAIf/aAAgBAwEBPwFH/8QAFREBAQAAAAAAAAAAAAAAAAAAABL/2gAIAQIBAT8BpT//xAAcEAACAAcAAAAAAAAAAAAAAAAAAgEQERIhMUH/2gAIAQEABj8C1OkUYxcdP//EABwQAQACAQUAAAAAAAAAAAAAAAEAERAhMVGBsf/aAAgBAQABPyFqVDOHh5lIho7Eb9p//9oADAMBAAIAAwAAABBXL//EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8QUa3CD//EABYRAAMAAAAAAAAAAAAAAAAAAAEQIf/aAAgBAgEBPxCCn//EABwQAQACAwADAAAAAAAAAAAAAAEAESExUXGBsf/aAAgBAQABPxDN4NOI6715F3kiJPjVjxmHKp0L+xyRfRP/2Q==&amp;apos;); background-size: cover; display: block;&quot;&gt;
      &lt;img class=&quot;gatsby-resp-image-image&quot; style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot; alt=&quot;dreamy consul 2&quot; title=&quot;&quot; src=&quot;/static/dreamy_consul-2-09a85130490779b28e2855643e68eb19-f8fb9.jpg&quot; srcset=&quot;/static/dreamy_consul-2-09a85130490779b28e2855643e68eb19-e8976.jpg 148w,
/static/dreamy_consul-2-09a85130490779b28e2855643e68eb19-63df2.jpg 295w,
/static/dreamy_consul-2-09a85130490779b28e2855643e68eb19-f8fb9.jpg 590w,
/static/dreamy_consul-2-09a85130490779b28e2855643e68eb19-9b0e7.jpg 600w&quot; sizes=&quot;(max-width: 590px) 100vw, 590px&quot;&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;br&gt;
&lt;span&gt;Jessie Eastland, &lt;a href=&quot;http://commons.wikimedia.org/wiki/File:Dreamy_Twilight.jpg&quot;&gt;&amp;#x201E;Dreamy Twilight&amp;#x201C;&lt;/a&gt;, Consul Logo added&lt;/span&gt;, &lt;a href=&quot;http://creativecommons.org/licenses/by-sa/3.0/legalcode/&quot;&gt;CC BY-SA 3.0&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;CloudFormation templates are a great way to manage AWS resources. All resources for a stack are configured in a json and CloudFormation takes care of creating or updating your stack.
Consul is a distributed, consistent data store for Service Discovery and configuration which also features health checks and supports for distributed locks.
Consul deployed and updated via CloudFormation provides a nice foundation for any kind of modern infrastructure.&lt;/p&gt;
&lt;h1&gt;AutoScalingGroup&lt;/h1&gt;
&lt;p&gt;If you specify instances in your stack directly by using the &lt;code&gt;AWS::EC2::Instance&lt;/code&gt; resource, updating anything that requires recreation of the instance will bring up a new instance and terminate the old one. If you need more control over the recreation, the instances need to be managed via a AutoScalingGroup.&lt;/p&gt;
&lt;p&gt;By default, updating a AutoScalingGroup won’t affect the instances. You need to manually terminate them and let AutoScale spin up a new instance.
To automatically update the instances, you can specify a UpdatePolicy requiring at least n instances in service while doing a rolling upgrade.&lt;/p&gt;
&lt;p&gt;In the case of consul this isn’t enough. Without checking that a new consul node actually came up and connected successfully to the cluster we might lose to quorum and the cluster becomes stale.&lt;/p&gt;
&lt;p&gt;Fortunately CloudFormation offers a UpdatePolicy option WaitOnResourceSignals which can be used to signal that any new consul node coming up connects sucessfully to the cluster before the any new instance gets terminated.&lt;/p&gt;
&lt;h1&gt;UpdatePolicy&lt;/h1&gt;
&lt;p&gt;The changes to the CloudFormation template are straight forward. In the AutoScalingGroup we add a &lt;code&gt;UpdatePolicy&lt;/code&gt; with &lt;code&gt;WaitOnResourceSignals: true&lt;/code&gt; like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;&quot;UpdatePolicy&quot; : {
  &quot;AutoScalingRollingUpdate&quot; : {
    &quot;MinInstancesInService&quot; : 2,
    &quot;PauseTime&quot; : &quot;PT15M&quot;,
    &quot;WaitOnResourceSignals&quot; : true
  }
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;If &lt;code&gt;WaitOnResourceSignals&lt;/code&gt; is set,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AWS CloudFormation suspends the update of an Auto Scaling
group after any new Amazon EC2 instances are launched into
the group. AWS CloudFormation must receive a signal from
each new instance within the specified pause time before
AWS CloudFormation continues the update.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now we simply need to make sure any new instance sends a success signal after it came up and consul connected successfully to the cluster.&lt;/p&gt;
&lt;h1&gt;Send success signal&lt;/h1&gt;
&lt;p&gt;Once a new instance starts, we need to wait for consul to join the cluster and replicate. Since we need to guarantee that a new node actually joined the cluster, not some partition of it, we need something that goes through the raft log.
When looking at the consul documentation, there are a few endpoint we could use. Unfortunately the documentation doesn’t state what the consistency guarantees of status endpoints like &lt;code&gt;peers&lt;/code&gt; are, so &lt;a href=&quot;https://github.com/hashicorp/consul/issues/880&quot;&gt;I had to ask&lt;/a&gt;. Turns out, we can trust &lt;code&gt;peers&lt;/code&gt;.
With that knowing, we can simply wait for consul being ready as part of the UserData script, then signal success by using the &lt;code&gt;cfn-signal&lt;/code&gt; tool:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;&quot;UserData&quot;: { &quot;Fn::Base64&quot; : { &quot;Fn::Join&quot; : [&quot;&quot;, [
  &quot;IP=$(ip addr show dev eth0|awk &apos;/inet /{print $2}&apos;|cut -d/ -f1)\n&quot;,
  &quot;while ! curl -s http://localhost:8500/v1/status/peers | grep -q $IP:; do echo Waiting for consul; sleep 1; done\n&quot;,
  &quot;cfn-signal --resource InfraScalingGroup --stack &quot;, {&quot;Ref&quot;: &quot;AWS::StackName&quot;}, &quot; --region &quot;, {&quot;Ref&quot; : &quot;AWS::Region&quot;}, &quot;\n&quot;
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;h1&gt;AMI&lt;/h1&gt;
&lt;p&gt;The ami runs consul on start and uses a IAM role with read-only EC2 access. Since we tagged all consul instances, this allows the init script to discover peers via the AWS API:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;URL=&quot;http://169.254.169.254/latest/&quot;
ID=$(curl $URL/meta-data/instance-id)
REGION=$(curl $URL/dynamic/instance-identity/document | \
  jq -r .region)

SERVERS=$(aws --region $REGION ec2 describe-instances \
  --filters \
  &quot;Name=tag:aws:cloudformation:stack-id,Values=$STACK_ID&quot; \
  &quot;Name=tag:role,Values=consul&quot; \
  &quot;Name=instance-state-name,Values=running&quot; | \
    jq -r &apos;.Reservations[].Instances[].PrivateIpAddress&apos; \
)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;We also need to check that SERVERS include the necessary number of peers to form a cluster. Since we’re using runit which restarts failing jobs, we just fail if we found less peers than expected and wait to get restarted:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;if [ $(echo &quot;$SERVERS&quot; | wc -l) -lt $BOOTSTRAP_EXPECT ]
then
  echo &quot;Not enough peers, expected $BOOTSTRAP_EXPECT nodes but got $SERVERS&quot;
  exit 1
fi&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Once all peers are up and running, we can start consul:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;exec /usr/bin/consul agent -data-dir /var/lib/consul \
  -config-dir=/etc/consul \
  $(echo &quot;$SERVERS&quot; | sed &apos;s/^/ -retry-join /&apos; | tr -d &apos;\n&apos;)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;h1&gt;Updating the Stack&lt;/h1&gt;
&lt;p&gt;With all this in place you can do a fully automated rolling upgrade while keeping a quorum. Just update the stack and you should get something like this while having a fully operable cluster:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/output_710_280-e458b67bbb060e2f44d87934ff67c7cd.gif&quot; alt=&quot;Update Stack Log&quot;&gt;&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Scope and Ownership in Tech Companies]]></title><description><![CDATA[
My first job was a typical corporate job at a financial service provider after which I worked at large hosting company before joining…]]></description><link>https://5pi.de//2015/04/22/scope-and-ownership-in-tech-companies/</link><guid isPermaLink="false">https://5pi.de//2015/04/22/scope-and-ownership-in-tech-companies/</guid><pubDate>Wed, 22 Apr 2015 12:19:36 GMT</pubDate><content:encoded>&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-b73cf.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 57.7727952167414%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAf/EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAF2JWXUTCf/xAAYEAADAQEAAAAAAAAAAAAAAAAAARESAv/aAAgBAQABBQLljJDbNNqn/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGhAAAQUBAAAAAAAAAAAAAAAAAAEQESExQf/aAAgBAQAGPwLVktWjhbf/xAAaEAACAwEBAAAAAAAAAAAAAAAAAREhMUHB/9oACAEBAAE/Ia3wa1hJwCw8HA60wsbCdo//2gAMAwEAAgADAAAAEDMP/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8Qp//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAgEBPxDEr//EABwQAQEAAwEBAQEAAAAAAAAAAAERACExQWGRof/aAAgBAQABPxB2+3NoXb37iZGEKvZ6fzHCdx7tfxzRklI0TZgUgnt98ywZfhM//9k=&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;ownership&quot;
        title=&quot;&quot;
        src=&quot;/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-f8fb9.jpg&quot;
        srcset=&quot;/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-e8976.jpg 148w,
/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-63df2.jpg 295w,
/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-f8fb9.jpg 590w,
/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-85e3d.jpg 885w,
/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-d1924.jpg 1180w,
/static/acient_jobs-c3811564b5efa34e1c0c89eb4d18fbe8-b73cf.jpg 1338w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
My first job was a typical corporate job at a financial service provider after which I worked at large hosting company before joining SoundCloud, a startup culture-wise closer to Silicon Valley startups before joining Docker end of 2013.&lt;/p&gt;
&lt;p&gt;Although I never was a manager, I was always interested in the company as a whole, how my work shapes it and how the environment impacts my work and how other companies are organized.&lt;/p&gt;
&lt;p&gt;On thing that strikes me most is that not only that we lot of companies can’t agree on &lt;em&gt;who should do what&lt;/em&gt; but that lot of companies don’t even think about this carefully and don’t realize that this affects everything else. They strives for a Microservice architecture without even knowing why and name their Ops team “DevOps” because that’s what people do.
I want to provide my thoughts on questions around Scope and Ownership but leave the implementation details to the reader. I’m happy to answer technical questions in the comments though and might blog about those implementation details eventually if interested.&lt;/p&gt;
&lt;h1&gt;What your Company should do&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;https://www.flickr.com/photos/jo-ghadban/3355654186/in/photostream/&quot;&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/3355654186_6751a40056_z-63cb55515393f4a5fa21cf6acc6f6d5d-db559.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 71.25%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAMBBP/EABUBAQEAAAAAAAAAAAAAAAAAAAIB/9oADAMBAAIQAxAAAAFvKpkkC//EABoQAAICAwAAAAAAAAAAAAAAAAERAAIDEhP/2gAIAQEAAQUCx2OhJUZrXo4p/8QAFREBAQAAAAAAAAAAAAAAAAAAEBH/2gAIAQMBAT8Bh//EABYRAAMAAAAAAAAAAAAAAAAAAAEQEf/aAAgBAgEBPwGhf//EABkQAAIDAQAAAAAAAAAAAAAAAAEQESGRAP/aAAgBAQAGPwKZO8bOqn//xAAbEAEAAgIDAAAAAAAAAAAAAAABABEhMUFhgf/aAAgBAQABPyFAptwy7RzD0yhwisU9gp//2gAMAwEAAgADAAAAEEMv/8QAFREBAQAAAAAAAAAAAAAAAAAAERD/2gAIAQMBAT8QbP/EABYRAAMAAAAAAAAAAAAAAAAAABARIf/aAAgBAgEBPxCSH//EAB0QAQADAAIDAQAAAAAAAAAAAAEAESFBcTFRkbH/2gAIAQEAAT8QsxCumZ9nF7Uo8+owd07nMjVvfLUoUF4238lHEDuf/9k=&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Scope of Company&quot;
        title=&quot;&quot;
        src=&quot;/static/3355654186_6751a40056_z-63cb55515393f4a5fa21cf6acc6f6d5d-f8fb9.jpg&quot;
        srcset=&quot;/static/3355654186_6751a40056_z-63cb55515393f4a5fa21cf6acc6f6d5d-e8976.jpg 148w,
/static/3355654186_6751a40056_z-63cb55515393f4a5fa21cf6acc6f6d5d-63df2.jpg 295w,
/static/3355654186_6751a40056_z-63cb55515393f4a5fa21cf6acc6f6d5d-f8fb9.jpg 590w,
/static/3355654186_6751a40056_z-63cb55515393f4a5fa21cf6acc6f6d5d-db559.jpg 640w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/a&gt;
No company does everything on their own. You don’t build your own servers, let alone CPUs and chances are high, you don’t even run servers on your own.
All this is fine. It’s not your business to build CPUs, so you shouldn’t. Same for managing datacenters and servers. If it’s not your core business, you probably won’t ever be as good at it as someone specialized in it.&lt;/p&gt;
&lt;p&gt;On the other hand, never outsource what makes up your company. You can’t have someone else build something crucial and tightly related to your companies overall business. If you don’t do your own Community Management, Support or PR, people will realize that your company and brand has nothing to do with your social media presence.
Outsourcing this is like outsourcing your face.&lt;/p&gt;
&lt;p&gt;If your product needs custom software, you should build it. It still seems like a lot of companies believe software can be bought in like car service parts without realizing software is never done. It’s not even ever bug free, so it needs constant attention by people who feel part of the company and see purpose in what they are doing.&lt;/p&gt;
&lt;p&gt;Software is also usually the interface with your customers and you don’t want something you don’t &lt;em&gt;own&lt;/em&gt; between you and your customers. Not only that you don’t understand and can’t control, but all feedback loops are much wider. You listen to your customers, explain your ideas to some agency, they explain it to their developers and after some time you get a result. For every problem with that piece of software, you need to go through the whole loop again. Every competitor doing this in-house will move &lt;em&gt;much&lt;/em&gt; faster due to their short feedback loops. You see a problem, you fix it.&lt;/p&gt;
&lt;p&gt;Even internally, think about what makes up your company and how much of the internal infrastructure and software you want to own. If you are a small web shop, you’re good to &lt;em&gt;use&lt;/em&gt; existing frameworks and services. You can usually expect them to work for your use cases. If not, you need to fill tickets and wait for someone to fix them.
Now if you are company operating at large scale, you need to actually &lt;em&gt;own&lt;/em&gt; those services since you will run into issues nobody else ran into. In our open source world, this doesn’t necessary mean to write your own software but have people to &lt;em&gt;own&lt;/em&gt; those components, people able to understand their internals and able to fix bugs without any vendor supporting them. You see a problem you fix it.&lt;/p&gt;
&lt;h1&gt;What your teams should do&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/samurai-ce4c6510a30130413060085014fcb3b3-a6aba.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 79.26330150068213%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAQABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAIEA//EABUBAQEAAAAAAAAAAAAAAAAAAAEC/9oADAMBAAIQAxAAAAFJt0hnFE//xAAYEAEBAQEBAAAAAAAAAAAAAAACARESMf/aAAgBAQABBQJGKoHCZz7UbqVl/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGhAAAgIDAAAAAAAAAAAAAAAAAAEQESFRkf/aAAgBAQAGPwK2YEbQ3XI//8QAGxABAAMAAwEAAAAAAAAAAAAAAQARITFBYYH/2gAIAQEAAT8hziYRzaTcEO8+nkAQ1WClC48n/9oADAMBAAIAAwAAABA3P//EABYRAQEBAAAAAAAAAAAAAAAAAAEQEf/aAAgBAwEBPxBNn//EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQIBAT8QThp//8QAHBABAAMAAgMAAAAAAAAAAAAAAQARITFhQVGB/9oACAEBAAE/EFJY3JVBzBItYGwGy1rfcyLhg00+vcYmWB1oN3O5aLk5eJ//2Q==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Team&quot;
        title=&quot;&quot;
        src=&quot;/static/samurai-ce4c6510a30130413060085014fcb3b3-f8fb9.jpg&quot;
        srcset=&quot;/static/samurai-ce4c6510a30130413060085014fcb3b3-e8976.jpg 148w,
/static/samurai-ce4c6510a30130413060085014fcb3b3-63df2.jpg 295w,
/static/samurai-ce4c6510a30130413060085014fcb3b3-f8fb9.jpg 590w,
/static/samurai-ce4c6510a30130413060085014fcb3b3-a6aba.jpg 733w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
No matter how you build teams or how you call them, I believe they should be as independent from each other as possible while providing benefits to all other teams where possible.
As too many cooks spoil the broth, having many engineers working on the same code base usually isn’t working. You run into all kind of problems, from merge conflicts to bikeshedding and unclear escalation paths.&lt;/p&gt;
&lt;p&gt;The probably best way to avoid this is by splitting your monolithic application into (micro)services. Each teams develops their service independently. Since services depend on each other, you need to make your APIs backward compatible or introduce some kind of versioning.&lt;/p&gt;
&lt;h2&gt;You build it, you run it&lt;/h2&gt;
&lt;p&gt;Each team does not only develop their own service, they also deploy, operate and monitor it. You will never fully understand how an application behaves if you don’t run it on your own. With the right monitoring in place, it’s usually not that hard to isolate in which service’s domain a problem occurred. This guarantees short feedback loops and sets the right incentives to balance features and work on technical debt.
This also implies that there is &lt;em&gt;no ops team&lt;/em&gt;. Ops isn’t a team, it’s a role, something everyone should be doing.&lt;/p&gt;
&lt;h2&gt;Self-service&lt;/h2&gt;
&lt;p&gt;The teams being able to own each aspect of their service requires a self-service infrastructure.
It depends on how similar the requirements of different teams are and how much operational experience they have.
If you’re running on AWS, you might just give every team their own login and ability to spin up machines and resources as they need. But this requires quite some operational experience to make this secure and reliable and there will be lot of pieces that every team needs to re-invent.
Therefore, the self-service platform should address all needs that are similar across all your services and support. It might look like Heroku or Elastic Beanstalk but might be more opinionated and specific to your company.
Those infrastructure services are the same as user facing services. Some team builds and operates them, it’s simply a service for which customers are other engineers within your company.
This isn’t Ops and it’s not DevOps either, although due to the nature of the service it might involve more systems knowledge than frontend development.&lt;/p&gt;
&lt;h2&gt;Scaling beyond that&lt;/h2&gt;
&lt;p&gt;At large scale funny things happen and in a large company, you want people specialized in those problem. If you have less than, let say, 200 engineers, you probably don’t.
Those scale issues are similar across services but nothing you can easily abstract away. To solve those problems, you need to look at the big picture. The problem domain is different which is why companies like Google and facebook introduced a new role. Whether you call it &lt;em&gt;Site Reliability Engineering&lt;/em&gt; or just &lt;em&gt;Production Engineering&lt;/em&gt;, please don’t call it Ops and don’t use it synonymous with. It’s not about &lt;em&gt;operating&lt;/em&gt; anything.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Some things proposed here might not be trivial to implement. It’s hard to hire people with operational and development skills and a Microservice architecture opens up a whole new class of problems in the realm of distributed systems, but in the end most successfully companies seem to arrive at similar conclusions. If you disagree and choose a completely different approach, that’s fine as long as you do it deliberately.
Nothing is worse than having some specific form of organization without knowing why.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Building AWS AMIs from Scratch for Immutable Infrastructures]]></title><description><![CDATA[Why? I’m a big fan of what some might call “immutable infrastructures” which, to me, boils down to manging complexity by isolating change to…]]></description><link>https://5pi.de//2015/03/13/building-aws-amis-from-scratch/</link><guid isPermaLink="false">https://5pi.de//2015/03/13/building-aws-amis-from-scratch/</guid><pubDate>Fri, 13 Mar 2015 15:42:47 GMT</pubDate><content:encoded>&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/Rahn_Kloster_Sanct_Gallen_nach_Lasius_700-7183da7353128252eb634f7ebc882360-49927.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 60%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAIBAwX/xAAUAQEAAAAAAAAAAAAAAAAAAAAA/9oADAMBAAIQAxAAAAHYW6QGD//EABgQAAMBAQAAAAAAAAAAAAAAAAABAhEQ/9oACAEBAAEFAqvBU2aOV3//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAg/9oACAEBAAY/Al//xAAaEAEAAwADAAAAAAAAAAAAAAABABEhEDFh/9oACAEBAAE/IbCV1NQL8gq0jEU0lHH/2gAMAwEAAgADAAAAEEAP/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxABAAIDAQEAAAAAAAAAAAAAAQARIUFRMYH/2gAIAQEAAT8Qe1Crt2TLIL41GlUeDA2OA+wsCgDkCp//2Q==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Image&quot;
        title=&quot;&quot;
        src=&quot;/static/Rahn_Kloster_Sanct_Gallen_nach_Lasius_700-7183da7353128252eb634f7ebc882360-f8fb9.jpg&quot;
        srcset=&quot;/static/Rahn_Kloster_Sanct_Gallen_nach_Lasius_700-7183da7353128252eb634f7ebc882360-e8976.jpg 148w,
/static/Rahn_Kloster_Sanct_Gallen_nach_Lasius_700-7183da7353128252eb634f7ebc882360-63df2.jpg 295w,
/static/Rahn_Kloster_Sanct_Gallen_nach_Lasius_700-7183da7353128252eb634f7ebc882360-f8fb9.jpg 590w,
/static/Rahn_Kloster_Sanct_Gallen_nach_Lasius_700-7183da7353128252eb634f7ebc882360-49927.jpg 700w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;h1&gt;Why?&lt;/h1&gt;
&lt;p&gt;I’m a big fan of what some might call “immutable infrastructures” which, to me, boils down to manging complexity by isolating change to specific points in the life cycle of your services.
In practice this might mean you build your application image once and just recreate it if you need to update it.
Or you install your complete server once and just recreate it if you need to update it. Where this is harder on bare metal, AWS is a nice platform for this kind of immutable servers since it supports creating and running pre-built Amazon Machine Images.&lt;/p&gt;
&lt;p&gt;In general there are two ways to build AMIs: Spinning up a new instance, customize it and create a snapshot or build the AMI from scratch.&lt;/p&gt;
&lt;p&gt;The first option seems to be the most common case. There are several tools like &lt;a href=&quot;https://www.packer.io/&quot;&gt;packer&lt;/a&gt; or Netflix’ &lt;a href=&quot;https://github.com/Netflix/aminator&quot;&gt;aminator&lt;/a&gt; but since it requires to actually boot a existing AMIs, it has a few downsides. For once it requires a existing AMI which might include things you don’t need but I’m more worried about the fact that it requires to actually boot a machine. This breaks the clean separation of build- and runtime and might mess up your build artefacts. Think logfiles, dhcp/cloud-init configuration and so on. Sure, you can clean up those things but I prefer avoiding it in the first place by never entering the runtime before actual deployment.
Unfortunately, there isn’t very good tooling nor documentation around that, so I thought I share my findings with you.
Maybe it sparks the interest in building some good tooling around that - or maybe I get to it at some point.&lt;/p&gt;
&lt;h1&gt;Building the image&lt;/h1&gt;
&lt;p&gt;There are &lt;a href=&quot;http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html&quot;&gt;PV and HVM instances&lt;/a&gt; where from a AMI building perspective the biggest difference is that PV AMIs consist of a &lt;em&gt;filesystem&lt;/em&gt; image and a AKI (Amazon “kernel” image) reference. That kernel isn’t really a kernel but in fact pv-grub, a modified grub which, depending on the AKI, assumes a grub config in &lt;code&gt;(hd0)/boot/grub/menu.lst&lt;/code&gt; to chainload the specified kernel.&lt;/p&gt;
&lt;p&gt;A HVM image on the other hand is a &lt;em&gt;disk&lt;/em&gt; image with a MBR that gets executed on boot.&lt;/p&gt;
&lt;h2&gt;Paravirtual&lt;/h2&gt;
&lt;p&gt;Apparently HVM is the future, so feel free to skip this section.&lt;/p&gt;
&lt;p&gt;Creating a PV image is simple. This creates a 10G sparse file with ext4 filesystem and a label ‘root’. &lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;dd if=/dev/zero of=filesystem.img bs=1 count=0 seek=10G
mkfs.ext4 -L root -F filesystem.img&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Now you you can mount that image, install your OS (debootstrap comes handy) and create a menu.lst. You probably also want to install cloud-init to take care of your networking and fstab setup. You need to refer to the filesystem label you specified running &lt;code&gt;mkfs.ext4 -L&lt;/code&gt;. Something like this should do it:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;default 0
fallback 1
timeout 0
hiddenmenu

title My-AMI
root (hd0)
kernel /boot/vmlinuz root=LABEL=root console=hvc0
initrd /boot/initrd.gz&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;h2&gt;HVM&lt;/h2&gt;
&lt;p&gt;HVM images are a bit more tricky, since we need to create a complete disk image including partition and MBR:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;dd if=/dev/zero of=disk.img bs=1 count=0 seek=$SIZE 
fdisk filesystem.img &lt;&lt; EOF
n
p
1


w
EOF
DEV_DISK=$(losetup -f --show disk.img)
DEV=/dev/mapper/$(kpartx -av $DEV_DISK | cut -d&apos; &apos; -f3)&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Now you can mount &lt;code&gt;$DEV&lt;/code&gt; and install your OS. Running &lt;code&gt;update-grub&lt;/code&gt;/&lt;code&gt;grub-mkconfig&lt;/code&gt; should detect and use the disk images UUID. On some systems it seems to require a unmount/detach and remount of the disk image, otherwise you end up with &lt;code&gt;root=&lt;/code&gt; pointing &lt;code&gt;/dev/loopX&lt;/code&gt;. To detach the mappings and loop device once you’re done, run:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;kpartx -d $DEV_DISK
losetup -d $DEV_DISK&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;After that, you need to install grub to the MBR of the disk image by running:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;grub-install disk.img --root-directory $PWD/mnt&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;&lt;em&gt;Note: Took me some time to figure out that —root-directory needs an absolute path.&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;Bundle, Upload and register AMI&lt;/h1&gt;
&lt;p&gt;For some reasons there is no way to bundle and upload images with the new &lt;code&gt;aws&lt;/code&gt; cli, so we’re using the old AWS tools.
&lt;code&gt;ec2-bundle-image&lt;/code&gt; will turn the disk or filesystem images into bundles ready to be uploaded to S3 by &lt;code&gt;ec2-upload-bundle&lt;/code&gt;. Now you just need to register your AMI with &lt;code&gt;aws ec2 register-image&lt;/code&gt; and make sure &lt;code&gt;--virtualization-type&lt;/code&gt; matches the kind of image you built.
If you run in multiple regions, you can use &lt;code&gt;aws ec2 copy-image&lt;/code&gt; to replicate your AMIs across regions.&lt;/p&gt;
&lt;h1&gt;Debugging&lt;/h1&gt;
&lt;p&gt;If you’re AMI doesn’t boot, you can get the console output by running &lt;code&gt;aws ec2 get-console-output --instance-id ...&lt;/code&gt;. This is encoded as json but you can use the great tool &lt;code&gt;jq&lt;/code&gt; to get just the output including all it’s ANSI colored glory like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;aws ec2 get-console-output --instance-id ... | jq -r .Output&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;If you don’t want to wait for your ec2 instance at all, you can also just test the images locally. For HVM this is trivial. Just run &lt;code&gt;kvm disk.img&lt;/code&gt; or &lt;code&gt;qemu disk.img&lt;/code&gt;.&lt;/p&gt;
&lt;h1&gt;Automation and Integration&lt;/h1&gt;
&lt;p&gt;Right now I’m using a, pretty messy, Makefile to automate all this. It takes some files and a provisioner script to customize the AMIs. The general idea is to use some CI server to build fresh AMIs on every push and to just recreate your EC2 instances with the new AMIs once you want to roll out your changes.
It would be great to have proper tooling for all this. Packer looks promising, just misses a way to build from scratch.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Prometheus on Raspberry Pi]]></title><description><![CDATA[Prometheus Prometheus  is a new open-source service monitoring system and time series database written in Go. Check out the  announcement…]]></description><link>https://5pi.de//2015/02/10/prometheus-on-raspberry-pi/</link><guid isPermaLink="false">https://5pi.de//2015/02/10/prometheus-on-raspberry-pi/</guid><pubDate>Tue, 10 Feb 2015 13:18:12 GMT</pubDate><content:encoded>&lt;h1&gt;Prometheus&lt;/h1&gt;
&lt;p&gt;&lt;a href=&quot;http://prometheus.io&quot;&gt;Prometheus&lt;/a&gt; is a new open-source service monitoring system and time series database written in Go.&lt;/p&gt;
&lt;p&gt;Check out the &lt;a href=&quot;https://developers.soundcloud.com/blog/prometheus-monitoring-at-soundcloud&quot;&gt;announcement&lt;/a&gt; and my article about &lt;a href=&quot;/2015/01/26/monitor-docker-containers-with-prometheus&quot;&gt;monitoring Docker Containers with Prometheus&lt;/a&gt; if you don’t know what I’m talking about.&lt;/p&gt;
&lt;h1&gt;My Stack&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;http://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Raspberry_Pi_Photo.jpg/800px-Raspberry_Pi_Photo.jpg&quot; alt=&quot;RaspberryPi&quot;&gt;
I stopped running my own full blown server(s) a while ago. Nowadays I just have a old Raspberry Pi at home and two tiny DigitalOcean instances to host this blog and for general R&amp;#x26;D stuff.&lt;/p&gt;
&lt;p&gt;But I still want to monitor all this and, as showed earlier, Prometheus is the way to go. So I could now spin up another DigitalOcean instance and pay another $5/Mo, but given how cheap I am, I’d rather want to run it on my Raspberry Pi.&lt;/p&gt;
&lt;h1&gt;Cross-Compiling Go&lt;/h1&gt;
&lt;p&gt;First you need to build Go with support for your target OS and architecture. If you installed Go from sources, you can do this by running:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;cd $GOROOT/src
GOARCH=arm ./make.bash&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;With pure Go, cross compilation is trivial. This example:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;package main

import &quot;fmt&quot;

func main() {
	fmt.Println(&quot;Hello World&quot;)
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;can be cross-compiling for arm with &lt;code&gt;GOARCH=arm go build test.go&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now things get much more complicated once you use CGO, meaning Go code calling C functions.&lt;/p&gt;
&lt;p&gt;The Prometheus server uses the &lt;a href=&quot;https://github.com/prometheus/client_golang&quot;&gt;Prometheus Go Client Library&lt;/a&gt; to provide metrics about itself. This client library uses &lt;a href=&quot;https://github.com/prometheus/procfs&quot;&gt;prometheus/procfs&lt;/a&gt; which requires CGO to get process metrics from procfs. Cross-compiling this for Raspberry Pi is a pain. ARM != ARM, there are several variants and when I tried to cross-compiling Prometheus with CGO, it just lead to segfaults or invalid instructions. It should be possible to cross-compile it with the CGO dependency, it’s just painful and I quickly gave up.&lt;/p&gt;
&lt;p&gt;Fortunately, we found an easy way to remove the dependency on procfs if CGO is disabled: &lt;a href=&quot;https://github.com/prometheus/client_golang/commit/93d11c8e35ffcd969fd881efe1873e715a6ef93b&quot;&gt;https://github.com/prometheus/client_golang/commit/93d11c8e35ffcd969fd881efe1873e715a6ef93b&lt;/a&gt;. This removes some useful process level metrics about prometheus itself, but it makes cross compiling prometheus is as easy as in the example above:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;cd $GOPATH/src/github.com/prometheus/prometheus
go get -u # Update your dependencies
GOARCH=arm go build -o prometheus.arm&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Since cross-compilation by default disables CGO, this builds a statically linked prometheus binary ready to be run on a Raspberry Pi. If you’re running a newer Pi, you can set GOARM to the version of your ARM processor. See &lt;a href=&quot;https://code.google.com/p/go-wiki/wiki/GoArm#Supported_architectures&quot;&gt;this&lt;/a&gt; for supported architectures.&lt;/p&gt;
&lt;h1&gt;Performance&lt;/h1&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-346ce.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 70.935960591133%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsSAAALEgHS3X78AAACCElEQVQoz32S3W4SURDH93lsMSFKSHgbrhoJYJ+hF175Dr6BMV6JjTEaDa0KNhS10nZxoYBQlt1zds/3R52zKxWM8ZfztbMz85+ds165XK5UKqVS6f69PxSLxUKhcHebnd3CnZ3dTYs3m8183w+CYHF9HYZhtFpFESxbLMOQJujJy97jp504A2OMEPKsMZQQRqnS+ua/CO0GYK3lnAshPQG7VFzZWye7Bo7/zGKMSUkqhHDKjHPGhVKKAZxBCdpYGNzllwBsSZJuJP2NB45KSphGOxSjMkESxTLFkEySFIwiweCjBTcZihKJY5sFazLy+c8rGYWaEj670gk2gmtOLYguF4bR5OxUzGcqQTfWgCCbjsDNBUOm+LSLv5zQ6YhNRun5V5t1Li8PMkJM1D3GvQ681Sk2SpHhgI6HedmaTsZkeA6m+PNxenEGcS7UOBG5WqYX36KP78L3r1C/S/zvbD4BZzoJsrKNNpkfeG82Iz9DzZBxdfRm8eJZ3Gnjfjdsv6Yjn03HWkqnDH3OlMztCoDdPWod9z6FR2/jbjs++UD8weLweTLoC+iFkp7ZFvwba+V8yoNLuZyz8Q8aXEJT4cuh8xDllJMM7EDJGvj7ciMKl3DmjEFVEkWgY9YX7sF1PDo4eLC312g0G416A7ZmE2atVoNzvV5/uL9frVZbrRbUoXT+M6i83l8eat949cKCqQAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;goroutines&quot;
        title=&quot;&quot;
        src=&quot;/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-fb8a0.png&quot;
        srcset=&quot;/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-1a291.png 148w,
/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-2bc4a.png 295w,
/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-fb8a0.png 590w,
/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-526de.png 885w,
/static/rpi_gorouting-0c0309899e4000452ea789b5324129b0-346ce.png 1015w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    
I didn’t have time to benchmark it yet, but it seems to perform much better than I expected. Right now, it only scrapes itself, giving us ~250 time series. After running it for a few hours it already collected 137806 samples. Graphing a simple time series like &lt;code&gt;process_goroutines&lt;/code&gt; for the past hour takes between 150-170ms. The probably most expensive operation you can do (and definitely shouldn’t do on a production system) is graphing &lt;em&gt;all&lt;/em&gt; time series by executing &lt;code&gt;{job=~&quot;.*&quot;}&lt;/code&gt; this returns in about 30s on the Raspberry Pi.

  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-7492f.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 66.40079760717846%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsSAAALEgHS3X78AAABy0lEQVQoz5VSWUsbURSeH1S0RqgNQUF86FtB/AG+WwWVxo2AedAHQ1SMWkrcoBEF9zSaxTXgg4gQERqUgi+aaOLMOHbmznrn3ngmrglV9GOYOZxzvvN9c+5lHA5HZWWV3W7//AR7RcUnm81WXoiSj2UfSsueZ5hsJnORTrMcx/O8IAg3/wNkkfgvsJ0cCh2KooiQhGRZkiSGUqqqqqZphmHkXgUmOd20AtM0EULQz+i6pulYNWiOvkiDCqH3ZUKJgQ1FkWEEKBPQVVQNYwwWFEWBkWYeYAc8URMfpwT/5t8Uh6xBDxpgmYEmoBFiFoE8pOCj6lhAGjaJrMpz8ZWRoD+0FwY+Q/K4m/Tkkxb8A80Dgsvry7rur1+c1T2B7neTU1m2tnGwpr6/ayBokcEzeHsjOcMK39q9Dc5et8d7fcMxj4XXybBkCBLJxOiPpsnx5qmJ7zx3ca8MtbslASg8VmsBH9YH7/ju74X5xli0IxZ1iWKGKRIpOl44Brg9kiJfCXzy9HQt6luPtG7E2rc2XJKUtcjn6bM/J0eJo/29g/jObiSyubS8Oj27OPZrZtg/5fH9dHt9HQNDzj5P03SgJRbpDAXbomEXy57dAlm/nQFJ4GwcAAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;goroutines&quot;
        title=&quot;&quot;
        src=&quot;/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-fb8a0.png&quot;
        srcset=&quot;/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-1a291.png 148w,
/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-2bc4a.png 295w,
/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-fb8a0.png 590w,
/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-526de.png 885w,
/static/rpi_all-1-41ee3e1033ec4be8cbd8b134b87fc946-7492f.png 1003w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;h1&gt;Downloads&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; There are now official images available:
&lt;a href=&quot;https://prometheus.io/download/&quot;&gt;https://prometheus.io/download/&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Checksums&lt;/h3&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;$ shasum prometheus.armv?.gz
db42a3f568bbab1d8a7d183b336d0b50dccc80b6  prometheus.armv5.gz
b6fdd3a77e16359631a0daaf9c06cd93a9948932  prometheus.armv6.gz
8d3e6580ee4ed3d10b93d60c13009156a5600193  prometheus.armv7.gz

$  sha256sum prometheus.armv?.gz
e206202bb07cc139eaabac4c51c07f0a332337eb331bd79f19294c558fb3de62  prometheus.armv5.gz
99d864e4fee8ded6b0f9c117f839fdf0fa9d12fadfe27b98090bee04b88c48c0  prometheus.armv6.gz
baf550448174198c57f3a28e2b86d49ba523e5b15d9d4c087267021e4785299b  prometheus.armv7.gz&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;</content:encoded></item><item><title><![CDATA[Find GHOSTs in your Docker images]]></title><description><![CDATA[A severe security vulnerability in glibc < 2.18, nicknamed  GHOST  was just reported.
Here is a handy one-liner (Debian/Ubuntu only though…]]></description><link>https://5pi.de//2015/01/27/find-ghosts-in-your-docker-images/</link><guid isPermaLink="false">https://5pi.de//2015/01/27/find-ghosts-in-your-docker-images/</guid><pubDate>Tue, 27 Jan 2015 17:30:17 GMT</pubDate><content:encoded>&lt;p&gt;A severe security vulnerability in glibc &amp;#x3C; 2.18, nicknamed &lt;a href=&quot;http://www.openwall.com/lists/oss-security/2015/01/27/9&quot;&gt;GHOST&lt;/a&gt; was just reported.
Here is a handy one-liner (Debian/Ubuntu only though) to walk through all your Docker images and see if they include a glibc older than 2.18:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;docker images -q | while read I; do V=`docker run --rm --entrypoint apt-cache $I policy libc6 2&gt;/dev/null | awk &apos; /Installed/ { print $2&quot;\n&quot;2.18 }&apos;|sort -V|head -1`; if [ -z &quot;$V&quot; ]; then echo &quot;$I not apt based&quot; &amp;&amp; continue; fi;  [ &quot;$V&quot; == &quot;2.18&quot; ] || echo &quot;$I is vulnerable&quot;; done&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;</content:encoded></item><item><title><![CDATA[Monitor Docker Containers with Prometheus]]></title><description><![CDATA[Monitoring Docker Running all your services in containers makes it possible to get in-depth resource and performance characteristics, since…]]></description><link>https://5pi.de//2015/01/26/monitor-docker-containers-with-prometheus/</link><guid isPermaLink="false">https://5pi.de//2015/01/26/monitor-docker-containers-with-prometheus/</guid><pubDate>Mon, 26 Jan 2015 15:15:12 GMT</pubDate><content:encoded>&lt;h3&gt;Monitoring Docker&lt;/h3&gt;
&lt;p&gt;Running all your services in containers makes it possible to get in-depth resource and performance characteristics, since every container runs in their own cgroup and the Linux kernel provides us with all kind of useful metrics.&lt;/p&gt;
&lt;p&gt;Although there are a few other Docker monitoring tools out there, I’ll show you why I think that SoundCloud’s &lt;a href=&quot;https://developers.soundcloud.com/blog/prometheus-monitoring-at-soundcloud&quot;&gt;newly released&lt;/a&gt; &lt;a href=&quot;https://prometheus.github.io/&quot;&gt;Prometheus&lt;/a&gt; is a perfect fit for monitoring container-based infrastructures.&lt;/p&gt;
&lt;p&gt;Prometheus features a highly dimensional &lt;a href=&quot;http://prometheus.github.io/docs/concepts/data_model/&quot;&gt;data model&lt;/a&gt; in which a time series is identified by a metric name and a set of key-value pairs. A flexible &lt;a href=&quot;http://prometheus.github.io/docs/querying/basics/&quot;&gt;query language&lt;/a&gt; allows querying and graphing this data. It features advanced &lt;a href=&quot;http://prometheus.github.io/docs/concepts/metric_types/&quot;&gt;metric types&lt;/a&gt; like summaries, building &lt;a href=&quot;http://prometheus.github.io/docs/querying/functions/#rate()&quot;&gt;rates&lt;/a&gt; from totals over specified time spans or &lt;a href=&quot;http://prometheus.github.io/docs/querying/rules/&quot;&gt;alerting&lt;/a&gt; on any expression and has no dependencies, making it a dependable system for debugging during outages.&lt;/p&gt;
&lt;p&gt;I will focus on why especially the data model and query language makes it such a good fit in a containerized, dynamic infrastructure where you think in clusters of services instead of single instances and servers as cattle instead of pets.&lt;/p&gt;
&lt;h3&gt;Classic Approach&lt;/h3&gt;
&lt;p&gt;Let’s say you want to monitor the memory usage of your containers. Without support for dimensional data, such a metric for the container named &lt;code&gt;webapp123&lt;/code&gt; might be called &lt;code&gt;container_memory_usage_bytes_webapp123&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;But what if you want to show the memory usage of all your &lt;code&gt;webapp123&lt;/code&gt; containers? More advanced monitoring solutions like &lt;a href=&quot;https://github.com/graphite-project&quot;&gt;graphite&lt;/a&gt; support that. It features a hierarchic, tree-like data model in which such metric might be called &lt;code&gt;container.memory_usage_bytes.webapp123&lt;/code&gt;. Now you can use wildcards like &lt;code&gt;container.memory_usage_bytes.webapp*&lt;/code&gt; to graph the memory usage of all your ‘webapp’ containers. Graphite also supports functions like &lt;code&gt;sum()&lt;/code&gt; to aggregate the memory usage of your application across all your machines by using an expression like &lt;code&gt;sum(container.memory_usage_bytes.webapp*)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;That’s all great and very useful, but limited. What if you don’t want to aggregate all containers with a given name but with a given image? Or you want to compare the deployments to your canary with those on your prod machines?&lt;/p&gt;
&lt;p&gt;It’s possible to come up with a hierarchy for each use case, but not one to support them all. And reality shows, you often don’t know in advance which questions you need to answer once things go dark and you start investigating.&lt;/p&gt;
&lt;h3&gt;Prometheus&lt;/h3&gt;
&lt;p&gt;With Prometheus’s support for dimensional data, you can have global and straightforward metric names like &lt;code&gt;container_memory_usage_bytes&lt;/code&gt; with multiple dimensions to identify the specific instances of your service.&lt;/p&gt;
&lt;p&gt;I’ve created a simple &lt;a href=&quot;https://github.com/docker-infra/container_exporter&quot;&gt;container-exporter&lt;/a&gt; to gather Docker Container metrics and expose them for Prometheus to consume. This exporter uses the container’s name, id and image as dimensions. Additional per-exporter dimension can be set in the &lt;code&gt;prometheus.conf&lt;/code&gt;.
If you use the metric name directly as a query expression, it will return all time series with their labels for this metric name:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;container_memory_usage_bytes{env=&quot;prod&quot;,id=&quot;23f731ee29ae12fef1ef6726e2fce60e5e37342ee9e35cb47e3c7a24422f9e88&quot;,instance=&quot;http://1.2.3.4:9088/metrics&quot;,job=&quot;container-exporter&quot;,name=&quot;haproxy-exporter-int&quot;,image=&quot;prom/haproxy-exporter:latest&quot;}	11468800.000000
container_memory_usage_bytes{env=&quot;prod&quot;,id=&quot;57690ddfd3bb954d59b2d9dcd7379b308fbe999bce057951aa3d45211c0b5f8c&quot;,instance=&quot;http://1.2.3.5:9088/metrics&quot;,job=&quot;container-exporter&quot;,name=&quot;haproxy-exporter&quot;,image=&quot;prom/haproxy-exporter:latest&quot;}	16809984.000000
container_memory_usage_bytes{env=&quot;prod&quot;,id=&quot;907ac267ebb3299af08a276e4ea6fd7bf3cb26632889d9394900adc832a302b4&quot;,instance=&quot;http://1.2.3.2:9088/metrics&quot;,job=&quot;container-exporter&quot;,name=&quot;node-exporter&quot;,image=&quot;prom/container-exporter:latest&quot;}
...
...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;If you run lot of containers, this looks something like this:&lt;/p&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/container_memory_usage_bytes-065a19374aba5620f406a9361ff35444-fd026.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 71.39240506329114%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsSAAALEgHS3X78AAACGklEQVQoz4VR2W7aUBD1H/Yb+kl97x/0oVWVhbKENA5LCAlWqHAwYJYEgvF+fX13m47tplJaVT06uvfOmbE8Z0YzTXNkGA8lDMNYr9eUUoxx+gp4ckr2Pnr3Yfz+o5kSyNM8zy3L0lzPdQ4OwHXdOI6hWgGy7BcrZJlUKkI8xhwiIcTxeFzYtrbbboMg3O22K8ByOZvNLGv6G0VUhnAt7fliPoM3NGvbC13XNWe/dz3/4Hoowdvdfrna7B03jGKgH4TrzfPeOXh+AFzYS0glCYliRJmYTEyNEEIZQ+CRsiQlmJAUJMYqQlgojEEWF1laiSrPH6dTDUxKCU6AApgVJ5flqQpdFNmSWSaqUAgGnsFR8XGWZcd/I8//VKr6YmDQI+E0IgixNHlLVDKm+I0IISXqeDRhVWBRKimLVrl6bayyoEpFSsYlK/SqBjYlKfzemj3+v+2/UdXbVduY4BfnefuyQTjyva3ne0yIJHbjcAdDCiLvabeKkXc4OEEYSgFDVeXAZprgcv00H45a40n3ZnR196PTG9b695f3427//qJ3174dNQajdmdQNybXhtm9Hevj6fBhelNrftEY45xhSiJCQkoRFymlIUI+7ENKEoWOFImSKedYqVSKGJiSIEncycQoBmaurJPrxle9ddZptgaN1uCi3m/XB+16/7I11M96l7Veu3nTPL+qn37/dq7XT65qtf7Fp9PPPwEqYAWEaDEQMwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;container_memory_usage_bytes.png&quot;
        title=&quot;&quot;
        src=&quot;/static/container_memory_usage_bytes-065a19374aba5620f406a9361ff35444-fb8a0.png&quot;
        srcset=&quot;/static/container_memory_usage_bytes-065a19374aba5620f406a9361ff35444-1a291.png 148w,
/static/container_memory_usage_bytes-065a19374aba5620f406a9361ff35444-2bc4a.png 295w,
/static/container_memory_usage_bytes-065a19374aba5620f406a9361ff35444-fb8a0.png 590w,
/static/container_memory_usage_bytes-065a19374aba5620f406a9361ff35444-fd026.png 790w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;p&gt;To help you make sense of this data, you can filter and/or aggregate these metrics&lt;/p&gt;
&lt;h4&gt;Slice &amp;#x26; Dice&lt;/h4&gt;
&lt;p&gt;With Prometheus’s query language, you can slice and dice data by any dimension you want. If you’re interested in all containers with a given name, you can use an expression like &lt;code&gt;container_memory_usage_bytes{name=&quot;consul-server&quot;}&lt;/code&gt;, which would show only the time series for which &lt;code&gt;name == &quot;consul-server&quot;&lt;/code&gt;.
Prometheus also supports regular expressions, so instead of matching the full script you can do &lt;code&gt;container_memory_usage_bytes{name=~&quot;^consul&quot;}&lt;/code&gt;, which would show something like this:&lt;/p&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/container_memory_usage_bytes_consul-2a7a760e3618b0ac5e2f4d9ba57082c5-8dc3d.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 66.50998824911868%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAAsSAAALEgHS3X78AAABeElEQVQoz4VSW27CMBDMeXuNXqVH6FUKSVClIoQohcQhD9u7azudjVtA7QeriTN+rHc8dtH3l9Pp1DZt0zRt2xpjegxdLiwSQkAzp3ju6enl/fl1N89zSnOOlFLRdd3hcDh+HhFIG4bBOWet9d4TkV/COm86ZzrrNNDz+GG8QNnz6bzf73e7XVmWq9WqLKu6rquqAl+v1yDolOu3TV1t6jqjKsuP7bYYhxHFp2nEdm3bGNNiWyYCjOmGvp8mO4zT8fhl+nF0PFjfW7KeY4wFPkdsSQDHQHDgnh0tHJDoOBIJWcvTyHbS1jscW5MRcAEf2owbR4gkZ5N3KQS1awG4Gqaz8CZv+Yu7rhIRoRh9CFeQiFbGhBrH0EloM+65puHGVF26IsSlcpY9PwosjQEpMSPkyqof++JFsMSF3HMQlQEDPQvdwMSajGfEaiUk0m0OPK9mSTH9l4YxlS3LM3yk+e+IXlB2O+hphGFh5KxbIDf88Ny95wv0zN9r5fEh8xVSsAAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;container_memory_usage_bytes_consul.png&quot;
        title=&quot;&quot;
        src=&quot;/static/container_memory_usage_bytes_consul-2a7a760e3618b0ac5e2f4d9ba57082c5-fb8a0.png&quot;
        srcset=&quot;/static/container_memory_usage_bytes_consul-2a7a760e3618b0ac5e2f4d9ba57082c5-1a291.png 148w,
/static/container_memory_usage_bytes_consul-2a7a760e3618b0ac5e2f4d9ba57082c5-2bc4a.png 295w,
/static/container_memory_usage_bytes_consul-2a7a760e3618b0ac5e2f4d9ba57082c5-fb8a0.png 590w,
/static/container_memory_usage_bytes_consul-2a7a760e3618b0ac5e2f4d9ba57082c5-8dc3d.png 851w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;p&gt;You can use any other dimension for filtering as well, so you can get metrics about all containers on a given host, in a given environment or zone.&lt;/p&gt;
&lt;h4&gt;Aggregation&lt;/h4&gt;
&lt;p&gt;Similar to Graphite, Prometheus supports aggregation functions but due to its dimensions this gets even more powerful. Summing up the memory usage of all your “consul-*” works as expected with &lt;code&gt;sum(container_memory_usage_bytes{name=~&quot;^consul&quot;})&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now let’s say you want to see the difference in average memory usage between your ‘consul’ and ‘consul-server’ containers. This is achieved by providing the dimensions to keep in the aggregated result like this &lt;code&gt;avg(container_memory_usage_bytes{name=~&quot;^consul&quot;}) by (name)&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/container_memory_usage_bytes_consul_by_name-1-b0d3d92bda3203b73787ae1285ca75bc-e00a9.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 76.15112160566706%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAAAsSAAALEgHS3X78AAABlElEQVQoz5WSy0rDQBSG84x2IT6AuvMRBAtVEStSXfgGWgviShAFF76BQi4KQqXS5p4m6SQzc2bimcS2USvYP3/gn8uXcyaJFkWR67q+7wdBEIYhhjEqjjHjEsmyQkKQ5lvnb9vX/aKQdWm4c2SPHNfxPC8tldfEGAPOKeNOmHlRzjlnpXgpzXEc27YHg0G//26apq7rppJlWS+GYRo63paFY0N/tcxqg1EKsxaPx9hxkiSEENdzPc8nE7zSeBINnQ/HHwaxFyb+YNgPogCAF6WwZ1VZoECUE8ogIQeSwwQkF1IICWipgpCFMCy9tdvaO2g9PN4joQGHnBEKysggibtnjy+mqrI9snu9Xvei+/T8pGB1cKiKKBeyakJ+Z7/42VQVFAwAPyr8zrMZKIUvfDm4PqyQ/8L1npeG/6yMEnPV4B8HruU5jMS3V8KoYAwNlHL6ldGc5rMlmmfq30bg7vbmuL1/dnraaR9eXXZ5RqA0BpomMB3SNOVkUmWWESyp4JPO0drqyvrGZqPR2Gk2F36khfoExtdVxGghHGUAAAAASUVORK5CYII=&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;container_memory_usage_bytes_consul_by_name&quot;
        title=&quot;&quot;
        src=&quot;/static/container_memory_usage_bytes_consul_by_name-1-b0d3d92bda3203b73787ae1285ca75bc-fb8a0.png&quot;
        srcset=&quot;/static/container_memory_usage_bytes_consul_by_name-1-b0d3d92bda3203b73787ae1285ca75bc-1a291.png 148w,
/static/container_memory_usage_bytes_consul_by_name-1-b0d3d92bda3203b73787ae1285ca75bc-2bc4a.png 295w,
/static/container_memory_usage_bytes_consul_by_name-1-b0d3d92bda3203b73787ae1285ca75bc-fb8a0.png 590w,
/static/container_memory_usage_bytes_consul_by_name-1-b0d3d92bda3203b73787ae1285ca75bc-e00a9.png 847w&quot;
        sizes=&quot;(max-width: 590px) 100vw, 590px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;p&gt;If you have services in multiple zones and configure the zone name as an additional label pair, you can also keep that dimension to show the memory usage per name and zone by using an expression like &lt;code&gt;avg(container_memory_usage_bytes{name=~&quot;^consul&quot;}) by (name,zone)&lt;/code&gt;.&lt;/p&gt;
&lt;h4&gt;Using Prometheus + Container-Exporter&lt;/h4&gt;
&lt;p&gt;As you know, I like to run everything in containers, including the container-exporter and Prometheus itself. Running the &lt;a href=&quot;https://github.com/docker-infra/container_exporter&quot;&gt;container-exporter&lt;/a&gt; should be as easy as this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;docker run -p 8080:8080 -v /sys/fs/cgroup:/cgroup \
           -v /var/run/docker.sock:/var/run/docker.sock prom/container-exporter&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Now you need to install Prometheus. For that refer to the &lt;a href=&quot;http://prometheus.github.io/docs/introduction/install/&quot;&gt;official documentation&lt;/a&gt;. To make Prometheus pull the metrics from the container-exporter, you need to add it as a target to the configuration. For example:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;job: {
  name: &quot;container-exporter&quot;
  scrape_interval: &quot;1m&quot;
  target_group: {
  	labels: {
	    label: {
    		name: &quot;zone&quot;
        	value: &quot;us-east-1&quot;
	    }
        label: {
        	name: &quot;env&quot;
            value: &quot;prod&quot;
        }
    }
    target: &quot;http://1.2.3.4:8080/metrics&quot;
  }
}&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Now rebuild your image as described in the documentation and start it. Prometheus should now poll your container-exporter every 60s.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;Because of Prometheus flexibility, its performance and minimal requirements, it’s the monitoring system of my choice.
This is why I introduced Prometheus beginning last year at Docker where we use it as our primary monitoring system.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Containerize your Infrastructure]]></title><description><![CDATA[The Hype Everyone is talking about containers.
Containers are a great way to bundle your application with all dependencies and isolate it…]]></description><link>https://5pi.de//2015/01/08/containerized-infrastructure/</link><guid isPermaLink="false">https://5pi.de//2015/01/08/containerized-infrastructure/</guid><pubDate>Thu, 08 Jan 2015 14:43:00 GMT</pubDate><content:encoded>&lt;h3&gt;The Hype&lt;/h3&gt;
&lt;p&gt;Everyone is talking about containers.
Containers are a great way to bundle your application with all dependencies and isolate it from other applications running on the same host.
That’s all great but we never cared that much about containerization even though it exists for quite some time: chroot() was born 1979 and IBM had tech similar to containers on S/370 over 40 years ago.&lt;/p&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/mainframe-771312892d96c881b4ae67cf16d6b28c-ec5fe.jpg&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 540px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 71.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAQFA//EABYBAQEBAAAAAAAAAAAAAAAAAAIAAf/aAAwDAQACEAMQAAABej1Uwsiqbf/EABoQAAMAAwEAAAAAAAAAAAAAAAECEQADEjL/2gAIAQEAAQUCaKGcnc3NNgAKes//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAACAQUAAAAAAAAAAAAAAAAAARECEiExUf/aAAgBAQAGPwLRZqmeGMohwLrJtpP/xAAaEAEBAAMBAQAAAAAAAAAAAAABEQAhMUFR/9oACAEBAAE/IbzEDuaGNG9TIskOP3Jrcmk8x/b6Jq4UUmf/2gAMAwEAAgADAAAAEDz/AP/EABYRAAMAAAAAAAAAAAAAAAAAAAABEf/aAAgBAwEBPxCorP/EABgRAAIDAAAAAAAAAAAAAAAAAAABESFB/9oACAECAQE/EIZSw//EABwQAQEAAwADAQAAAAAAAAAAAAERACExQVFx8f/aAAgBAQABPxBNHCqN8yaQkJCeuBIQmvT3jdbMM7HlF38zYqbG0LLMI03K/mf/2Q==&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Grumpy Ops&quot;
        title=&quot;&quot;
        src=&quot;/static/mainframe-771312892d96c881b4ae67cf16d6b28c-ec5fe.jpg&quot;
        srcset=&quot;/static/mainframe-771312892d96c881b4ae67cf16d6b28c-59252.jpg 148w,
/static/mainframe-771312892d96c881b4ae67cf16d6b28c-20821.jpg 295w,
/static/mainframe-771312892d96c881b4ae67cf16d6b28c-ec5fe.jpg 540w&quot;
        sizes=&quot;(max-width: 540px) 100vw, 540px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;p&gt;So why all the hype right now? It’s not like we just started with all that.
There are various use cases for containers but here I’ll focus on use them as building blocks in modern IT infrastructures.&lt;/p&gt;
&lt;h3&gt;Infrastructure evolution&lt;/h3&gt;
&lt;p&gt;If we look back 10 or maybe 15 years, things looked very differently.
It’s not particular news that software eats the world and every company today is, at least partially, an IT company. But back in the 90s, IT was still something for rather specialized companies. Finance, Insurance, Government, Health care. Basically companies juggling lots of numbers.&lt;/p&gt;
&lt;p&gt;Big and monolithic software and services designed by a waterfall model. Release cycles in the terms of months and large Ops departments managing them. All scaled mostly vertically by just buying bigger boxes.
Things happened really slow back in the days. Despite the fact that there are still lot of companies stuck back then, even in traditionally more conservative businesses like finance you see &lt;a href=&quot;https://www.youtube.com/watch?v=6FPXbQ2WpAM&quot;&gt;a radical shift towards agile development methodologies&lt;/a&gt;. Why this is a crucial move and I don’t belive any company will survive without being “agile” is probably a topic for another blog article.&lt;/p&gt;
&lt;p&gt;Fact is, today things are different. To be agile means to be fast, to be able to adapt to a changing environment quickly. The result is a infrastructure optimized for change. Instead of having monolithic application where every change needs to be coordinated with every stakeholder, such infrastructures consist of dozens of loosely coupled (micro)services with independent teams iterating on them, scaled horizontally across hybrid infrastructures with some workload on bare metal, other on cloud instances.
Instead of a release every few weeks, it’s not uncommon to deploy multiple times a day fully automated by CI/CD pipelines.&lt;/p&gt;
&lt;h3&gt;Challenges&lt;/h3&gt;
&lt;p&gt;Now we have more services, more changes, more systems and arguably less time to market. Fact is, it’s incredibly hard to really understand such infrastructure. Just think about what it means to manage a single service. To deploy it, monitor it, update it.  Now multiply by a few dozens. The millions of knobs and billions of permutations. Nobody can manage that.&lt;/p&gt;
&lt;p&gt;
  &lt;a
    class=&quot;gatsby-resp-image-link&quot;
    href=&quot;/static/automate-7971ab1879abb9b42b152dd5b66ef7bf-74bd4.png&quot;
    style=&quot;display: block&quot;
    target=&quot;_blank&quot;
    rel=&quot;noopener&quot;
  &gt;
  
  &lt;span
    class=&quot;gatsby-resp-image-wrapper&quot;
    style=&quot;position: relative; display: block; ; max-width: 500px; margin-left: auto; margin-right: auto;&quot;
  &gt;
    &lt;span
      class=&quot;gatsby-resp-image-background-image&quot;
      style=&quot;padding-bottom: 71.00000000000001%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAARlAAAEZQAGA43XUAAACpklEQVQoz12RX0xSURzH70NrPffQ1vKhZrZeyq22Wg/Viz2x5VbNLedWq7aeemnT5ZbVyv7AfEiRzLGVkU5JWoIiEOvCFcj4MxXxIiggwaIQCJCa697fOadzQdH67d5zf/fc3+f3/f7uYQghGGO6xmIxv98fCoXm5+cTiUQwGJyZmVlaWgoEAgsLCzSnCc/zkUiEbAZTIWlYLJaurq7R0XcajUar1Y6NjZnNZpqPjIy4XC6r1cqyrFqtNhj0ALABV9t4vR5aajIZPR63iT6MEw7HFO1FsfFxg83GOhz2gYFXLMtRr//DSAqpKcaIXqIoiCKUbYEgCKVSiX5C6A9Cv7ZsV1npxl9FUEkwWcdYwJiu6xINa9FoDCACSI5QuFyPt8O48g5wAtB5QJ8R7gfch7CTkEVBaE7/uAWwC1BfuXjbzNQbxiLChbL55wgzCNcWinWJpGyteK20diqb3ZvJ0MrLlSkqNqvKqMxbAXUD4gH20f3lyJFw2OqfGzab3/vnOlZX9xCyXIbRPzPn87nFRW16NYVxuwgHAdVQ8Vj0tkLRV1Ozv7GxOZF0xxNthGTLGikAC8alDTge57PZ69l87+/1MMBuEegmUygceNnfzzBM7aG6oG92ha8npFUQ7gtiI6BPW8qCIGZyLi/3dCVoJKQJ05nRjnSa+Z6S37vzRNGr+HZTn5bXErITi1cIKW3/2whLUwR8zg7VsybWftLnO2y3tdjsMr3uzLhmaFI9/PFsp+3uDc4r+zI9m0oNEaJGqLAJS104j6/hgz7c+WhQq7uoUr3V6ZI9vQ96Xlx9M2l87TW4knFl94RSqXG7mwhpxzhTPSpJOhQadHsaph2XNMpjdu6o03HOx9ezrbKp44+dpx9yF9qcjhaOM/F89Gce53LFQrH4F8AJr4gJDWm4AAAAAElFTkSuQmCC&apos;); background-size: cover; display: block;&quot;
    &gt;
      &lt;img
        class=&quot;gatsby-resp-image-image&quot;
        style=&quot;width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;&quot;
        alt=&quot;Automate all the things&quot;
        title=&quot;&quot;
        src=&quot;/static/automate-7971ab1879abb9b42b152dd5b66ef7bf-74bd4.png&quot;
        srcset=&quot;/static/automate-7971ab1879abb9b42b152dd5b66ef7bf-309d6.png 148w,
/static/automate-7971ab1879abb9b42b152dd5b66ef7bf-fe30e.png 295w,
/static/automate-7971ab1879abb9b42b152dd5b66ef7bf-74bd4.png 500w&quot;
        sizes=&quot;(max-width: 500px) 100vw, 500px&quot;
      /&gt;
    &lt;/span&gt;
  &lt;/span&gt;
  
  &lt;/a&gt;
    &lt;/p&gt;
&lt;p&gt;So what do we do? We automate.
Historically, you had some (shell) scripts to manage things in a imperative way. Whether it’s creating some users or deploying an app.&lt;/p&gt;
&lt;p&gt;Nowadays almost everyone uses some sort of configuration management systems. The mother of modern configuration management is probably &lt;a href=&quot;http://en.wikipedia.org/wiki/CFEngine&quot;&gt;CFEngine&lt;/a&gt;, even though the idea goes back to the &lt;a href=&quot;http://en.wikipedia.org/wiki/Configuration_management#History&quot;&gt;50s&lt;/a&gt;.
As oppose to the imperative approach, the idea here is declarative. You describe the desired state of your infrastructure and have some logic in place to actual tune all those knobs. Running you config management tool &lt;em&gt;converges&lt;/em&gt; your infrastructure towards the desired state. In theory, no matter what state your system is in right now, the configuration management system will converge the system to the desired state eventually.&lt;/p&gt;
&lt;p&gt;A big advantage over the imperative approach. But in reality, there are just too many knobs and unexpected side effects for this to be very reliable.
In the end, state convergence helps with managing change in your infrastructure but it doesn’t make the infrastructure itself simpler. You still first need to understand all the dependencies and interactions between your knobs. You need to somehow reduce the complexity, the number of knobs.&lt;/p&gt;
&lt;h3&gt;Managing Complexity&lt;/h3&gt;
&lt;p&gt;The most important aspect of managing any kind of system is to understand it. To understand how various components influence each other, which parts are healthy and which not and ultimately understand the effects of every possible change to your infrastructure.
Containers help to manage complexity.&lt;/p&gt;
&lt;h4&gt;Abstraction&lt;/h4&gt;
&lt;p&gt;Every time the cognitive overhead imposed by a system growing in complexity reaches a certain threshold, people start to &lt;em&gt;abstract&lt;/em&gt; things.
It probably started in human communication. We don’t have to be biologists to talk about cats. We don’t need to understand every intrinsic &lt;em&gt;implementation detail&lt;/em&gt;. Same for classes or modules in programming languages. And the same is true for containers: They are an abstraction around a specific application, abstracting all its implementation details and provide a generic interface shared by all other applications. All the knobs are still there, but you hide those you don’t care about.&lt;/p&gt;
&lt;p&gt;Given the common interface among all your applications, all your systems are doing is running containers. Your DB servers? Running containers. Your application servers? Running containers. Which means instead of having a bunch of servers for different use cases, you only have servers running containers. This allows for systems like cluster schedulers to abstract away the servers and just present a block of resources to run containers on.&lt;/p&gt;
&lt;p&gt;You might argue that the complexity isn’t gone, it’s just hidden and that’s true. But the same is true for abstractions in linguistics. Stereotypes are abstractions as well and they often do not accurately reflect reality. Just take Gender as an example. It’s important to keep in mind that &lt;a href=&quot;http://www.joelonsoftware.com/articles/LeakyAbstractions.html&quot;&gt;all abstractions are leaky&lt;/a&gt; and you still need to understand the implementation details or layers below. But you don’t need to think about them all the time.&lt;/p&gt;
&lt;h4&gt;Ownership&lt;/h4&gt;
&lt;p&gt;Especially if a company grows fast, bottlenecks arise. If your company started with five friends sitting around a desk and bouncing ideas back and forth, everything is fine. Every problem is apparent and can be solved quickly by whoever has the necessary time and skills.
Once you grow larger, this isn’t possible anymore. Suddenly you need to send mails to request resources from other teams or even pull out the &lt;em&gt;global write lock&lt;/em&gt; sledgehammer called “meeting” to coordinate change.
Containers can help by enforcing a clean separation of concern between the teams providing resources by operating systems to run containers and the teams running containers on them. If your develpment team needs to deploy a new application, it’s just another container. They don’t care where it runs. If the infrastructure team needs to provide more resources, they just provide a new system to run containers on.&lt;/p&gt;
&lt;h4&gt;Immutability&lt;/h4&gt;
&lt;p&gt;The great thing about computers is their versatility. It’s a general purpose device which can be programmed to solve specific tasks. You can install various software on your server, remove it again, change configuration files, set OS settings. You can change the &lt;em&gt;state&lt;/em&gt; of your machine whenever you want. Remember, this is how configuration management handles change in your infrastructure. Mutating the current state until you get to your desired state. The complexity arises from the fact that every state mutation leads to a new initial condition for all further operations in your system. Configuration management tries to account for all this, but in reality you often end up in states where your assumptions don’t apply anymore and you need to manually intervene.
Containers are usually, and as implemented by Docker, based on immutable images. Build- and runtime are clearly separated. Once built, your application, all its dependencies and even parts of your configuration are baked into your images. Even though &lt;a href=&quot;https://github.com/docker/docker/issues/7923&quot;&gt;Docker doesn’t enforce it yet&lt;/a&gt;, conceptually by default the container doesn’t change. From deployment to removal, it stays the same and there won’t be a new initial condition which need to be considered for further operations on the system.
Of course, your infrastructure isn’t immutable. You most likely store and process data and with every deploy the state of your infrastructure changes as well. And you probably want some dynamic configuration to avoid a redeploy just because some backend IP changes. But by keeping possibility for change small, having clear definitions of what can and what can’t change, what is state that is supposed to change during runtime as opposed to state frozen at built time, it reduces the cognitive overhead required to understand an infrastructure at a given point in time.&lt;/p&gt;
&lt;h3&gt;Container vs VMs&lt;/h3&gt;
&lt;p&gt;Some might argue that all this is possible with VMs already and conceptually this is entirely true. Although in practice the overhead imposed by virtualization make it less feasible to run every little service in their own VM. And once you start making exceptions, you need to manage those exceptions, so you need to account again for mutable systems and raw applications with all their knobs.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Using docker run --net=container:XX ... to debug network issues]]></title><description><![CDATA[Sharing the network namespace with a existing container is a less known feature of Docker. If you run: Your container runs in the same…]]></description><link>https://5pi.de//2014/11/13/using-docker-run-netcontainerxx-to-debug-network-issues/</link><guid isPermaLink="false">https://5pi.de//2014/11/13/using-docker-run-netcontainerxx-to-debug-network-issues/</guid><pubDate>Thu, 13 Nov 2014 23:04:22 GMT</pubDate><content:encoded>&lt;p&gt;Sharing the network namespace with a existing container is a less known feature of Docker. If you run:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;docker run --net=container:my-existing-container ...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;Your container runs in the same network namespace, sharing the same network configuration as &lt;code&gt;my-existing-container&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is very useful for debugging purposes: I just wanted to verify the current outgoing connections from a container in our infrastructure and realized that due to the network scoping that’s not easily possible. You can use &lt;code&gt;ip netns exec&lt;/code&gt; but that assumes you have the namespace mounted to &lt;code&gt;/var/run/netns&lt;/code&gt;. You can symlink things around but it’s ugly. Much nicer:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;docker run -t -i --net=container:my-existing-container --rm ubuntu netstat -anp&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;</content:encoded></item><item><title><![CDATA[Running a highly available load balancer on Docker]]></title><description><![CDATA[For quite some time I felt like I should do a blog - again. Instead of spending time writting rants on facebook or commenting other peoples…]]></description><link>https://5pi.de//2014/11/10/running-a-highly-available-load-balancer-on-docker/</link><guid isPermaLink="false">https://5pi.de//2014/11/10/running-a-highly-available-load-balancer-on-docker/</guid><pubDate>Mon, 10 Nov 2014 19:18:57 GMT</pubDate><content:encoded>&lt;p&gt;For quite some time I felt like I should do a blog - again. Instead of spending time writting rants on facebook or commenting other peoples posting, I should write some blog articles!
Thing is, it’s not that easy. Where to start? What should I talk about? What are you interested in reading?&lt;/p&gt;
&lt;p&gt;Instead of diving into abstract thoughts about the universe and human kind, I’ll just present you how to run a highly available load balancer on Docker!&lt;/p&gt;
&lt;h3&gt;Components&lt;/h3&gt;
&lt;p&gt;For the load balancing part I’ve choosen haproxy. Since 1.5 it supports SSL termination. Since SSL is expensive, compared to running haproxy without it, we enable multiprocess support by specifying nbproc. Each request now may be handled by a different process. This is a problem for the stats endpoint: Since this endpoint exposes internal state which can’t be shared across multiple processes, you only get stats for the process which handles the current request.
To still get metrics for all processes, you need to create a listen endpoint for each process and pin it to that process like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;listen stats01 :8001
  stats uri /
  stats auth admin:foobar23
  bind-process 1
  stats enable

listen stats02 :8002
  stats uri /
  stats auth admin:foobar23
  bind-process 2
  stats enable

listen stats03 :8003
  stats uri /
  stats auth admin:foobar23
  bind-process 3
  stats enable
...&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;With all that in place we can terminate SSL and load balance across multiple hosts but we still need to make this highly available.
There are several ways to do that. I really like ECMP routing but that requires access to the routing layer which I don’t have. Then there is IPVS which is a good fit if you need to scale to multiple load balancers but it’s also harder and more complex to setup in my opinion.
Because of that, I decided for UCARP which is a implementation of the CARP protocol for linux. CARP is similar to VRRP but patent free and uses cryptography to make it resilient against attackes on the protocol.
When running, it makes sure that there is only one CARP master and executes hooks to add or remove IP adresses.&lt;/p&gt;
&lt;h3&gt;Docker Image&lt;/h3&gt;
&lt;p&gt;I wrapped that all up in a easy to use &lt;a href=&quot;https://registry.hub.docker.com/u/fish/haproxy/&quot;&gt;Docker image&lt;/a&gt;.
To configure haproxy and nginx, you can bind-mount the config location like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;$ docker run \
-v /path/to/haproxy.cfg:/haproxy/haproxy.cfg \
-v /path/to/nginx.d:/haproxy/nginx.d/ \
--net=host --privileged fish/haproxy  \
10.0.1.201 foobar23 [...additional IPs]&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;&lt;em&gt;—net=host is required to access the real hosts interfaces. Privileged is necessary to allow the container to bind IPs. Instead of privileged mode, you probably can use —cap-add=NET_ADMIN&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This will run nginx+haproxy+ucarp and make it listen on 10.0.1.201. You can start the same on another host in the same network and ucarp will make sure only one listens on 10.0.1.201. If you kill the active container, the passive one will failover.&lt;/p&gt;
&lt;p&gt;Since bind-mounting makes things host dependent, I prefer using the fish/haproxy image as a base image and add my deployment specific configuration in a separated Dockerfile like this:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;FROM fish/haproxy
ADD  . /haproxy
RUN  haproxy -c -f /haproxy/haproxy.cfg&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;&lt;em&gt;The last RUN serves as a cheap test; It will prevent the build from suceeding if the configs are malformed&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;This Dockerfile overwrites haproxy.cfg and nginx.d/ in the image by using the files in the local directory.
Just build it with:&lt;/p&gt;
&lt;div class=&quot;gatsby-highlight&quot;&gt;
      &lt;pre class=&quot;language-none&quot;&gt;&lt;code&gt;$ docker build -t my-lb .&lt;/code&gt;&lt;/pre&gt;
      &lt;/div&gt;
&lt;p&gt;And run it as above. This has the downside that you need to rebuild the image on configuration changes and recreate the container to deploy it.&lt;/p&gt;
&lt;h3&gt;Service Discovery&lt;/h3&gt;
&lt;p&gt;If you’re using some configuration management system, you can just render the config on the host and bind-mount it. Still better than running you CM inside a container.
A much better solution is to use some kind of service registry for discovery of your backends. I haven’t found time for that yet, but I would suggest looking into &lt;a href=&quot;http://consul.io&quot;&gt;consul&lt;/a&gt;, &lt;a href=&quot;https://github.com/progrium/registrator&quot;&gt;registrator&lt;/a&gt; and &lt;a href=&quot;https://github.com/kelseyhightower/confd&quot;&gt;confd&lt;/a&gt;.&lt;/p&gt;</content:encoded></item><item><title><![CDATA[Hire me]]></title><description><![CDATA[Did you know? You can hire me! I grew infrastructures from a handful engineers and thousands of users to platforms with hundreds of millions…]]></description><link>https://5pi.de//hire-me/</link><guid isPermaLink="false">https://5pi.de//hire-me/</guid><pubDate>Sat, 01 Jan 2000 00:00:00 GMT</pubDate><content:encoded>&lt;p&gt;&lt;a title=&quot;See page for author [Public domain], via Wikimedia Commons&quot; href=&quot;https://commons.wikimedia.org/wiki/File%3APetin_viewing_airship_platform.jpg&quot;&gt;&lt;img width=&quot;1024&quot; alt=&quot;Petin viewing airship platform&quot; src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/e/eb/Petin_viewing_airship_platform.jpg/1024px-Petin_viewing_airship_platform.jpg&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Did you know? You can hire me!&lt;/h2&gt;
&lt;p&gt;I grew infrastructures from a handful engineers and thousands of users to platforms with hundreds of millions of users and dozens of teams, each with dozens of engineers. This taught me valuable lessons about building, operating and scaling infrastructure.&lt;/p&gt;
&lt;p&gt;While one size does not fit all, doing things right the first time will not only save time refactoring. A good infrastructure is a resilient foundation for your service that supports the agility you need early on without much ramp up costs. A bad infrastructure not only requires manual operation but also slows down every single engineer, preventing them from being confident in changes, which leads to accumulation of technical debt and can end in a grid lock across engineering.&lt;/p&gt;
&lt;p&gt;I’ve seen them all. The good, the bad and the outright ugly. Enterprise and startup. And I’ve built, fixed and deployed technology like &lt;a href=&quot;/tag/docker/&quot;&gt;Docker&lt;/a&gt;, &lt;a href=&quot;/2016/11/20/15-producation-grade-kubernetes-cluster/&quot;&gt;Kubernetes&lt;/a&gt; and &lt;a href=&quot;/2015/01/26/monitor-docker-containers-with-prometheus/&quot;&gt;Prometheus&lt;/a&gt; to address those issues. I know &lt;a href=&quot;/2015/04/22/scope-and-ownership-in-tech-companies/&quot;&gt;what to buy and what to build&lt;/a&gt; and patterns to &lt;a href=&quot;http://5pi.de/2015/08/31/dont-manage-config-unless-you-have-to/&quot;&gt;manage complexity&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now I want to help you with your infrastructure!&lt;/p&gt;
&lt;h2&gt;Inquiry&lt;/h2&gt;
&lt;p&gt;Building something new and want to avoid the common and not-so-common pitfalls?
Your developers can’t iterate quickly, you miss deadlines and need help figuring out how to fix that?
Do you need help with your Kubernetes cluster, Prometheus monitoring or bare metal infrastructure automation?&lt;/p&gt;
&lt;p&gt;You can email me at &lt;a href=&quot;mailto:inquiry+hire@5pi.de?subject=Inquiry%3A&quot;&gt;inquiry+hire@5pi.de&lt;/a&gt; or use the following &lt;a href=&quot;https://formspree.io&quot;&gt;formspree.io&lt;/a&gt; form:&lt;/p&gt;
&lt;form method=&quot;POST&quot; action=&quot;https://formspree.io/inquiry+hire@5pi.de&quot;&gt;
  &lt;input name=&quot;email&quot; placeholder=&quot;Your email&quot; type=&quot;email&quot;&gt;&lt;br /&gt;
  &lt;textarea name=&quot;message&quot; placeholder=&quot;Your message&quot;&gt;&lt;/textarea&gt;&lt;br /&gt;
  &lt;button type=&quot;submit&quot;&gt;Send&lt;/button&gt;
&lt;/form&gt;&lt;br /&gt;
&lt;p&gt;Find my pgp key &lt;a href=&quot;https://keybase.io/fish/pgp_keys.asc&quot;&gt;here&lt;/a&gt; if you wish to encrypt the message.&lt;/p&gt;
&lt;p&gt;I’m located in Berlin but available to travel and used to work remotely (PT 1am-11am, UTC 9am-7pm).&lt;/p&gt;</content:encoded></item></channel></rss>
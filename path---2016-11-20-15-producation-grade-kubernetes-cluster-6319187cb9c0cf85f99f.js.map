{"version":3,"sources":["webpack:///path---2016-11-20-15-producation-grade-kubernetes-cluster-6319187cb9c0cf85f99f.js","webpack:///./.cache/json/2016-11-20-15-producation-grade-kubernetes-cluster.json"],"names":["webpackJsonp","508","module","exports","data","site","siteMetadata","title","author","markdownRemark","id","html","frontmatter","date","pathContext","slug","previous","fields","next"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,mBAAAC,OAAA,2BAA8DC,gBAAmBC,GAAA,oHAAAC,KAAA,wsxBAAwupBC,aAA+yIL,MAAA,oDAAAM,KAAA,+BAAiGC,aAAgBC,KAAA,uDAAAC,UAA0EC,QAAUF,KAAA,6CAAmDH,aAAgBL,MAAA,kCAAyCW,MAASD,QAAUF,KAAA,2EAAiFH,aAAgBL,MAAA","file":"path---2016-11-20-15-producation-grade-kubernetes-cluster-6319187cb9c0cf85f99f.js","sourcesContent":["webpackJsonp([162183589928130],{\n\n/***/ 508:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"5π - fish's blog\",\"author\":\"Johannes 'fish' Ziemke\"}},\"markdownRemark\":{\"id\":\"/usr/src/src/pages/2016/11/20/15-producation-grade-kubernetes-cluster/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><a title=\\\"By Charlotte Marillet (originally posted to Flickr as Quatre macarons) [CC BY-SA 2.0 (http://creativecommons.org/licenses/by-sa/2.0)], via Wikimedia Commons\\\" href=\\\"https://commons.wikimedia.org/wiki/File%3AQuatre_macarons%2C_October_2009.jpg\\\">\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-518ec.jpg\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 85.71428571428571%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAARABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAQCAwX/xAAXAQADAQAAAAAAAAAAAAAAAAAAAgMB/9oADAMBAAIQAxAAAAHPKuGVblqi6hBPVGn/xAAcEAEAAgEFAAAAAAAAAAAAAAABAgMTABEiMjT/2gAIAQEAAQUCQySYysjDcQy2TLLazg96PS6//8QAGREAAQUAAAAAAAAAAAAAAAAAAQAQERIh/9oACAEDAQE/ARA0qz//xAAWEQADAAAAAAAAAAAAAAAAAAABIDH/2gAIAQIBAT8BFT//xAAaEAEBAAIDAAAAAAAAAAAAAAABABAhEWGx/9oACAEBAAY/AnrK2jXsckzj/8QAHhAAAgEDBQAAAAAAAAAAAAAAAAEhETFBEFFhgaH/2gAIAQEAAT8hmGREk0l1UnVYRfVnkq1mHIaoh7Hi0Nx//9oADAMBAAIAAwAAABC4IED/xAAYEQADAQEAAAAAAAAAAAAAAAAAAREQIf/aAAgBAwEBPxB49B26PP/EABcRAAMBAAAAAAAAAAAAAAAAAAABERD/2gAIAQIBAT8QpuQWf//EAB4QAQABBAIDAAAAAAAAAAAAAAERABAhUTGBQaHR/9oACAEBAAE/ENExM8FS0uhFy7dUM2ycUhUciJCPMUA4hIO9+VmmyXTXcWD3Wx//2Q=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Four tiny Macarones - or one month Kubernetes cluster\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-f8fb9.jpg\\\"\\n        srcset=\\\"/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-e8976.jpg 148w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-63df2.jpg 295w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-f8fb9.jpg 590w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-85e3d.jpg 885w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-d1924.jpg 1180w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-9452e.jpg 1770w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-518ec.jpg 2268w\\\"\\n        sizes=\\\"(max-width: 590px) 100vw, 590px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </a>\\n<small>Four sweets or production Kubernetes for a month</small></p>\\n<h2>Introduction</h2>\\n<p>As you might already know, I’m into <a href=\\\"https://5pi.de/2015/01/08/containerized-infrastructure/\\\">containers</a>, <a href=\\\"https://5pi.de/2015/08/31/dont-manage-config-unless-you-have-to/\\\">static configuration</a> and <a href=\\\"https://5pi.de/2015/04/22/scope-and-ownership-in-tech-companies/\\\">self-service infrastructures</a>. Naturally, I love <a href=\\\"http://kubernetes.io/\\\">Kubernetes</a>, which I consider the most promising cluster scheduler around.</p>\\n<p>In fact, the biggest reason to use containers is that they make it possible for something like Kubernetes to <em>operate your cluster</em>. Cluster scheduler like Kubernetes, Mesos or Swarm take care of deploying and moving your applications around without requiring an Operator to allocate resources and redeploy services manually.</p>\\n<p>Cluster schedulers are here to stay. They will become as ubiquitous as version control and getting experience with it is something I can encourage everyone in the DevOps world to do. Especially if your job is mainly <em>operating</em>. Chances are, your job gets automated.</p>\\n<p>Getting Kubernetes up <em>somehow</em> is easy. There are tons of scripts for doing that. But those setups are intended as temporary test environments. Setting up a production environment is much harder and unfortunately not very well documented.</p>\\n<p>Reading this you will realize that there are things you might not want to do this way in production and I agree. So I’m sorry if the title is a bit click-baity. The reason I’m still calling this ‘production’ is because this setup is highly available, TLS authenticated and has a way going forward which doesn’t require you to start from scratch like most Containers/Kubernetes getting started guides.</p>\\n<p>You can find all code mentioned here: <a href=\\\"https://github.com/5pi\\\">https://github.com/5pi</a></p>\\n<h3>Overview</h3>\\n<p><img src=\\\"/5pi-Infra-020fc8ece35a34a40c4d951f78df9481.svg\\\" alt=\\\"Infrastructure Diagram\\\">\\nMainly to keep things cheap, I choose <a href=\\\"https://digitalocean.com\\\">DigitalOcean</a> and use the smallest $5/month instances. For a highly available cluster, we need at least three hosts.</p>\\n<p>Kubernetes doesn’t schedule containers directly, it schedules <a href=\\\"http://kubernetes.io/docs/user-guide/pods/\\\">Pods</a> which again can consist of multiple containers sharing the same storage volumes and IPs. The pods running on different hosts need to communicate with each other. There are several options to do this, like using overlay networks, routing IP ranges to each server on you routers or co-locating the servers on the same ethernet segment.</p>\\n<p>Since static routes are least complex to setup and maintain, while allowing to grow easier than with one ethernet segment for all pod IPs, it’s the option I choose here.</p>\\n<p>Because on DigitalOcean there is no way to have custom routes, I’m using <a href=\\\"https://www.tinc-vpn.org/\\\">tinc</a> to form a private, flat ethernet segment and route a /24 to each host.</p>\\n<p>To consider the infrastructure immutable, we need to store state externally. Fortunately DigitalOcean just released their <a href=\\\"https://www.digitalocean.com/products/storage/\\\">block storage</a> product which we use to store pod volumes on.</p>\\n<p>We will also create DNS names to make accessing the cluster easier:</p>\\n<ul>\\n<li>master0X.[domain] points to a hosts internal IP; Used for tinc to connect to peers</li>\\n<li>edge0X.[domain] points to a hosts public IP; Used for remote access etc</li>\\n<li>edge.[domain] points to edge float IP; Used to reach LB directly</li>\\n<li>*.edge.[domain] also points to edge float IP; Used for virtual hosts on LB</li>\\n</ul>\\n<h2>Kubernetes Deployment</h2>\\n<h3>Configuration</h3>\\n<script type=\\\"text/javascript\\\" src=\\\"https://asciinema.org/a/dgfb2mik71by4tkt5urd9lasj.js\\\" id=\\\"asciicast-dgfb2mik71by4tkt5urd9lasj\\\" async></script>\\n<p>The <a href=\\\"https://github.com/5pi/infra\\\">infra repository</a> contains all sources needed to build the images and deploy the stack. Beside the committed configuration in the repository, there is also some cluster specific configuration required. While some of it only affects the cluster deployment, other is included in the images. This means changing this always requires rebuilding the images.\\nThe configuration is kept in <a href=\\\"https://github.com/5pi/infra/tree/master/config\\\">config/</a>.</p>\\n<p>To create a new cluster, first checkout the repo, edit <code>config/env</code> and run <code>./mk_credentials</code> to create credentials:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>$ git clone git@github.com:5pi/infra.git\\n$ cd infra\\n$ vi config/env\\n$ ./mk_credentials</code></pre>\\n      </div>\\n<h3>Building Images</h3>\\n<script type=\\\"text/javascript\\\" src=\\\"https://asciinema.org/a/786ffzwvjdupff56ka5258ckk.js\\\" id=\\\"asciicast-786ffzwvjdupff56ka5258ckk\\\" async></script>\\n<p><tty-player autoplay controls loop src=packer-k8s-build.rec></tty-player>\\nI’m using <a href=\\\"https://www.packer.io\\\">packer</a> to build machine images. Since all our hosts are controller+worker nodes, we only have one image. This image includes tinc, etcd, kubernetes and the <a href=\\\"https://prometheus.io\\\">Prometheus</a> node-exporter for monitoring. Beside installing general configuration files and services, the <a href=\\\"https://github.com/5pi/infra/blob/e8ccca5de9a5d7759c40355c860d3ed13e349fc7/packer/base.json#L22\\\">packer config</a> also refers the cluster specific configuration. This means for each cluster you need to build custom images. It does <strong>not</strong> include host specific configuration like a host’s TLS keys, or configuration that simply isn’t available before deployment, like a host’s IP address. This need to be configured at deployment.</p>\\n<p>As described in the <a href=\\\"https://github.com/5pi/infra#deploying-a-new-stack\\\">README</a>, to build the images run <code>make -C packer</code> after you created the configuration and credentials.</p>\\n<h3>Deploying Stack</h3>\\n<script type=\\\"text/javascript\\\" src=\\\"https://asciinema.org/a/1hpbxy5pcvmidxxuu1u6w39vr.js\\\" id=\\\"asciicast-1hpbxy5pcvmidxxuu1u6w39vr\\\" async></script>\\n<p>Deployment is configured in <a href=\\\"https://github.com/5pi/infra/tf\\\">tf/</a> and uses <a href=\\\"https://www.terraform.io\\\">terraform</a>. It spins up hosts with the specified image and allows the provided ssh key to connect. It also creates the DNS records required for tinc to connect to peers and external users to access services on the cluster.</p>\\n<p>Since we didn’t want to include all necessary configuration and credentials in the image for flexibility and security, we need to upload the remaining configuration, like TLS keys, after spinning up the host. To set general configuration, <code>/etc/environment.tf</code> is created and can be sourced by scripts in the image to get deploy-time configuration.</p>\\n<p>To deploy a new cluster, you first probably want to change <code>tf/id_rsa.pub</code> to include your SSH public key. You need to run ssh-agent and the key needs to be added to it’s keyring with <code>ssh-add</code>. The key may not exist on DigitalOcean already. You can run <code>ssh-keygen -f id_rsa &#x26;&#x26; ssh-add id_rsa</code> in <code>tf/</code> to create a new keypair.</p>\\n<p>You also need to create the domain you specified in <code>config/env</code> in DigitalOcean before proceeding. For whatever reason DO requires you to attach the records to some droplet. It doesn’t really matter which one.</p>\\n<p>To spin up the configured cluster with the built image, run:</p>\\n<p><code>./terraform apply -var cluster_state=new -var 'image=\\\"image-id-from-last-step\\\"'</code></p>\\n<p><small>Be careful to quote the image parameter properly like described here). <code>cluster_state</code> is required to make etcd not wait for consensus before spinning up the next instance.</small></p>\\n<p>That’s it! After a few minutes, the cluster should be up and running.</p>\\n<p>If you created a new domain for your cluster, it may take some time until the cluster is formed. If you run <code>journalctl -fu tinc@default</code> to watch the tinc log you should see that the DNS records are not resolvable yet. This should fix itself after a few minutes.\\nOnce tinc is running, etcd should reach it peers. Run <code>journalctl -fu etcd</code> to see the etcd log.</p>\\n<h3>Rolling Upgrades</h3>\\n<p>Another thing most getting started guides are missing is upgrades. Most likely because they are often very environment specific and, well, hard. Which is also why while upgrades <em>should</em> work, this is the most brittle part of it and one of the reason I hesitated to call it “Production Grade”. But heck, if Docker 1.0 was <em>production ready</em> this here is as well.</p>\\n<p>Since all pod volumes get stored on DO block storage, we consider the systems immutable. To upgrade the cluster, all instances get replaced while Kubernetes makes sure to reschedule services and maintain their availability. The tricky part is to orchestrate this with terraform which does <a href=\\\"https://github.com/hashicorp/terraform/issues/2896\\\">not really support this</a>.</p>\\n<p>Because of that we need a <a href=\\\"https://github.com/5pi/infra/blob/master/tf/upgrade\\\">wrapper script</a>. This script stops etcd on the first old server, which will cause it to remove itself from the cluster. Then it executes terraform and sets <code>-target</code> for each server individually. Unless <code>cluster_state=new</code> is given, the new instance’s provisioning script will block until etcd joined the existing cluster and the cluster is healthy. Only after that, the script continues with the next instance.</p>\\n<p>Removing an instance from the cluster before replacing it is required. Otherwise the replacement instance can’t join the cluster. This means, if an instance ever dies, you need to manually remove it from the cluster with <code>etcd member remove</code>.</p>\\n<h2>Deployment on Kubernetes</h2>\\n<p>A cluster without services doesn’t make much sense, so I’ll also show quickly how to deploy services to the cluster and make them accessible. I’m using just a bunch of yaml files I can apply with <code>kubectl apply -f</code>: <a href=\\\"https://github.com/5pi/services\\\">https://github.com/5pi/services</a>\\nThey are pretty specific to my setup, so probably only useful as example. For real reusable components on top of Kubernetes have a look at <a href=\\\"https://github.com/5pi/services\\\">helm</a>.</p>\\n<p>Beside the public configuration, you need to require to setup some secrets:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>apiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: default\\ntype: Opaque\\ndata:\\n  pg-password:...\\n  pg-ghost-fish-password: ...\\n  pg-grafana-password: ...\\n  do-token: ...\\n  grafana-gauth-client-secret: ...\\n  smtp-infra-password: ...</code></pre>\\n      </div>\\n<ul>\\n<li>For the passwords, just generate something, base64 encode and put them into a file and apply it</li>\\n<li>The <code>do-token</code> is a DigitalOcean API token required for floating IP and volume configuration</li>\\n<li><code>grafana-gauth-client-secret</code> is a Google OAuth client secret for Google Auth based Grafana authentication</li>\\n<li><code>smtp-infra-password</code> is the password for a Google Mail I use to send Prometheus alerts</li>\\n</ul>\\n<h3>SkyDNS</h3>\\n<p>This is a core service providing DNS resolution for pods. I’m using an <a href=\\\"https://github.com/5pi/services/blob/master/01_skydns-rc.yml\\\">adapted copy</a> of the <a href=\\\"https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/skydns-rc.yaml.in\\\">upstream config</a>.</p>\\n<h3>Traffic Tier</h3>\\n<p>I’m using <a href=\\\"https://github.com/containous/traefik\\\">traefik</a> as reverse proxy / load balancer. It depends on a <code>traefik-config</code> ConfigMap where I specific TLS keys etc. It runs three replica and assumes a three node cluster, so we run one instance per host. The vhost configuration is part of the application yaml files.</p>\\n<p>To have a stable entrypoint into our infrastructure while the host’s IPs change on every rolling upgrade, terraform deployed a floating IP. To assign this IP to any available traefik instance, I’ve created a simple <a href=\\\"https://github.com/5pi/img-do-float-ip\\\">container image</a> and this <a href=\\\"https://github.com/5pi/services/blob/master/01_float_ip.yml\\\">yaml spec</a> (The IP needs to get changed when using this spec).\\nThis will make sure the float IP is always assigned to one of the running hosts.</p>\\n<p>This requires the <code>do-token</code>.</p>\\n<h3>PostgreSQL</h3>\\n<p>PostgreSQL is used as main database powering this blog for instance. The <a href=\\\"https://github.com/5pi/services/blob/master/20_postgres.yml\\\">configuration</a> is straight forward but uses my custom DigitalOcean Storage flexvolume plugin. Before using it the first time, the volume needs to get created, attached, formatted and detached again. You can run the <a href=\\\"https://github.com/5pi/infra/blob/master/packer/files/usr/libexec/kubernetes/kubelet-plugins/volume/exec/5pi.de~do-volume/do-volume\\\">flexvol plugin</a> to do this:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>do-volume create pgdata 10G my postgres volume\\ndo-volume attach '{ \\\"volume\\\": \\\"pgdata\\\" }'\\nmkfs.ext3 /dev/disk/by-id/scsi-0DO_Volume_pgdata</code></pre>\\n      </div>\\n<p>After that, the yaml file can get applied and postgres should spin up.</p>\\n<h3>Ghost</h3>\\n<p>Next is Ghost, the blogging platform powering this and <a href=\\\"https://textkrieg.de\\\">textkrieg.de</a>. It uses Postgres as database and another DO volume for assets.\\nSince the official Ghost image can’t be easily configured automatically and is huge, I’m using my own (<a href=\\\"https://alpinelinux.org/\\\">alpine</a> based) <a href=\\\"https://hub.docker.com/r/fish/ghost/\\\">Docker image</a>.\\nAlthough designed as a <em>modern</em> blogging platform, it doesn’t fit particular well into the new container/12factor world. For example, it assumes that the current working directory is writable. Since we want to run ghost as unprivileged user, we need to make the volume writable by that user. Unfortunately there is no good way to do that. People often create images that run chown as root when starting, then dropping privileges. But this can become time consuming and has possible security implications. The upcoming <a href=\\\"https://github.com/kubernetes/kubernetes/pull/26926\\\">flexvolume redesign</a> will fix this issue. For now we need to make the volume world-writable initially:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>do-volume create ghost-fish 1\\ndo-volume attach '{ \\\"volume\\\": \\\"ghost-fish\\\" }'\\nmkfs.ext4 /dev/disk/by-id/scsi-0DO_Volume_ghost-fish\\n\\nmount /dev/disk/by-id/scsi-0DO_Volume_ghost-fish /mnt\\nmkdir -m 777 /mnt/{apps,data,images} # See 5pi/infra#11\\numount /mnt\\n\\ndo-volume detach /dev/disk/by-id/scsi-0DO_Volume_ghost-fish</code></pre>\\n      </div>\\n<p>Here I choose simplicity over security for the time being since I’m the only operator of this cluster anyway. You might want to make a different trade off.</p>\\n<p>Now we still need to create databases and users, another thing that hasn’t been addressed by Kubernetes yet. For that we need to find the Postgres pod, then <code>exec</code> into it to create database and users:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>kubectl exec -ti postgres-2416409090-jf8uh -- psql -U postgres\\ncreate user ghost_fish with password 'pwd-from-secrets.yml';\\ncreate database ghost_fish;\\ngrant all privileges on database ghost_fish to ghost_fish;</code></pre>\\n      </div>\\n<p>Now we can use the yaml file to deploy Ghost. Beside deploying the pods, we also create a <a href=\\\"https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/50_ghost_fish.yml#L13\\\">Ingress to configure Traefik</a>. This routes requests for <code>5pi.de</code> to service <code>ghost-fish</code> on port 80 which is defined above and maps to the Ghost deployment</p>\\n<p>To access the blog, you still need to create an CNAME DNS record matching the Ingress route and pointing to the managed <code>edge.[DOMAIN]</code> DNS name.</p>\\n<h2>Monitoring</h2>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-7f419.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 61.0479797979798%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsSAAALEgHS3X78AAACAklEQVQozzVR2Y4UMQzsz9jp3HGSdtJJ3+fM7swuAgQC3nlD4v8/ArMIycpRjl2pcrUiP7bl/nhdt/N6vS7L0ufQlTwMQ0oJEUMIdMg5l1KmeR77vA6xy9i2bWWg5kzkFl+PmDBIzjljFErIVGJb0Cj1HxEIIXWYUtRKs7quBONWi9+fza83NydNNYJzamGccggpAlr1D1FMhC/gB5+DBcIYq56eWPbiy2aeZIPU8J1Z1lxHialB5wP141y8k8MnU3JOzoEgoK7AUY7acC1YcI4k0lNil7Sx2mqjpeY1J/o2YgoNr6mOPkLFrHJgiQ+UjKCCVcmZPRollRSK1yw2OHem4OW2XN6Oeu35Utja1cdYN+5SeXCth6+7/n7qnw/74zTfrvrs5ZJ0MMy7sI3N82SPzj2Pbu+bvXdDtHPxoEUlhVhTvPd47/DR4+sQz4y3El+6GLSiL2zTcIzdVtJayj4PS1eOodyWEbSqXJBUX9cXciGmGCN6D6SWFNNKZkmSyC4EtzFQYOMIlySpvlRkUjDOCnLfgfL2PbRqjABJs/mbBSsMaGc12UNvnFLBSKu4qLxzvcctpCXEDZsz4tykCeMeMSgFAAVS59o+5M5j52N2uSvDXHKjdGW1Xkt6mciV8JibjyveJ6Trhy02lkYmxgz7AOcE1xFuk9sHey7hZQGv+R9YFW0nnzb+igAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Grafana\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fb8a0.png\\\"\\n        srcset=\\\"/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-1a291.png 148w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-2bc4a.png 295w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fb8a0.png 590w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-526de.png 885w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fa2eb.png 1180w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-7f419.png 1584w\\\"\\n        sizes=\\\"(max-width: 590px) 100vw, 590px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>Now that all this is running, it also needs to be monitored properly. Of course I’m using <a href=\\\"https://prometheus.io\\\">Prometheus</a> for this. The setup is straight forward now that Ingress and Volume configuration is nothing new. It tunes Prometheus to run better on small instances and create a DO Volume to store metrics on. To gather metrics about the services running on Kubernetes, the <a href=\\\"https://github.com/kubernetes/kube-state-metrics\\\">kube-state-metrics</a> exporter is deployed. To monitor website response times, I also deploy the <a href=\\\"https://github.com/prometheus/blackbox_exporter\\\">blackbox-exporter</a>.\\nTo send out alerts, two instances of the HA Alertmanager are deployed.\\n<a href=\\\"https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/prometheus/prometheus-config.yml\\\">This ConfigMap</a> configures Prometheus, sets up alerts and configures the Alertmanager. The SMTP credentials get passed in at container startup.\\nThe alerts are quite trigger happy, which is fine in such small cluster. In a bigger cluster you need to adjust those to limit noise. In the end you’re not interested even if whole nodes crash as long as Kubernetes can reschedule the pods. In my tiny cluster I would be surprised if a node crashes, so I alert on this for now.</p>\\n<p>I’m also using Grafana to show metrics on dashboards. Grafana is configured to use Google OAuth authentication against my Google Apps account. This allows Grafana to be available on the public internet and allow everyone in my Google Apps Org to access it.</p>\\n<p>Here is where the wildcard DNS domain comes in handy. Instead of having to add new DNS record for each new service like Grafana, we just access it via the wildcard domain: A request to <code>anything.edge.[DOMAIN]</code> ends up at the floating IP and is accepted by a traefik instance. So the only thing we need to configure is the host in the <a href=\\\"https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/grafana.yml#L19\\\">Ingress definition</a>.</p>\\n<h1>Retrospective and Future</h1>\\n<p>I’ve started all this a few month ago already with the goal of building a small cluster from scratch. I didn’t want to use tons of bash scripts not running on a specific cloud. Back then there was <a href=\\\"https://github.com/kubernetes/kops\\\">kops</a>, which I would recommend to look at first if you’re deploying to AWS.</p>\\n<p>Now there is also kubeadm which looks like a promising way to setup a cluster and I might change my deployment to use it instead. I’ll also consider using CoreOS and cloud-config to configure the Kubernetes components, as well as looking deeper into systemd units to coordinate stopping of services and draining of hosts.</p>\\n<p>If you’re looking for reusable components on top of Kubernetes, there is <a href=\\\"https://github.com/kubernetes/helm\\\">Helm</a> which might be a better option than just keeping the services yaml files around. You might be also interested in CoreOS’s <a href=\\\"https://coreos.com/blog/introducing-operators.html\\\">Operator</a> to fully automate Prometheus operations.</p>\\n<p>The most important next thing on my TODO list is proper tests though. I want a full integration test which spins up a new cluster, deploys services to it, runs blackbox tests, runs an rolling upgrade and tests that during and after that the cluster services are available.</p>\\n<p>Pull requests to make this more generic and fix issues are welcome, but I won’t accept larger changes until I got the tests going.</p>\\n<hr>\\n<h4>Update</h4>\\n<p>Feel free to discuss and comment on <a href=\\\"https://news.ycombinator.com/item?id=13006296\\\">Hacker News</a>.</p>\",\"frontmatter\":{\"title\":\"$15 Production Kubernetes Cluster on DigitalOcean\",\"date\":\"2016-11-20 10:36:00 +0000\"}}},\"pathContext\":{\"slug\":\"/2016/11/20/15-producation-grade-kubernetes-cluster/\",\"previous\":{\"fields\":{\"slug\":\"/2015/10/05/the-future-fabrication-cloud/\"},\"frontmatter\":{\"title\":\"The Future: Fabrication Cloud\"}},\"next\":{\"fields\":{\"slug\":\"/2017/04/17/selenium-webdriver-chromedriver-nightwatch-chrome-headless/\"},\"frontmatter\":{\"title\":\"Frontend Testing Zoo - Or, Nightwatch without Selenium\"}}}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---2016-11-20-15-producation-grade-kubernetes-cluster-6319187cb9c0cf85f99f.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"5π - fish's blog\",\"author\":\"Johannes 'fish' Ziemke\"}},\"markdownRemark\":{\"id\":\"/usr/src/src/pages/2016/11/20/15-producation-grade-kubernetes-cluster/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<p><a title=\\\"By Charlotte Marillet (originally posted to Flickr as Quatre macarons) [CC BY-SA 2.0 (http://creativecommons.org/licenses/by-sa/2.0)], via Wikimedia Commons\\\" href=\\\"https://commons.wikimedia.org/wiki/File%3AQuatre_macarons%2C_October_2009.jpg\\\">\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-518ec.jpg\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 85.71428571428571%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAARABQDASIAAhEBAxEB/8QAGAABAQEBAQAAAAAAAAAAAAAAAAQCAwX/xAAXAQADAQAAAAAAAAAAAAAAAAAAAgMB/9oADAMBAAIQAxAAAAHPKuGVblqi6hBPVGn/xAAcEAEAAgEFAAAAAAAAAAAAAAABAgMTABEiMjT/2gAIAQEAAQUCQySYysjDcQy2TLLazg96PS6//8QAGREAAQUAAAAAAAAAAAAAAAAAAQAQERIh/9oACAEDAQE/ARA0qz//xAAWEQADAAAAAAAAAAAAAAAAAAABIDH/2gAIAQIBAT8BFT//xAAaEAEBAAIDAAAAAAAAAAAAAAABABAhEWGx/9oACAEBAAY/AnrK2jXsckzj/8QAHhAAAgEDBQAAAAAAAAAAAAAAAAEhETFBEFFhgaH/2gAIAQEAAT8hmGREk0l1UnVYRfVnkq1mHIaoh7Hi0Nx//9oADAMBAAIAAwAAABC4IED/xAAYEQADAQEAAAAAAAAAAAAAAAAAAREQIf/aAAgBAwEBPxB49B26PP/EABcRAAMBAAAAAAAAAAAAAAAAAAABERD/2gAIAQIBAT8QpuQWf//EAB4QAQABBAIDAAAAAAAAAAAAAAERABAhUTGBQaHR/9oACAEBAAE/ENExM8FS0uhFy7dUM2ycUhUciJCPMUA4hIO9+VmmyXTXcWD3Wx//2Q=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Four tiny Macarones - or one month Kubernetes cluster\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-f8fb9.jpg\\\"\\n        srcset=\\\"/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-e8976.jpg 148w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-63df2.jpg 295w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-f8fb9.jpg 590w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-85e3d.jpg 885w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-d1924.jpg 1180w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-9452e.jpg 1770w,\\n/static/Quatre_macarons-_October_2009-db5a9488a10ec170588bdbaab13f615a-518ec.jpg 2268w\\\"\\n        sizes=\\\"(max-width: 590px) 100vw, 590px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </a>\\n<small>Four sweets or production Kubernetes for a month</small></p>\\n<h2>Introduction</h2>\\n<p>As you might already know, I’m into <a href=\\\"https://5pi.de/2015/01/08/containerized-infrastructure/\\\">containers</a>, <a href=\\\"https://5pi.de/2015/08/31/dont-manage-config-unless-you-have-to/\\\">static configuration</a> and <a href=\\\"https://5pi.de/2015/04/22/scope-and-ownership-in-tech-companies/\\\">self-service infrastructures</a>. Naturally, I love <a href=\\\"http://kubernetes.io/\\\">Kubernetes</a>, which I consider the most promising cluster scheduler around.</p>\\n<p>In fact, the biggest reason to use containers is that they make it possible for something like Kubernetes to <em>operate your cluster</em>. Cluster scheduler like Kubernetes, Mesos or Swarm take care of deploying and moving your applications around without requiring an Operator to allocate resources and redeploy services manually.</p>\\n<p>Cluster schedulers are here to stay. They will become as ubiquitous as version control and getting experience with it is something I can encourage everyone in the DevOps world to do. Especially if your job is mainly <em>operating</em>. Chances are, your job gets automated.</p>\\n<p>Getting Kubernetes up <em>somehow</em> is easy. There are tons of scripts for doing that. But those setups are intended as temporary test environments. Setting up a production environment is much harder and unfortunately not very well documented.</p>\\n<p>Reading this you will realize that there are things you might not want to do this way in production and I agree. So I’m sorry if the title is a bit click-baity. The reason I’m still calling this ‘production’ is because this setup is highly available, TLS authenticated and has a way going forward which doesn’t require you to start from scratch like most Containers/Kubernetes getting started guides.</p>\\n<p>You can find all code mentioned here: <a href=\\\"https://github.com/5pi\\\">https://github.com/5pi</a></p>\\n<h3>Overview</h3>\\n<p><img src=\\\"/5pi-Infra-020fc8ece35a34a40c4d951f78df9481.svg\\\" alt=\\\"Infrastructure Diagram\\\">\\nMainly to keep things cheap, I choose <a href=\\\"https://digitalocean.com\\\">DigitalOcean</a> and use the smallest $5/month instances. For a highly available cluster, we need at least three hosts.</p>\\n<p>Kubernetes doesn’t schedule containers directly, it schedules <a href=\\\"http://kubernetes.io/docs/user-guide/pods/\\\">Pods</a> which again can consist of multiple containers sharing the same storage volumes and IPs. The pods running on different hosts need to communicate with each other. There are several options to do this, like using overlay networks, routing IP ranges to each server on you routers or co-locating the servers on the same ethernet segment.</p>\\n<p>Since static routes are least complex to setup and maintain, while allowing to grow easier than with one ethernet segment for all pod IPs, it’s the option I choose here.</p>\\n<p>Because on DigitalOcean there is no way to have custom routes, I’m using <a href=\\\"https://www.tinc-vpn.org/\\\">tinc</a> to form a private, flat ethernet segment and route a /24 to each host.</p>\\n<p>To consider the infrastructure immutable, we need to store state externally. Fortunately DigitalOcean just released their <a href=\\\"https://www.digitalocean.com/products/storage/\\\">block storage</a> product which we use to store pod volumes on.</p>\\n<p>We will also create DNS names to make accessing the cluster easier:</p>\\n<ul>\\n<li>master0X.[domain] points to a hosts internal IP; Used for tinc to connect to peers</li>\\n<li>edge0X.[domain] points to a hosts public IP; Used for remote access etc</li>\\n<li>edge.[domain] points to edge float IP; Used to reach LB directly</li>\\n<li>*.edge.[domain] also points to edge float IP; Used for virtual hosts on LB</li>\\n</ul>\\n<h2>Kubernetes Deployment</h2>\\n<h3>Configuration</h3>\\n<script type=\\\"text/javascript\\\" src=\\\"https://asciinema.org/a/dgfb2mik71by4tkt5urd9lasj.js\\\" id=\\\"asciicast-dgfb2mik71by4tkt5urd9lasj\\\" async></script>\\n<p>The <a href=\\\"https://github.com/5pi/infra\\\">infra repository</a> contains all sources needed to build the images and deploy the stack. Beside the committed configuration in the repository, there is also some cluster specific configuration required. While some of it only affects the cluster deployment, other is included in the images. This means changing this always requires rebuilding the images.\\nThe configuration is kept in <a href=\\\"https://github.com/5pi/infra/tree/master/config\\\">config/</a>.</p>\\n<p>To create a new cluster, first checkout the repo, edit <code>config/env</code> and run <code>./mk_credentials</code> to create credentials:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>$ git clone git@github.com:5pi/infra.git\\n$ cd infra\\n$ vi config/env\\n$ ./mk_credentials</code></pre>\\n      </div>\\n<h3>Building Images</h3>\\n<script type=\\\"text/javascript\\\" src=\\\"https://asciinema.org/a/786ffzwvjdupff56ka5258ckk.js\\\" id=\\\"asciicast-786ffzwvjdupff56ka5258ckk\\\" async></script>\\n<p><tty-player autoplay controls loop src=packer-k8s-build.rec></tty-player>\\nI’m using <a href=\\\"https://www.packer.io\\\">packer</a> to build machine images. Since all our hosts are controller+worker nodes, we only have one image. This image includes tinc, etcd, kubernetes and the <a href=\\\"https://prometheus.io\\\">Prometheus</a> node-exporter for monitoring. Beside installing general configuration files and services, the <a href=\\\"https://github.com/5pi/infra/blob/e8ccca5de9a5d7759c40355c860d3ed13e349fc7/packer/base.json#L22\\\">packer config</a> also refers the cluster specific configuration. This means for each cluster you need to build custom images. It does <strong>not</strong> include host specific configuration like a host’s TLS keys, or configuration that simply isn’t available before deployment, like a host’s IP address. This need to be configured at deployment.</p>\\n<p>As described in the <a href=\\\"https://github.com/5pi/infra#deploying-a-new-stack\\\">README</a>, to build the images run <code>make -C packer</code> after you created the configuration and credentials.</p>\\n<h3>Deploying Stack</h3>\\n<script type=\\\"text/javascript\\\" src=\\\"https://asciinema.org/a/1hpbxy5pcvmidxxuu1u6w39vr.js\\\" id=\\\"asciicast-1hpbxy5pcvmidxxuu1u6w39vr\\\" async></script>\\n<p>Deployment is configured in <a href=\\\"https://github.com/5pi/infra/tf\\\">tf/</a> and uses <a href=\\\"https://www.terraform.io\\\">terraform</a>. It spins up hosts with the specified image and allows the provided ssh key to connect. It also creates the DNS records required for tinc to connect to peers and external users to access services on the cluster.</p>\\n<p>Since we didn’t want to include all necessary configuration and credentials in the image for flexibility and security, we need to upload the remaining configuration, like TLS keys, after spinning up the host. To set general configuration, <code>/etc/environment.tf</code> is created and can be sourced by scripts in the image to get deploy-time configuration.</p>\\n<p>To deploy a new cluster, you first probably want to change <code>tf/id_rsa.pub</code> to include your SSH public key. You need to run ssh-agent and the key needs to be added to it’s keyring with <code>ssh-add</code>. The key may not exist on DigitalOcean already. You can run <code>ssh-keygen -f id_rsa &#x26;&#x26; ssh-add id_rsa</code> in <code>tf/</code> to create a new keypair.</p>\\n<p>You also need to create the domain you specified in <code>config/env</code> in DigitalOcean before proceeding. For whatever reason DO requires you to attach the records to some droplet. It doesn’t really matter which one.</p>\\n<p>To spin up the configured cluster with the built image, run:</p>\\n<p><code>./terraform apply -var cluster_state=new -var 'image=\\\"image-id-from-last-step\\\"'</code></p>\\n<p><small>Be careful to quote the image parameter properly like described here). <code>cluster_state</code> is required to make etcd not wait for consensus before spinning up the next instance.</small></p>\\n<p>That’s it! After a few minutes, the cluster should be up and running.</p>\\n<p>If you created a new domain for your cluster, it may take some time until the cluster is formed. If you run <code>journalctl -fu tinc@default</code> to watch the tinc log you should see that the DNS records are not resolvable yet. This should fix itself after a few minutes.\\nOnce tinc is running, etcd should reach it peers. Run <code>journalctl -fu etcd</code> to see the etcd log.</p>\\n<h3>Rolling Upgrades</h3>\\n<p>Another thing most getting started guides are missing is upgrades. Most likely because they are often very environment specific and, well, hard. Which is also why while upgrades <em>should</em> work, this is the most brittle part of it and one of the reason I hesitated to call it “Production Grade”. But heck, if Docker 1.0 was <em>production ready</em> this here is as well.</p>\\n<p>Since all pod volumes get stored on DO block storage, we consider the systems immutable. To upgrade the cluster, all instances get replaced while Kubernetes makes sure to reschedule services and maintain their availability. The tricky part is to orchestrate this with terraform which does <a href=\\\"https://github.com/hashicorp/terraform/issues/2896\\\">not really support this</a>.</p>\\n<p>Because of that we need a <a href=\\\"https://github.com/5pi/infra/blob/master/tf/upgrade\\\">wrapper script</a>. This script stops etcd on the first old server, which will cause it to remove itself from the cluster. Then it executes terraform and sets <code>-target</code> for each server individually. Unless <code>cluster_state=new</code> is given, the new instance’s provisioning script will block until etcd joined the existing cluster and the cluster is healthy. Only after that, the script continues with the next instance.</p>\\n<p>Removing an instance from the cluster before replacing it is required. Otherwise the replacement instance can’t join the cluster. This means, if an instance ever dies, you need to manually remove it from the cluster with <code>etcd member remove</code>.</p>\\n<h2>Deployment on Kubernetes</h2>\\n<p>A cluster without services doesn’t make much sense, so I’ll also show quickly how to deploy services to the cluster and make them accessible. I’m using just a bunch of yaml files I can apply with <code>kubectl apply -f</code>: <a href=\\\"https://github.com/5pi/services\\\">https://github.com/5pi/services</a>\\nThey are pretty specific to my setup, so probably only useful as example. For real reusable components on top of Kubernetes have a look at <a href=\\\"https://github.com/5pi/services\\\">helm</a>.</p>\\n<p>Beside the public configuration, you need to require to setup some secrets:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>apiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: default\\ntype: Opaque\\ndata:\\n  pg-password:...\\n  pg-ghost-fish-password: ...\\n  pg-grafana-password: ...\\n  do-token: ...\\n  grafana-gauth-client-secret: ...\\n  smtp-infra-password: ...</code></pre>\\n      </div>\\n<ul>\\n<li>For the passwords, just generate something, base64 encode and put them into a file and apply it</li>\\n<li>The <code>do-token</code> is a DigitalOcean API token required for floating IP and volume configuration</li>\\n<li><code>grafana-gauth-client-secret</code> is a Google OAuth client secret for Google Auth based Grafana authentication</li>\\n<li><code>smtp-infra-password</code> is the password for a Google Mail I use to send Prometheus alerts</li>\\n</ul>\\n<h3>SkyDNS</h3>\\n<p>This is a core service providing DNS resolution for pods. I’m using an <a href=\\\"https://github.com/5pi/services/blob/master/01_skydns-rc.yml\\\">adapted copy</a> of the <a href=\\\"https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/skydns-rc.yaml.in\\\">upstream config</a>.</p>\\n<h3>Traffic Tier</h3>\\n<p>I’m using <a href=\\\"https://github.com/containous/traefik\\\">traefik</a> as reverse proxy / load balancer. It depends on a <code>traefik-config</code> ConfigMap where I specific TLS keys etc. It runs three replica and assumes a three node cluster, so we run one instance per host. The vhost configuration is part of the application yaml files.</p>\\n<p>To have a stable entrypoint into our infrastructure while the host’s IPs change on every rolling upgrade, terraform deployed a floating IP. To assign this IP to any available traefik instance, I’ve created a simple <a href=\\\"https://github.com/5pi/img-do-float-ip\\\">container image</a> and this <a href=\\\"https://github.com/5pi/services/blob/master/01_float_ip.yml\\\">yaml spec</a> (The IP needs to get changed when using this spec).\\nThis will make sure the float IP is always assigned to one of the running hosts.</p>\\n<p>This requires the <code>do-token</code>.</p>\\n<h3>PostgreSQL</h3>\\n<p>PostgreSQL is used as main database powering this blog for instance. The <a href=\\\"https://github.com/5pi/services/blob/master/20_postgres.yml\\\">configuration</a> is straight forward but uses my custom DigitalOcean Storage flexvolume plugin. Before using it the first time, the volume needs to get created, attached, formatted and detached again. You can run the <a href=\\\"https://github.com/5pi/infra/blob/master/packer/files/usr/libexec/kubernetes/kubelet-plugins/volume/exec/5pi.de~do-volume/do-volume\\\">flexvol plugin</a> to do this:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>do-volume create pgdata 10G my postgres volume\\ndo-volume attach '{ \\\"volume\\\": \\\"pgdata\\\" }'\\nmkfs.ext3 /dev/disk/by-id/scsi-0DO_Volume_pgdata</code></pre>\\n      </div>\\n<p>After that, the yaml file can get applied and postgres should spin up.</p>\\n<h3>Ghost</h3>\\n<p>Next is Ghost, the blogging platform powering this and <a href=\\\"https://textkrieg.de\\\">textkrieg.de</a>. It uses Postgres as database and another DO volume for assets.\\nSince the official Ghost image can’t be easily configured automatically and is huge, I’m using my own (<a href=\\\"https://alpinelinux.org/\\\">alpine</a> based) <a href=\\\"https://hub.docker.com/r/fish/ghost/\\\">Docker image</a>.\\nAlthough designed as a <em>modern</em> blogging platform, it doesn’t fit particular well into the new container/12factor world. For example, it assumes that the current working directory is writable. Since we want to run ghost as unprivileged user, we need to make the volume writable by that user. Unfortunately there is no good way to do that. People often create images that run chown as root when starting, then dropping privileges. But this can become time consuming and has possible security implications. The upcoming <a href=\\\"https://github.com/kubernetes/kubernetes/pull/26926\\\">flexvolume redesign</a> will fix this issue. For now we need to make the volume world-writable initially:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>do-volume create ghost-fish 1\\ndo-volume attach '{ \\\"volume\\\": \\\"ghost-fish\\\" }'\\nmkfs.ext4 /dev/disk/by-id/scsi-0DO_Volume_ghost-fish\\n\\nmount /dev/disk/by-id/scsi-0DO_Volume_ghost-fish /mnt\\nmkdir -m 777 /mnt/{apps,data,images} # See 5pi/infra#11\\numount /mnt\\n\\ndo-volume detach /dev/disk/by-id/scsi-0DO_Volume_ghost-fish</code></pre>\\n      </div>\\n<p>Here I choose simplicity over security for the time being since I’m the only operator of this cluster anyway. You might want to make a different trade off.</p>\\n<p>Now we still need to create databases and users, another thing that hasn’t been addressed by Kubernetes yet. For that we need to find the Postgres pod, then <code>exec</code> into it to create database and users:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-none\\\"><code>kubectl exec -ti postgres-2416409090-jf8uh -- psql -U postgres\\ncreate user ghost_fish with password 'pwd-from-secrets.yml';\\ncreate database ghost_fish;\\ngrant all privileges on database ghost_fish to ghost_fish;</code></pre>\\n      </div>\\n<p>Now we can use the yaml file to deploy Ghost. Beside deploying the pods, we also create a <a href=\\\"https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/50_ghost_fish.yml#L13\\\">Ingress to configure Traefik</a>. This routes requests for <code>5pi.de</code> to service <code>ghost-fish</code> on port 80 which is defined above and maps to the Ghost deployment</p>\\n<p>To access the blog, you still need to create an CNAME DNS record matching the Ingress route and pointing to the managed <code>edge.[DOMAIN]</code> DNS name.</p>\\n<h2>Monitoring</h2>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-7f419.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 590px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 61.0479797979798%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAIAAADtbgqsAAAACXBIWXMAAAsSAAALEgHS3X78AAACAklEQVQozzVR2Y4UMQzsz9jp3HGSdtJJ3+fM7swuAgQC3nlD4v8/ArMIycpRjl2pcrUiP7bl/nhdt/N6vS7L0ufQlTwMQ0oJEUMIdMg5l1KmeR77vA6xy9i2bWWg5kzkFl+PmDBIzjljFErIVGJb0Cj1HxEIIXWYUtRKs7quBONWi9+fza83NydNNYJzamGccggpAlr1D1FMhC/gB5+DBcIYq56eWPbiy2aeZIPU8J1Z1lxHialB5wP141y8k8MnU3JOzoEgoK7AUY7acC1YcI4k0lNil7Sx2mqjpeY1J/o2YgoNr6mOPkLFrHJgiQ+UjKCCVcmZPRollRSK1yw2OHem4OW2XN6Oeu35Utja1cdYN+5SeXCth6+7/n7qnw/74zTfrvrs5ZJ0MMy7sI3N82SPzj2Pbu+bvXdDtHPxoEUlhVhTvPd47/DR4+sQz4y3El+6GLSiL2zTcIzdVtJayj4PS1eOodyWEbSqXJBUX9cXciGmGCN6D6SWFNNKZkmSyC4EtzFQYOMIlySpvlRkUjDOCnLfgfL2PbRqjABJs/mbBSsMaGc12UNvnFLBSKu4qLxzvcctpCXEDZsz4tykCeMeMSgFAAVS59o+5M5j52N2uSvDXHKjdGW1Xkt6mciV8JibjyveJ6Trhy02lkYmxgz7AOcE1xFuk9sHey7hZQGv+R9YFW0nnzb+igAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"Grafana\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fb8a0.png\\\"\\n        srcset=\\\"/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-1a291.png 148w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-2bc4a.png 295w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fb8a0.png 590w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-526de.png 885w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-fa2eb.png 1180w,\\n/static/grafana-kube-a6e84b03bb42b9933c5e83d8b5943a97-7f419.png 1584w\\\"\\n        sizes=\\\"(max-width: 590px) 100vw, 590px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>Now that all this is running, it also needs to be monitored properly. Of course I’m using <a href=\\\"https://prometheus.io\\\">Prometheus</a> for this. The setup is straight forward now that Ingress and Volume configuration is nothing new. It tunes Prometheus to run better on small instances and create a DO Volume to store metrics on. To gather metrics about the services running on Kubernetes, the <a href=\\\"https://github.com/kubernetes/kube-state-metrics\\\">kube-state-metrics</a> exporter is deployed. To monitor website response times, I also deploy the <a href=\\\"https://github.com/prometheus/blackbox_exporter\\\">blackbox-exporter</a>.\\nTo send out alerts, two instances of the HA Alertmanager are deployed.\\n<a href=\\\"https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/prometheus/prometheus-config.yml\\\">This ConfigMap</a> configures Prometheus, sets up alerts and configures the Alertmanager. The SMTP credentials get passed in at container startup.\\nThe alerts are quite trigger happy, which is fine in such small cluster. In a bigger cluster you need to adjust those to limit noise. In the end you’re not interested even if whole nodes crash as long as Kubernetes can reschedule the pods. In my tiny cluster I would be surprised if a node crashes, so I alert on this for now.</p>\\n<p>I’m also using Grafana to show metrics on dashboards. Grafana is configured to use Google OAuth authentication against my Google Apps account. This allows Grafana to be available on the public internet and allow everyone in my Google Apps Org to access it.</p>\\n<p>Here is where the wildcard DNS domain comes in handy. Instead of having to add new DNS record for each new service like Grafana, we just access it via the wildcard domain: A request to <code>anything.edge.[DOMAIN]</code> ends up at the floating IP and is accepted by a traefik instance. So the only thing we need to configure is the host in the <a href=\\\"https://github.com/5pi/services/blob/df95f43b1db833a49b6693000787b44d9a92624d/grafana.yml#L19\\\">Ingress definition</a>.</p>\\n<h1>Retrospective and Future</h1>\\n<p>I’ve started all this a few month ago already with the goal of building a small cluster from scratch. I didn’t want to use tons of bash scripts not running on a specific cloud. Back then there was <a href=\\\"https://github.com/kubernetes/kops\\\">kops</a>, which I would recommend to look at first if you’re deploying to AWS.</p>\\n<p>Now there is also kubeadm which looks like a promising way to setup a cluster and I might change my deployment to use it instead. I’ll also consider using CoreOS and cloud-config to configure the Kubernetes components, as well as looking deeper into systemd units to coordinate stopping of services and draining of hosts.</p>\\n<p>If you’re looking for reusable components on top of Kubernetes, there is <a href=\\\"https://github.com/kubernetes/helm\\\">Helm</a> which might be a better option than just keeping the services yaml files around. You might be also interested in CoreOS’s <a href=\\\"https://coreos.com/blog/introducing-operators.html\\\">Operator</a> to fully automate Prometheus operations.</p>\\n<p>The most important next thing on my TODO list is proper tests though. I want a full integration test which spins up a new cluster, deploys services to it, runs blackbox tests, runs an rolling upgrade and tests that during and after that the cluster services are available.</p>\\n<p>Pull requests to make this more generic and fix issues are welcome, but I won’t accept larger changes until I got the tests going.</p>\\n<hr>\\n<h4>Update</h4>\\n<p>Feel free to discuss and comment on <a href=\\\"https://news.ycombinator.com/item?id=13006296\\\">Hacker News</a>.</p>\",\"frontmatter\":{\"title\":\"$15 Production Kubernetes Cluster on DigitalOcean\",\"date\":\"2016-11-20 10:36:00 +0000\"}}},\"pathContext\":{\"slug\":\"/2016/11/20/15-producation-grade-kubernetes-cluster/\",\"previous\":{\"fields\":{\"slug\":\"/2015/10/05/the-future-fabrication-cloud/\"},\"frontmatter\":{\"title\":\"The Future: Fabrication Cloud\"}},\"next\":{\"fields\":{\"slug\":\"/2017/04/17/selenium-webdriver-chromedriver-nightwatch-chrome-headless/\"},\"frontmatter\":{\"title\":\"Frontend Testing Zoo - Or, Nightwatch without Selenium\"}}}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/2016-11-20-15-producation-grade-kubernetes-cluster.json\n// module id = 508\n// module chunks = 162183589928130"],"sourceRoot":""}
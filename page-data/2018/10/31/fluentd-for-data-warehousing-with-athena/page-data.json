{"componentChunkName":"component---src-templates-blog-post-js","path":"/2018/10/31/fluentd-for-data-warehousing-with-athena/","result":{"data":{"site":{"siteMetadata":{"title":"5π Consulting","author":"Johannes 'fish' Ziemke"}},"markdownRemark":{"id":"81a100ed-36c9-501d-b3ad-dba0351d5dcd","html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/1f368/firehose.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.16564417177914%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAOABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAf/EABUBAQEAAAAAAAAAAAAAAAAAAAID/9oADAMBAAIQAxAAAAFusYxIWFB//8QAGhAAAQUBAAAAAAAAAAAAAAAAAAECERIiE//aAAgBAQABBQJ2TSnNSZWxdx//xAAYEQACAwAAAAAAAAAAAAAAAAAAAQISIf/aAAgBAwEBPwFxwof/xAAYEQACAwAAAAAAAAAAAAAAAAAAAQITIf/aAAgBAgEBPwFT0tP/xAAYEAADAQEAAAAAAAAAAAAAAAAAEDEBQf/aAAgBAQAGPwKYoRcP/8QAGxAAAgIDAQAAAAAAAAAAAAAAAREAMRAhQWH/2gAIAQEAAT8hEobOEqakbXJ6pYSOwwNVif/aAAwDAQACAAMAAAAQLx//xAAWEQEBAQAAAAAAAAAAAAAAAAABEBH/2gAIAQMBAT8QRwT/AP/EABYRAQEBAAAAAAAAAAAAAAAAAAEQEf/aAAgBAgEBPxADUf/EAB4QAQACAQQDAAAAAAAAAAAAAAERIQAxUXGBQWHh/9oACAEBAAE/EKUrRMRvzsY2MFBDXXrByoeHFpNaCfH3LyXOQaTi7R2nP//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"By English: Petty Officer 2nd Class Casey H. Kyhl, U.S. Navy (www.defense.gov), Public domain, via Wikimedia Commons\"\n        title=\"By English: Petty Officer 2nd Class Casey H. Kyhl, U.S. Navy (www.defense.gov), Public domain, via Wikimedia Commons\"\n        src=\"/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/6aca1/firehose.jpg\"\n        srcset=\"/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/d2f63/firehose.jpg 163w,\n/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/c989d/firehose.jpg 325w,\n/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/6aca1/firehose.jpg 650w,\n/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/7c09c/firehose.jpg 975w,\n/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/01ab0/firehose.jpg 1300w,\n/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/1f368/firehose.jpg 2100w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p><em>Or how I replaced AWS Firehose by fluentd instances and saved >80% EC2 traffic\ncosts.</em></p>\n<h1 id=\"using-aws-kinesis-firehose\" style=\"position:relative;\"><a href=\"#using-aws-kinesis-firehose\" aria-label=\"using aws kinesis firehose permalink\" class=\"anchor before\"></a>Using AWS Kinesis Firehose</h1>\n<p><a href=\"https://www.koko.ai/\">koko</a> is one of my current clients. They are running on\nthe <a href=\"/2018/02/01/kubecfn-cloudformation-installer-for-reasonably-secure-multi-master-kubernetes-cluster/\">Kubernetes\ninfrastructure I\nbuilt</a>\nearlier this year.\nFor analytics purposes with <a href=\"https://aws.amazon.com/athena/\">AWS Athena</a>, the\nbackend services use <a href=\"https://aws.amazon.com/kinesis/data-firehose/\">AWS Kinesis\nFirehose</a> to ship messages to S3.\nBasically it receives messages via HTTP, batches them based on time or size and\nuploads them to S3.</p>\n<p>We isolated the Kubernetes VMs a VPC for security reasons, so they need to use a\nNAT Gateway to connect to Kinesis Firehose. This was responsible for most of the\ntraffic handled by the NAT Gateways and made up a significant portion of the\nmonthly EC2 costs.  To allow systems in a VPC to reach AWS Services without\nhaving to pass a NAT Gateway, AWS provides <em>VPC Endpoints</em> for various services.\nUnfortunately not for Firehose.</p>\n<p>Beside that, the files produced by Firehose still needed\nprocessing to shard them by customer for efficient Athena querying, which\ninvolved downloading the files from S3 to process them and upload them again.\nMoreover, Firehose wasn’t as reliable as we hoped.</p>\n<p>We discussed introducing Kafka to process the streams, but ideally wanted to\navoid running a complex, distributed system like this. Fortunately, since\nall messages shipped are independent of each other and get deduplicated in\nAthena anyway, we shouldn’t need expensive coordination. So what alternatives do\nwe have?</p>\n<h1 id=\"warehousing-with-fluentd\" style=\"position:relative;\"><a href=\"#warehousing-with-fluentd\" aria-label=\"warehousing with fluentd permalink\" class=\"anchor before\"></a>Warehousing with Fluentd</h1>\n<p>Having introduced <a href=\"https://www.fluentd.org/\">fluentd</a> for shipping logs\nrecently, I realized it might check all the boxes.</p>\n<p>Most people probably use <a href=\"https://www.fluentd.org/\">fluentd</a> for shipping logs.\nThese days it’s the de facto standard for log shipping on Kubernetes. For that\nit reads the local container logs and ships them to a central logging service,\nlike <a href=\"https://www.elastic.co/products/elasticsearch\">Elasticsearch</a>. Then you’d\nuse something like <a href=\"https://www.elastic.co/products/kibana\">Kibana</a>. That\ncompletes the so called <em>EFK Stack</em>.</p>\n<h2 id=\"configuration\" style=\"position:relative;\"><a href=\"#configuration\" aria-label=\"configuration permalink\" class=\"anchor before\"></a>Configuration</h2>\n<p>Fluentd allows use to define a source for receiving messages from the\napplication. Each message contains a shard key that’s used as path prefix on S3.\nFluentd allows parsing this with a custom regexp:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;source&gt;\n  @type http\n  port 8888\n  bind 0.0.0.0\n  body_size_limit 32m\n  keepalive_timeout 10s\n  &lt;parse&gt;\n    @type regexp\n    expression /^(?&lt;path&gt;[^\\s]+) (?&lt;content&gt;.+)$/m\n  &lt;/parse&gt;\n&lt;/source&gt;</code></pre></div>\n<p>Now we can use the s3 output to send the messages to s3.\nSince we need to batch up the files, we use a <em>file buffer</em>. This buffers the\nmessages to a local directory until either the <code class=\"language-text\">chunk_limit_size</code> or the\n<code class=\"language-text\">timekey</code> is reached, after which the file gets uploaded to s3.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;match **&gt;\n  @type s3\n\n  s3_bucket the-output-bucket\n  s3_region us-east-1\n\n  path ${path}\n  s3_object_key_format &quot;%{path}/#{Socket.gethostname}_%{time_slice}_%{index}.%{file_extension}&quot;\n\n  store_as gzip_command\n  &lt;buffer path,time&gt;\n    @type file\n    timekey 3600\n    chunk_limit_size 25MB\n  &lt;/buffer&gt;\n  &lt;format&gt;\n    @type single_value\n    message_key content\n  &lt;/format&gt;\n&lt;/match&gt;</code></pre></div>\n<p>The <code class=\"language-text\">path</code> and <code class=\"language-text\">time</code> arguments to <code class=\"language-text\">&lt;buffer path,time&gt;</code> groups the chunks by\ntime and path. This allows us to set <code class=\"language-text\">s3_object_key_format</code> to add <code class=\"language-text\">path</code> to\nthe destination path as prefix when flushing the buffer to S3.</p>\n<p>We also add the hostname to the destination path by using\n<code class=\"language-text\">#{Socket.gethostname}</code>. This prevents conflicts when running multiple instance\nwithout requiring any coordination between the instances. Since the messages are\nindependent of each other and their order is irrelevant, this is all we need to\nprocess them in parallel.</p>\n<h2 id=\"monitoring\" style=\"position:relative;\"><a href=\"#monitoring\" aria-label=\"monitoring permalink\" class=\"anchor before\"></a>Monitoring</h2>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 650px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/87a6eb80f7df852d72c949c282e58488/2bef9/dashboard.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 69.32515337423312%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAIAAACgpqunAAAACXBIWXMAAAsTAAALEwEAmpwYAAACi0lEQVQoz01T207bQBD1l7RpbO96r76s74ljOyZOAk1CwqWCBooAVTSFiDxA6QX1M/oTfehDn/p1nRBUVRqNZmZndo/nHGuO41EqwHw/iKIkDGOlAko4o4IQTtaBhFPGZBDEUZRK6UJxU9EgMUzLRMQwLF3HTcNCTGJmG4g0DYwQwZiCBzNNy/ivAl5zbCfzRVvwSAhBeKbUfKd8O+qEXDqEwryFKYZWRIjFIP6XQqC1g+Biu3Oyk8/76eV2dj0r7g7LxbS4mpT9VDWamBFGMAWj1nPwbBbTelm8mvc/nQ5WR/X96eD+pH97XK+O68fzwTALmjoGeOYT5g1y0yTrdAM7UN5unY22WpO6PellY7C6M6k7s0Eee45uWlI4nNtC2JxvAocze7NFTQrbVaHrBo6jXBULqRARFpEmZvAsZxz277lKeUr5IUxCG2cSIKxfhivR0/We6zPKTQQ7N+HAMBCQAd26gU0T6zpCmAIQeJ8yu6Ej6IRhp2GShmG9bOIXTfzKANosQoWFGUKUExpJDgGMubYdO9xhLFJelYScME3a7sfd8vawt5xWV6NyuTdQNgiA554shNgOvJudOGIUPqFIguu9ajEtVwfVl7f1rBtrvvJ/nL/+vZj8XMz+3Ex/LXY9zqrI//qmujuoPs+H389Gy1HeaKI0VJcHvbNZ93RWne9vjbcSDeQ5H+fXh9X7/e7yqPpwUBCLFGn0eDn+djF6eDd8OBsuj/ugFiEdLiSHDdu247qE8jXsThJ220nZiqpOWrZTjC0h5KYCvpvFsQ8SxmGUtFp54IdJ3ErTbK1t+B86edVqF8CH4waeimDVvh9mRaU8HypAXlMHVaM0bnWrHjCXZXlRdIHzv04jtAKsLOvoAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Dashboard\"\n        title=\"Dashboard\"\n        src=\"/static/87a6eb80f7df852d72c949c282e58488/a6d36/dashboard.png\"\n        srcset=\"/static/87a6eb80f7df852d72c949c282e58488/222b7/dashboard.png 163w,\n/static/87a6eb80f7df852d72c949c282e58488/ff46a/dashboard.png 325w,\n/static/87a6eb80f7df852d72c949c282e58488/a6d36/dashboard.png 650w,\n/static/87a6eb80f7df852d72c949c282e58488/e548f/dashboard.png 975w,\n/static/87a6eb80f7df852d72c949c282e58488/2bef9/dashboard.png 1024w\"\n        sizes=\"(max-width: 650px) 100vw, 650px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>The\n<a href=\"https://github.com/fluent/fluent-plugin-prometheus\">fluent-plugin-prometheus</a>\nplugin provides metrics about the throughput and output buffer length which can\nbe used to alert on. The dashboard above also shows cAdvisor and kubelet metrics\nfor the fluent pods to monitor disk, CPU and network usage.</p>\n<h2 id=\"deployment\" style=\"position:relative;\"><a href=\"#deployment\" aria-label=\"deployment permalink\" class=\"anchor before\"></a>Deployment</h2>\n<p>We deploy this on Kubernetes with a <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\">Persistent\nVolume</a> for\neach instance to persist the buffer even if a pod crashes or gets recreated.</p>\n<p>Since a single instance of fluentd isn’t able to handle the message throughput,\nwe need to run multiple instances. Since we use volumes, we need to use a\n<a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSet</a>\nfor this.</p>\n<p>To have a endpoint for the application to connect to, we use a\n<a href=\"https://kubernetes.io/docs/concepts/services-networking/service/\">Service</a>\nwhich routes request in round robin. This means one message for a batch might\nend up on a different instance than the next message, which fortunately is no\nproblem even without a complex, coordinated system like kafka.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"></a>Conclusion</h2>\n<p>It took us just two days to implement and rollout this change and it’s been\nworking well for several weeks now without requiring any maintenance so far. It\nsimplified the setup, removed the dependency on Firehose and cut over 80% of the\ntraffic costs.</p>\n<p>Often the question running something yourself vs using a managed service. But\nsometimes operating something simple yourself is better than using something\ncomplex managed by a 3rd party.</p>","frontmatter":{"title":"fluentd for data warehousing with athena","date":"October 31, 2018","image":{"childImageSharp":{"fixed":{"base64":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAUABQDASIAAhEBAxEB/8QAGAABAAMBAAAAAAAAAAAAAAAAAAEDBAL/xAAXAQEBAQEAAAAAAAAAAAAAAAADAQIE/9oADAMBAAIQAxAAAAGyNMpjO0lOkcr9Cz//xAAZEAADAAMAAAAAAAAAAAAAAAAAAQIDEBL/2gAIAQEAAQUCYsiZ1RSJgUbTP//EABgRAAIDAAAAAAAAAAAAAAAAAAARARAh/9oACAEDAQE/AcQpv//EABgRAAIDAAAAAAAAAAAAAAAAAAERABAS/9oACAECAQE/AWzNC//EABoQAAMAAwEAAAAAAAAAAAAAAAABIQIQICL/2gAIAQEABj8CiPKj0sVCsvH/xAAdEAEAAwEAAgMAAAAAAAAAAAABACExEUFRYZGx/9oACAEBAAE/ISA2c2bFoL+T1MZPovE7jerbyDerp7pPmZFCp//aAAwDAQACAAMAAAAQ+8CD/8QAGBEAAwEBAAAAAAAAAAAAAAAAAAERIVH/2gAIAQMBAT8QUYQ+pSH/xAAYEQADAQEAAAAAAAAAAAAAAAAAAREhUf/aAAgBAgEBPxBtoxcyaU//xAAcEAEBAAIDAQEAAAAAAAAAAAABEQAxIVFhQcH/2gAIAQEAAT8QrIiqKzz18xqSkOyfXTvIbA6ybaUR4cPuUrXCgXX5gk6uBD2JN4KtLZLcgAlmquRyhvvP/9k=","width":1200,"height":1200,"src":"/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/ebdd7/firehose.jpg","srcSet":"/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/ebdd7/firehose.jpg 1x,\n/static/d8a7d70f8cee9fbbacce4fcac00fb6cc/b9876/firehose.jpg 1.5x"}}}}}},"pageContext":{"slug":"/2018/10/31/fluentd-for-data-warehousing-with-athena/","previous":{"fields":{"slug":"/2018/02/17/found-severe-azure-kubernetes-bug-and-aint-even-got-a-lousy-t-shirt/"},"frontmatter":{"title":"Found severe Azure Kubernetes Bug but ain't even got a lousy T-Shirt"}},"next":{"fields":{"slug":"/2018/12/10/prometheus-blackbox-exporter-lambda/"},"frontmatter":{"title":"Run Prometheus Blackbox Exporter as Lambda"}}}}}
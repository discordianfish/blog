webpackJsonp([0xc36edfa61f9e],{513:function(e,t){e.exports={data:{site:{siteMetadata:{title:"5π - fish's blog",author:"Johannes 'fish' Ziemke"}},markdownRemark:{id:"/usr/src/src/pages/2017/12/15/production-grade-kubernetes/index.md absPath of file >>> MarkdownRemark",html:'<p>About a year ago I blogged about how to build a <a href="/2016/11/20/15-producation-grade-kubernetes-cluster/">$15 Production Kubernetes\nCluster on DigitalOcean</a>\nand submitted it to <a href="https://news.ycombinator.com/">Hacker News</a>.</p>\n<p>HN being HN, soon after these comments trickled in:</p>\n<p>\n  <a\n    class="gatsby-resp-image-link"\n    href="/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-26951.png"\n    style="display: block"\n    target="_blank"\n    rel="noopener"\n  >\n  \n  <span\n    class="gatsby-resp-image-wrapper"\n    style="position: relative; display: block; ; max-width: 560px; margin-left: auto; margin-right: auto;"\n  >\n    <span\n      class="gatsby-resp-image-background-image"\n      style="padding-bottom: 20.357142857142858%; position: relative; bottom: 0; left: 0; background-image: url(\'data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAIAAAABPYjBAAAACXBIWXMAAAsTAAALEwEAmpwYAAAArElEQVQI101P2QqDQBDz/7+ql9e6QqXFB31o672HuuvtWyMLpRAyYYbMZKxpaue505p3XdP3jVJca6EUG0e5LD1GAMRP/3esqnpzXtT1ByxE2TQZUJYv+LdNG6yrMmLfB8PoHGYUHHTdM6VuHN/D0PX9a5o+Pe8SRQGlNqUOIbckeUQRIcR2nFMQ2IxlMFomQNtWUpZSVrjPeY510AYYAQgCRjQhCsbyYTie+gKvHdkxzMfpiwAAAABJRU5ErkJggg==\'); background-size: cover; display: block;"\n    >\n      <img\n        class="gatsby-resp-image-image"\n        style="width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;"\n        alt="What people call production nowadays... - pst"\n        title=""\n        src="/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-26951.png"\n        srcset="/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-b2ad5.png 148w,\n/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-2283b.png 295w,\n/static/hn-comment-acfed1c5b7512a39140c4288b9d642b6-26951.png 560w"\n        sizes="(max-width: 560px) 100vw, 560px"\n      />\n    </span>\n  </span>\n  \n  </a>\n    .</p>\n<p>Fair enough. If you only need $15 worth of resources, running a three node\nKubernetes cluster might not be the best idea. As explained in that article, I\nwas more referring to the way it’s deployed:</p>\n<ul>\n<li>Highly available: Clustered etcd, multiple master/controller instances</li>\n<li>Secure: TLS for etcd clients and peers and apiserver+kubelet</li>\n</ul>\n<p>Personally I built it mainly to have a Kubernetes playground. But in the\nmeanwhile I founded <a href="https://latency.at">Latency.at</a>. It’s a service to measures\nperformance and availability of sites and services from multiple global\nlocations and provides the results as Prometheus metrics. And of course it’s\nrunning on Kubernetes, so I didn’t have the need for a playground and migrated\nthis blog to <a href="https://github.com/gatsbyjs/gatsby">Gatsby.js</a>.</p>\n<p>I’m also <a href="/hire-me/">consulting</a> people on, among other things, how to build\nproduction grade Kubernetes infrastructure. For this I looked into various way\nto deploy Kubernetes today. There are <em>many</em> and most claimed to be “production\ngrade”:</p>\n<ul>\n<li><a href="https://github.com/kubernetes/kops">https://github.com/kubernetes/kops</a></li>\n<li><a href="https://github.com/coreos/tectonic-installer">https://github.com/coreos/tectonic-installer</a></li>\n<li><a href="https://github.com/aws-quickstart/quickstart-heptio">https://github.com/aws-quickstart/quickstart-heptio</a></li>\n<li><a href="https://github.com/kubernetes-incubator/kube-aws">https://github.com/kubernetes-incubator/kube-aws</a></li>\n<li><a href="https://github.com/kubernetes-incubator/kubespray">https://github.com/kubernetes-incubator/kubespray</a></li>\n<li><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a></li>\n</ul>\n<h3>Highly available?</h3>\n<p>In my book, having a highly available cluster is a strict requirement for\nproduction deployments. You might have specialized use case where this isn’t\nnecessary but if you run realtime, business critical applications this is a\nrequirement.</p>\n<p>While an outage of the controller components won’t affect running applications,\nyou can’t operate the cluster anymore: If your ingress controller gets restarted\nduring that time, it won’t know about your backends. If an important pod dies,\nnobody will restart it.</p>\n<p>This requirement already rules out one of the most popular options:\n<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm</a>.\nAt least it doesn’t claim to be stable yet and everything except this looks very\npromising.</p>\n<p>The same limitation applies to projects built upon kubeadm, like\n<a href="https://github.com/kris-nova/kubicorn">kubicorn</a> and the <a href="https://github.com/aws-quickstart/quickstart-heptio">hepio cloudformation\ntemplates</a>.</p>\n<h3>Secure?</h3>\n<p>A Kubernetes cluster can be “(in)secure” on multiple layers. First etcd should\nrequire peers and client certificates to be signed by a trusted CA. Next the\napiserver needs to verify that the certificate of the etcd endpoint it connects\nto is signed by a trusted CA, as well as certificates of clients connecting to\nthe apiserver. Now the kubelet connecting to the apiserver needs to validate\nthat certificate too.</p>\n<p>Beside the transport level security provided by TLS, Kubernetes supports RBAC to\nlimit the access pods in the cluster have to the Kubernetes API. This needs to\nbe enabled too.</p>\n<p>Again, in my option all these are production requirements. Keep in mind that\njust the ability to run a privileged container or mount a volume is enough to\ncompromise your infrastructure. This can be done on each of these layers with\ndifferent effort. Not require TLS on any connection should be consider missing\nno authentication at all.</p>\n<p>There are setups which don’t require etcd TLS but limit it’s reachability to\ncontroller nodes, then using tains to prevent “untrusted” containers to get\nscheduled on the masters. While this is better than nothing, it’s still a risk\nnot worth taking.</p>\n<p>Unfortunately none of the more light-weight options fulfill these requirements.\nFrankly, there are so many ‘installers’ out there, I can’t say this for sure but\nthe most popular options like kops are lacking. When running with calico\nnetworking, it even requires etcd to be writable from all nodes without\nauthentication. Other options like kubespray don’t support full TLS either. Not\neven the tectonic-installer, which in general looks very promising, supports TLS\nout the box by default.</p>\n<h3>(Self-)Sufficiency?</h3>\n<p>The most common Kubernetes setups are not self-sufficient but depend often on\nmultiple external Docker registries and components during runtime. Since usually\nthese are referred to by mutable tags, there is also no way to guarantee that\nnobody changes a dependency without you noticing.\nUnfortunately this is something rarely discussed and there isn’t any\nlight-weight tooling I came across helping with this.</p>\n<h3>Conclusion</h3>\n<p>I can’t claim I looked into every Kubernetes installer project and ignored\ncomplex “enterprise” stacks like <a href="http://www.projectatomic.io">Red Hat’s Atomic</a>\nor <a href="https://jujucharms.com/canonical-kubernetes/">Canonical Kubernetes Juju</a> but\nI’m surprised how many different options there are, yet how few of them provide\nwhat I’m looking for: A simple, immutable, secure and available cluster.</p>\n<p>As of right now, the\n<a href="https://github.com/coreos/tectonic-installer">tectonic-installer</a> looks like\nthe best option, especially if you need to deploy on bare metal or openstack.\nBut it’s a fast moving project and it could become a drag to keep up with\nupstream changes. Another thing leaving a bad taste is that the component for\nautomated updates is closed source and not available for free.</p>\n<p><a href="https://github.com/kubernetes/kubeadm/issues/261">kubeadm works on HA</a> but I\nwouldn’t be surprised if it takes another year until this works reliably.\nFortunately you can build upon kubeadm to create secure and HA clusters. Since\nthis, to me, appears to be the best option, I implemented this based on\ncloudformation for AWS for one of my clients. Hopefully I can share the results\nas open source soon. For now you can find valuable hints on <a href="https://github.com/kubernetes/kubeadm/issues/546">this GitHub\nissue</a>.</p>\n<p>But should you even run your own Kubernetes cluster? Probably not! Kubernetes is\na incredibly fast moving project. As a rule of thumb, I’d say operating a\nKubernetes cluster is a full time job. Don’t assume any of the installers will\nfree you from developing a deep understanding of Kubernetes’ internals. If you\noperate your own cluster, be prepared to read source code and fix bugs yourself.\nThis is especially true with all the managed solutions and <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes\nEngine</a> now being available for\nfree.</p>',frontmatter:{title:"Production Grade Kubernetes",date:"2017-12-15T14:08:29.000Z"}}},pathContext:{slug:"/2017/12/15/production-grade-kubernetes/",previous:{fields:{slug:"/2017/11/09/use-prometheus-vector-matching-to-get-kubernetes-utilization-across-any-pod-label/"},frontmatter:{title:"Use Prometheus Vector Matching to get Kubernetes Utilization across any Pod Label"}},next:{fields:{slug:"/2018/02/01/kubecfn-cloudformation-installer-for-reasonably-secure-multi-master-kubernetes-cluster/"},frontmatter:{title:"kubecfn: Cloudformation installer for reasonably secure multi-master Kubernetes Cluster"}}}}}});
//# sourceMappingURL=path---2017-12-15-production-grade-kubernetes-39e40abbf3ecbebe7f14.js.map